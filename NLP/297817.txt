t bayesian classif gaussian process a abstractw consid problem assign input vector one class predict pcschmi x m twoclass problem probabl class one given schmi x estim yschmi x gaussian process prior place yschmi x combin train data obtain predict new schmi x point provid bayesian treatment integr uncertainti paramet control gaussian process prior necessari integr carri use laplac approxim method gener multiclass problem m use softmax function demonstr effect method number dataset b introduct consid problem assign input vector x one class predict p cjx classic exampl method logist regress twoclass problem probabl class given x estim oew x b gammay howev method flexibl ie discrimin surfac simpli hyperplan xspace problem overcom extent expand input x set basi function foexg exampl quadrat function compon x highdimension input space larg number basi function one associ paramet one risk overfit train data motiv bayesian treatment problem prior paramet encourag smooth model put prior paramet basi function indirectli induc prior function produc model howev possibl and would argu perhap natur put prior directli function themselv one advantag functionspac prior impos gener smooth constraint without tie limit number basi function regress case task predict realvalu output possibl carri nonparametr regress use gaussian process gp see eg solut regress problem gp prior and gaussian nois model place kernel function train data point coeffici determin solv linear system paramet describ gaussian process unknown bayesian infer carri them describ gaussian process method extend classif problem defin gp y input sigmoid function idea use number author although previou treatment typic take fulli bayesian approach ignor uncertainti posterior distribut given data uncertainti paramet paper attempt fulli bayesian treatment problem also introduc particular form covari function gaussian process prior which believ use model point view structur remaind paper follow section discuss use gaussian process regress problem essenti background classif case section describ applic gaussian process twoclass classif problem extend multipleclass problem section experiment result present section follow discuss section paper revis expand version gaussian process regress use first consid regress problem ie predict real valu output new input valu x given set train data ng relev strategi transform classif problem regress problem deal input valu logist transfer function stochast process prior function allow us specifi given set input x distribut correspond output denot prior function p y similarli p y y joint distribut includ also specifi p tji probabl observ particular valu actual valu ie nois model z z z henc predict distribut found margin product prior nois model note order make predict necessari deal directli prior function space n n dimension joint densiti howev still easi carri calcul unless densiti involv special form p tji p y y gaussian p y jt gaussian whose mean varianc calcul use matrix comput involv matric size n theta n specifi p y y multidimension gaussian for valu n placement point x mean prior function gaussian process formal stochast process collect random variabl fy xjx xg index set x case x input space dimens d number input gp stochast process fulli specifi mean function covari function cx x finit set variabl joint multivari gaussian distribut consid gp x j assum nois model p tji gaussian mean zero varianc oe predict mean varianc x given eg parameter covari function mani reason choic covari function formal requir specifi function gener nonneg definit covari matrix set point model point view wish specifi covari point nearbi input give rise similar predict find follow covari function work well l x l lth compon x vector paramet need defin covari function note analog hyperparamet neural network defin paramet log variabl equat sinc posit scaleparamet covari function obtain network gaussian radial basi function limit infinit number hidden unit w l paramet equat allow differ length scale input dimens irrelev input correspond w l becom small model ignor input close relat automat relev determin ard idea mackay neal v variabl specifi overal scale prior v specifi varianc zeromean offset gaussian distribut gaussian process framework allow quit wide varieti prior function exampl ornsteinuhlenbeck process with covari function cx x rough sampl path meansquar differenti hand squar exponenti covari function equat give rise infinit ms differenti process gener believ gp method quit generalpurpos rout impos prior belief desir amount smooth reason highdimension problem need combin model assumpt ard anoth model assumpt may use build covari function sum covari function one may depend input variabl see section detail deal paramet given covari function straightforward make predict new test point howev practic situat unlik know covari function use one option choos parametr famili covari function with paramet vector either estim paramet for exampl use method maximum likelihood use bayesian approach posterior distribut paramet obtain calcul facilit fact log likelihood l log p dj calcul analyt log kj denot determin k also possibl express analyt partial deriv log likelihood respect paramet l see eg given l deriv respect straightforward feed inform optim packag order obtain local maximum likelihood gener one may concern make point estim number paramet larg rel number data point paramet may poorli determin may local maxima likelihood surfac reason bayesian approach defin prior figur x obtain yx squash sigmoid function oe distribut paramet obtain posterior distribut data seen attract make predict new test point x one simpli averag posterior distribut z gp possibl integr analyt gener numer method may use suffici low dimens techniqu involv grid space use highdimension difficult locat region parameterspac high posterior densiti grid techniqu import sampl case markov chain mont carlo method may use work construct markov chain whose equilibrium distribut desir distribut p jd integr equat approxim use sampl markov chain two standard method construct mcmc method gibb sampler metropolishast algorithm see eg howev condit paramet distribut amen gibb sampl covari function form given equat metropolishast algorithm util deriv inform avail mean tend ineffici randomwalk behaviour parameterspac follow work neal bayesian treatment neural network william rasmussen rasmussen use hybrid mont carlo hmc method duan et al obtain sampl p jd hmc algorithm describ detail appendix d gaussian process twoclass classif simplic exposit first present method appli twoclass problem extens multipl class cover section use logist transfer function produc output interpret x probabl input x belong class job specifi prior function transform specifi prior input transfer function shall call activ denot see figur twoclass problem use logist function gammay denot probabl activ correspond input x respect fundament gp approach classif regress problem similar except error model ny oe regress case replac bernoey choic v equat affect hard classif is ie x hover around take extrem valu previou relat work approach discuss section regress case two problem address a make predict fix paramet b deal paramet shall discuss issu turn make predict fix paramet make predict use fix paramet would like comput r requir us find p new input x done find distribut activ use appropri jacobian transform distribut formal equat obtain p y jt ident equat howev even use gp prior p y y gaussian usual express classif data where ts take valu mean margin obtain p y jt longer analyt tractabl face problem two rout follow i use analyt approxim integr equat ii use mont carlo method specif mcmc method approxim it consid analyt approxim base laplac approxim approxim discuss section laplac approxim integrand p y yjt approxim gaussian distribut center maximum function respect invers covari matrix given gammarr log p y yjt find maximum carri use newtonraphson iter method y allow approxim distribut calcul detail maxim procedur found appendix a integr paramet make predict integr predict probabl posterior p jt p tjp saw regress problem p tj calcul exactli use p r p tjyp yjdi integr analyt tractabl classif problem let use log log use laplac approxim maximum find log log denot righthand side equat log p tj where stand approxim integr space also cannot done analyt employ markov chain mont carlo method follow neal william rasmussen use hybrid mont carlo hmc method duan et al describ appendix d use log p tj approxim log p tj use broad gaussian prior paramet previou relat work work gaussian process regress classif develop observ larg class neural network model converg gp limit infinit number hidden unit comput bayesian treatment gp easier neural network regress case infinit number weight effect integr out one end deal hyperparamet result show gaussian process regress compar perform stateoftheart method nonparametr method classif problem seen aris combin two differ strand work start linear regress mccullagh nelder develop gener linear model glm twoclass classif context give rise logist regress strand work develop nonparametr smooth regress problem view gaussian process prior function trace back least far work kolmogorov wiener s gaussian process predict well known geostatist field see eg known krige altern consid rough penalti function one obtain spline method recent overview see close connect gp rough penalti view explor combin glm nonparametr regress one obtain shall call nonparametr glm method classif earli refer method includ discuss also found text two differ nonparametr glm method usual describ bayesian treatment firstli fix paramet nonparametr glm method ignor uncertainti henc need integr as describ section second differ relat treatment paramet discuss section given paramet one either attempt obtain point estim paramet carri integr posterior point estim may obtain maximum likelihood estim crossvalid gener crossvalid gcv method see eg one problem cvtype method dimens larg comput intens search regiongrid parameterspac look paramet maxim criterion sens hmc method describ similar search use gradient inform carri averag posterior distribut paramet defenc gcv method note wahba comment eg refer back method may robust unrealist prior one differ kind nonparametr glm model usual consid method exact natur prior use often rough penalti use express term penalti kth deriv yx give rise power law power spectrum prior yx also differ parameter covari function exampl unusu find paramet like ard introduc equat nonparametr glm model hand wahba et al consid smooth spline analysi varianc ssanova decomposit gaussian process term build prior sum prior function decomposit ff ff import point function involv order interact from univari function give rise addit model includ sum full interact term one use bayesian point view question kind prior appropri interest model issu also recent work relat method present paper section mention necessari approxim integr equat describ use laplac approxim follow preliminari version paper present gibb mackay develop altern analyt approxim use variat method find approxim gaussian distribut bound margin likelihood p tj below approxim distribut use predict p y jt thu x paramet gibb mackay estim maxim lower bound p tj also possibl use fulli mcmc treatment classif problem discuss recent paper neal method carri integr posterior distribut simultan work gener sampl p y jd two stage process firstli fix n individu s updat sequenti use gibb sampl sweep take time on matrix k gamma comput in time on actual make sens perform quit gibb sampl scan updat paramet probabl make markov chain mix faster secondli paramet updat use hybrid mont carlo method make predict one averag predict made would possibl obtain deriv cvscore respect not knowledg use practic gp multipleclass classif extens preced framework multipl class essenti straightforward although notat complex throughout employ oneofm class code scheme use multiclass analogu logist functionth softmax functionto describ class probabl probabl instanc label class c denot c upper index denot exampl number lower index class label similarli activ associ probabl denot c formal link function relat activ probabl c automat enforc constraint target similarli repres c specifi use oneofm code log likelihood take form c softmax link function give c two class case shall assum gp prior oper activ space specifi correl activ c one import assumpt make prior knowledg restrict correl activ particular class whilst difficulti extend framework includ interclass correl yet encount situat felt abl specifi correl formal activ correl take form hy c k ii c element covari matrix cth class individu correl matrix k c form given equat twoclass case shall use separ set paramet class use independ process perform classif redund forc activ one process say zero would introduc arbitrari asymmetri prior simplic introduc augment vector notat where twoclass case c denot activ correspond input x class c notat also use defin similar manner defin y exclud valu correspond test point x let definit augment vector gp prior take form ae oe where equat covari matrix k block diagon matric k individu matrix k c express correl activ within class c twoclass case use laplac approxim need find mode p y jt procedur describ appendix c twoclass case make predict x averag softmax function gaussian approxim posterior distribut present simpli estim integr use draw gaussian random vector gener is class repres vector length zero entri everywher except correct compon contain experiment result use newtonraphson algorithm initi time entri m iter mean rel differ element w consecut iter less gamma hmc algorithm step size use paramet larg possibl keep reject rate low use trajectori made leapfrog step gave low correl success state prior paramet set gaussian mean gamma standard deviat simul step size produc low reject rate paramet correspond w l s initi gamma v sampl procedur run iter first third run discard burnin intend give paramet time come close equilibrium distribut test carri use rcoda packag exampl section suggest inde effect remov transient although note wide recogn see eg determin equilibrium distribut reach difficult problem although number iter use much less typic use mcmc method rememb i iter involv leapfrog step ii use hmc aim reduc random walk behaviour seen method metropoli algorithm autocorrel analysi paramet indic gener low correl obtain lag iter matlab code use run experi avail ftpcsastonacukneuralwillickigpclass two class tri method two well known two class classif problem leptograpsu crab pima indian diabet dataset first rescal input mean zero unit varianc train set matlab implement hmc simul task take sever hour sgi challeng machin mhz r although good result obtain much less time also tri standard metropoli mcmc algorithm crab problem found similar result although sampl method somewhat slower hmc result crab pima task togeth comparison method from given tabl respect tabl also includ result obtain gaussian process use a estim paramet maxim penalis likelihood found use iter scale conjug gradient optimis b neal mcmc method detail setup use neal method given appendix e leptograpsu crab problem attempt classifi sex crab basi five anatom attribut option addit colour attribut exampl avail crab sex colour make total label exampl split train set crab sex colour make train exampl exampl use test set perform gp method equal best method report name hidden unit neural network direct input output connect logist output unit train maximum likelihood network tabl neal method gave similar level perform also found estim paramet use maximum penalis likelihood mpl gave similar perform less minut comput time pima indian diabet problem use data made avail prof ripley trainingtest split exampl respect baselin error obtain simpli classifi record come diabet give rise error again neal gp method compar best altern perform error around encourag result obtain use laplac approxim neal method similar also estim paramet use maximum penalis likelihood rather mont carlo integr perform case littl wors error minut comput time avail comprehens r archiv network httpwwwcituwienacat avail httpmarkovstatsoxacukpubprnn perform obtain gibb mackay similar method made error crab task with colour given error pima dataset method colour given colour given neural network neural network linear discrimin logist regress pp regress ridg gaussian process laplac approxim hmc gaussian process laplac approxim mpl gaussian process neal method tabl number test error leptograpsu crab task comparison taken ripley ripley respect network use two hidden unit predict approach ripley use laplac approxim weight network local minimum method pima indian diabet neural network linear discrimin logist regress pp regress ridg function gaussian mixtur gaussian process laplac approxim hmc gaussian process laplac approxim mpl gaussian process neal method tabl number test error pima indian diabet task comparison taken ripley ripley respect neural network one hidden unit train maximum likelihood result wors net two hidden unit ripley analysi posterior distribut w paramet covari function equat inform figur plot posterior margin mean standard deviat error bar seven input dimens recal variabl scale zero mean unit varianc would appear variabl shortest lengthscal and therefor variabl associ them multipl class due rather long time taken run code abl test rel small problem mean hundr data point sever class furthermor found full bayesian integr possibl paramet set beyond comput mean therefor satisfi maximum penalis likelihood approach rather use potenti gradient hmc routin simpli use input scale conjug gradient optimis base instead attempt find mode class posterior rather averag posterior distribut test multipl class method forens glass dataset describ dataset exampl input output class dataset small perform figur plot log w paramet pima dataset circl indic posterior margin mean obtain hmc run after burnin one standard deviat error bar squar symbol show log wparamet valu found maxim penal likelihood variabl number pregnanc plasma glucos concentr diastol blood pressur tricep skin fold thick bodi mass index diabet pedigre function age comparison wahba et al use gener linear regress found variabl import estim use fold cross valid comput penalis maximum likelihood estim multipl gp method took approxim hour sgi challeng gave classif error rate see tabl compar best method perform neal method surprisingli poor may due fact allow separ paramet process constrain equal neal code also small perhap signific differ specif prior see appendix e detail discuss paper extend work william rasmussen classif problem demonstr perform well dataset tri believ kind gaussian method forens glass neural network hu linear discrimin pp regress ridg function gaussian mixtur decis tree gaussian process la mpl gaussian process neal method tabl percentag test error forens glass problem see ripley detail method process prior use easili interpret model such neural network prior parameter function space exampl posterior distribut ard paramet as illustr figur pima indian diabet problem indic rel import variou input interpret also facilit incorpor prior knowledg new problem quit strong similar gp classifi supportvector machin svm svm use covari kernel differ gp approach use differ data fit term the maximum margin optim found use quadrat program comparison two algorithm interest direct futur research problem method base gp requir comput trace determin linear solut involv n theta n matric n number train exampl henc run problem larg dataset look method use bayesian numer techniqu calcul trace determin although found techniqu work well rel small size problem test method comput method use speed quadrat program problem svm may also use gp classifi problem also investig use differ covari function improv approxim employ acknowledg thank prof b ripley make avail leptograpsu crab pima indian diabet forens glass dataset work partial support epsrc grant grj novel develop learn theori neural network much work carri aston univers author grate acknowledg hospit provid isaac newton institut mathemat scienc cambridg uk paper written up thank mark gibb david mackay radford neal help discuss anonym refere comment help improv paper appendix maxim case describ find iter vector p y jt maxim materi also cover x x provid complet term equat welldefin complet set activ bay theorem log log p t depend it normal factor maximum p y jt found maxim psi respect use log log k covari matrix gp evalu x psi defin similarli equat k partit term n theta n matrix k n theta vector k scalar k viz enter equat quadrat prior term data point associ it maxim respect achiev first maxim psi respect quadrat optim determin find maximum psi use newtonraphson iter new differenti equat respect find nois matrix given result iter equat avoid unnecessari invers usual conveni rewrit form note gammarrpsi alway posit definit optim problem convex given converg solut y easili found use w zero append n diagon posit given mean varianc easi find r mean distribut p jt order calcul gaussian integr logist sigmoid function employ approxim base expans sigmoid function term error function gaussian integr error function anoth error function approxim fast comput specif use basi set five scale error function interpol logist sigmoid chosen point give accur approxim to desir integr small comput cost justif laplac approxim case somewhat differ argument usual put forward eg asymptot normal maximum likelihood estim model finit number paramet dimens problem grow number data point howev consid infil asymptot see eg number data point bound region increas local averag train data point x provid tightli local estim x henc yx thi reason parallel formal argument found thu would expect distribut p y becom gaussian increas data appendix b deriv log p tj wrt hmc mpl method requir deriv l log p tj respect compon exampl k deriv involv two term one due explicit depend l log k also chang caus chang y howev chosen rpsiyj y l gamma log jk depend jk gamma w j aris depend w henc y differenti one obtain henc requir deriv calcul appendix maxim multipleclass case gp prior likelihood defin equat defin posterior distribut activ jt appendix interest laplac approxim posterior therefor need find mode respect drop unnecessari constant multiclass analogu equat c exp detail use basi function erfx use interpol oex principl appendix a defin psi analog equat first optim psi respect y afterward perform quadrat optim psi respect order optim psi respect y make use hessian given k mn theta mn blockdiagon matrix block k c m although form two class case equat slight chang definit nois matrix w conveni way defin w introduc matrix pi mn theta n matrix form use notat write nois matrix form diagon matrix outer product twoclass case note gammarrpsi posit definit optim problem convex updat equat iter optim psi respect activ follow form given equat advantag represent nois matrix equat invert matric find determin use ident blockdiagon invert blockwis thu rather requir determin invers mn theta mn matrix need carri expens matrix comput n theta n matric result updat equat form given equat nois matrix covari matric multipl class form essenti result need gener method multipleclass problem although mention abov time complex problem scale rather due ident equat calcul function gradient still rather expens experi sever method mode find laplac approxim advantag newton iter method fast quadrat converg integr part newton step calcul invers matrix act upon vector ie gamma b order speed particular step use conjug gradient cg method solv iter correspond linear system b repeatedli need solv system becaus w chang updat save time run cg method converg time call experi cg algorithm termin n calcul deriv log p tj wrt multipleclass case analog twoclass case describ appendix b appendix d hybrid mont carlo hmc work creat fictiti dynam system paramet regard posit variabl augment momentum variabl p purpos dynam system give paramet inertia randomwalk behaviour space avoid total energi h system sum kinet energi potenti energi e potenti energi defin pjd expgamma ie sampl joint distribut p given p p expgamma gamma k margin distribut requir posterior sampl paramet posterior therefor obtain simpli ignor momenta sampl joint distribut achiev two step i find new point phase space nearident energi h simul dynam system use discretis approxim hamiltonian dynam ii chang energi h gibb sampl momentum variabl hamilton first order differenti equat h approxim use leapfrog method requir deriv e respect given gaussian prior log p straightforward differenti deriv log p tj also straightforward although implicit depend and henc must taken account describ appendix b calcul energi quit expens new need perform maxim requir laplac approxim equat propos state accept reject use metropoli rule depend final energi h which necessarili equal initi energi h discret appendix e simul setup neal code use fbm softwar avail httpwwwcsutorontocaradfordfbmsoftwarehtml exampl command use run pima exampl modelspec pimalog binari gpgen pimalog fix mcspec pimalog repeat scanvalu heatbath hybrid gpmc pimalog follow close exampl given neal document gpspec command specifi form gaussian process particular prior paramet v ws see equat express specifi gammadistribut prior v x specifi hierarch gamma prior ws note jitter also specifi prior covari function improv condit covari matrix mcspec command give detail mcmc updat procedur specifi repetit scan valu follow hmc updat paramet use stepsiz adjust factor gpmc specifi sequenc carri time aim reject rate around exceed stepsiz reduct factor reduc simul run again r statist spatial data hybrid mont carlo bayesian data analysi effici implement gaussian process variat gaussian process classifi nonparametr regress gener linear model correspond bayesian estim stochast process smooth spline bayesian method backpropag network maximum likelihood estim model residu covari spatial regress gener linear model scale conjug gradient algorithm fast supervis learn mont carlo implement gaussian process model bayesian regress classif bayesian learn neural network automat smooth regress function gener linear model evalu gaussian process method nonlinear regr sion pattern recognit neural network statist aspect neural network flexibl nonlinear approach classif densiti ratio bayesian numer analysi natur statist learn theori comparison gcv gml choos smooth paramet gener spline smooth problem spline model observ data classif comput infinit network gaussian process regress comparison krige nonparametr regress method tr ctr christoph k i william connect kernel pca metric multidimension scale machin learn v n p s s keerthi k b duan s k shevad a n poo fast dual algorithm kernel logist regress machin learn v n p novemb mrio a t figueiredo adapt spars supervis learn ieee transact pattern analysi machin intellig v n p septemb koji tsuda propag distribut hypergraph dual inform regular proceed nd intern confer machin learn p august bonn germani w d addison r h glendin robust imag classif signal process v n p juli hyunchul kim daijin kim zoubin ghahramani sung yang bang appearancebas gender classif gaussian process pattern recognit letter v n p april yasemin altun alex j smola thoma hofmann exponenti famili condit random field proceed th confer uncertainti artifici intellig p juli banff canada wei chu zoubin ghahramani prefer learn gaussian process proceed nd intern confer machin learn p august bonn germani balaji krishnapuram alexand j hartemink lawrenc carin mario a t figueiredo bayesian approach joint featur select classifi design ieee transact pattern analysi machin intellig v n p septemb wei chu s sathiya keerthi chong jin ong bayesian trigonometr support vector classifi neural comput v n p septemb yasemin altun thoma hofmann alexand j smola gaussian process classif segment annot sequenc proceed twentyfirst intern confer machin learn p juli banff alberta canada hyunchul kim jaewook lee cluster base gaussian process neural comput v n p novemb bart bakker tom hesk task cluster gate bayesian multitask learn journal machin learn research p mark girolami simon roger variat bayesian multinomi probit regress gaussian process prior neural comput v n p august liefeng bo ling wang licheng jiao featur scale kernel fisher discrimin analysi use leaveoneout cross valid neural comput v n p april volker tresp gener bayesian committe machin proceed sixth acm sigkdd intern confer knowledg discoveri data mine p august boston massachusett unit state malt kuss carl edward rasmussen assess approxim infer binari gaussian process classif journal machin learn research p lehel csat manfr opper spars onlin gaussian process neural comput v n p march manfr opper ole winther gaussian process classif meanfield algorithm neural comput v n p novemb michael lindenbaum shaul markovitch dmitri rusakov select sampl nearest neighbor classifi machin learn v n p februari volker tresp scale kernelbas system larg data set data mine knowledg discoveri v n p juli charl a micchelli massimiliano a pontil learn vectorvalu function neural comput v n p januari michael e tip spars bayesian learn relev vector machin journal machin learn research p balaji krishnapuram lawrenc carin mario a t figueiredo alexand j hartemink spars multinomi logist regress fast algorithm gener bound ieee transact pattern analysi machin intellig v n p june t van gestel j a k suyken g lanckriet a lambrecht b de moor j vandewal bayesian framework leastsquar support vector machin classifi gaussian process kernel fisher discrimin analysi neural comput v n p may zhihua zhang jame t kwok dityan yeung modelbas transduct learn kernel matrix machin learn v n p april gavin c cawley nicola l c talbot prevent overfit model select via bayesian regularis hyperparamet journal machin learn research p matthia seeger pacbayesian generalis error bound gaussian process classif journal machin learn research p arnulf b a graf felix a wichmann heinrich h blthoff bernhard h schlkopf classif face man machin neural comput v n p januari ralf herbrich thore graepel colin campbel bay point machin journal machin learn research p liam paninski jonathan w pillow eero p simoncelli maximum likelihood estim stochast integrateandfir neural encod model neural comput v n p decemb alexand j smola bernhard schlkopf bayesian kernel method advanc lectur machin learn springerverlag new york inc new york ny