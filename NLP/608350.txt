t pacbayesian stochast model select a pacbayesian learn method combin inform prior bayesian method distributionfre pac guarante stochast model select predict class label stochast sampl classifi accord posterior distribut classifi paper give pacbayesian perform guarante stochast model select superior analog guarante determinist model select guarante state term train error stochast classifi kldiverg posterior prior shown posterior optim perform guarante gibb distribut simpler posterior distribut also deriv nearli optim perform guarante b introduct pacbayesian approach machin learn attempt combin advantag pac bayesian approach bayesian approach advantag use arbitrari domain knowledg form bayesian prior pac approach advantag one prove guarante gener error without assum truth prior pacbayesian approach base bia learn algorithm arbitrari prior distribut thu allow incorpor domain knowledg yet provid guarante gener error independ truth prior pacbayesian approach relat structur risk minim srm interpret broadli describ learn algorithm optim tradeoff complex structur prior probabl concept model good fit descript length likelihood train data interpret srm bayesian algorithm select concept maximum posterior probabl map algorithm view kind srm algorithm variou approach srm compar theoret experiment kearn et al give experiment evid bayesian mdl algorithm tend fit experiment set bayesian assumpt fail pacbayesian approach use prior distribut analog use map mdl provid theoret guarante fit independ truth prior perhap simplest exampl pacbayesian theorem note consid countabl class concept f f f concept f map set x twovalu set f g let p arbitrari prior probabl distribut function let probabl distribut pair hx yi x x f g assum relat p d defin fflf error rate f ie probabl select hx yi accord f x y let sampl pair drawn independ accord defin fraction pair hx yi f x y fflf measur well f fit train data log view descript length concept f note simpl combin chernoff union bound yield probabl least choic sampl follow f inequ justifi concept select algorithm select f f minim descriptionlength vs goodnessoffit tradeoff right hand side happen lowdescriptionlength concept fit well algorithm perform well if howev simpl concept fit poorli perform guarante poor practic probabl arrang concept apriori view like fit well given high probabl domain specif knowledg use select distribut p precis sens p analog bayesian prior concept f like fit well given high prior probabl p f note howev inequ hold independ assumpt relat distribut p d formula model select algorithm select singl model concept howev model select inferior model averag certain applic exampl statist languag model speech recognit one smooth trigram model bigram model smooth bigram model unigram model smooth essenti minim cross entropi between say model test corpu newspap sentenc turn smooth statist languag model natur formul model averag model select smooth languag model larg contain full trigram model full bigram model full unigram model part one use mdl select structur languag model select model paramet maximum likelihood result structur much smaller smooth trigram model furthermor mdl model perform quit badli smooth trigram model theoret deriv compact represent bayesian mixtur exponenti number smaller suffix tree model model averag also appli decis tree produc probabl leav rather hard classif common method construct decis tree first build overli larg tree fit train data prune tree way get smaller tree fit data tree probabl leav altern construct weight mixtur subtre origin fit tree possibl construct concis represent weight exponenti mani differ subtre paper stochast model select algorithm stochast select model accord posterior distribut model stochast model select seem intermedi model select model averag like model averag base posterior distribut model use distribut differ model averag determinist pick valu favor major model weight posterior stochast model select stochast pick singl model accord posterior distribut first main result paper bound perform stochast model select improv stochast model select given better guarante determinist model select intuit model averag perform even better stochast model select prove pac guarante model averag superior pac guarante given stochast model select remain open problem paper also investig natur posterior distribut provid best perform guarante stochast model select shown optim posterior gibb distribut howev also shown simpler posterior distribut nearli optim section give statement main result paper section relat result previou work remain section present proof summari main result formula appli countabl class concept turn guarante stochast model select hold continu class well eg concept realvalu paramet assum prior probabl measur p possibl uncount continu concept class c sampl distribut possibl uncount set instanc x also assum measur loss function l concept c instanc x lc x exampl might concept predic instanc target concept c lc x defin lc expect sampl instanc x lc x ie e xd lc x let rang sampl instanc drawn independ accord distribut d defin lc s xs lc x q probabl measur concept lq denot e cq lc lq s denot notat signifi probabl gener sampl phi least gamma ffi countabl concept class formula gener follow loss function l lemma mcallest probabl distribut p countabl rule class c follow discuss introduct lead learn algorithm select concept c minim srm tradeoff right hand side inequ first main result paper gener uniform statement distribut arbitrari concept class new bound involv kullbackleibl diverg denot dqjjp distribut q distribut p quantiti dqjjp defin dp c follow first main result paper prove section theorem probabl distribut measur possibl uncount set c measur loss function l follow q rang distribut measur c note definit lq name e cq lc averag loss stochast model select algorithm make predict first select c accord distribut q interpret theorem bound loss stochast model select algorithm use posterior q case countabl concept class q concentr singl concept c quantiti dqjjp equal c and larg m theorem essenti lemma theorem consider stronger lemma handl case uncount continu concept class even countabl class theorem lead better guarante lemma posterior q spread exponenti mani differ model similar empir error rate might occur exampl mixtur decis tree construct second main result paper posterior distribut minim error rate bound given theorem gibb distribut valu fi defin q fi posterior distribut defin follow z normal constant z posterior distribut q defin bq follow second main result paper follow theorem c finit exist fi q fi optim ie bq fi bq q fi satisfi follow unfortun multipl local minima bq fi function fi even multipl local minima satisfi fortun simpler posterior distribut achiev nearli optim perform simplifi discuss consid parameter concept class concept specifi paramet vector theta r n let ltheta x loss concept name paramet vector theta data point x as discuss abov simplifi analysi assum given x ltheta x continu function theta exampl might take theta coeffici nth order polynomi p theta take ltheta x max ffjp theta x gamma fxj fx fix target function ff fix paramet loss function note two valu loss function continu function theta unless predict independ theta consid sampl consist data point data point defin empir loss ltheta paramet vector theta empir loss averag finit number express form ltheta x henc ltheta must continu function theta assum prior theta given continu densiti get exist continu densiti p l empir error satisfi follow p u denot measur subset u concept accord prior measur concept x second main result paper summar follow approxim equat bq denot inf q bq approxim inequ justifi two theorem state below state formal theorem howev interest compar lemma countabl concept class defin c concept minim bound lemma larg m lemma interpret follow clearli structur similar howev two formula fundament differ appli continu concept densiti appli countabl concept class anoth contribut paper theorem give upper lower bound bq justifi first give simpl posterior distribut nearli achiev perform defin l follow defin posterior distribut q l follow z normal constant follow theorem theorem prior probabl measur concept class concept name vector theta r n sampl instanc loss function ltheta x alway interv continu theta prior theta continu probabl densiti r n densiti p l nondecreas interv follow assumpt use theorem quit mild final assumpt densiti p l nondecreas interv defin q l justifi fact definit l impli differenti densiti function p l must densiti p l increas final show q l nearli optim posterior theorem prior probabl measur concept class concept name vector theta r n sampl instanc loss function ltheta x alway interv continu theta prior theta continu probabl densiti r n follow posterior q relat work model select guarante similar given barron assum concept f f f true empir error rate fflf fflf let f defin follow case error rate also known loss barron theorem reduc follow sever differ discuss take f concept f minim right hand side nearli definit f formula impli follow note bound expect fflf larg deviat result give bound fflf function desir confid level ffi also note provid bound fflf term inform avail sampl provid bound the expect of fflf term unknown quantiti fflf mean learn algorithm base output perform guarante along select concept true even concept select incomplet search concept space henc differ f guarante comput bound term unknown quantiti fflf desir proof method use prove yield follow also note like unlik theorem vacuou continu concept class variou model select result similar appear literatur guarante involv index concept arbitrari given sequenc concept given bound base index concept class sequenc class increas vc dimens given neither bound handl arbitrari prior distribut concept do howev give pac srm perform guarante involv form prior knowledg learn bia guarante model select algorithm densiti estim given yamanishi barron cover guarante bound measur distanc select model distribut true data sourc distribut case model assum select optim srm tradeoff model complex good fit train data bound hold without assumpt relat prior distribut data distribut howev perform guarante better exist simpl model fit well precis statement bound somewhat involv perhap less interest eleg guarante given formula discuss below guarante model averag also prove first consid model averag densiti estim let f f f infinit sequenc model defin probabl distribut set x let p prior probabl densiti f assum unknown distribut g x need equal f let sampl element x sampl iid accord distribut g let h natur posterior densiti x defin follow z normal constant note posterior densiti h function sampl henc random variabl catoni yang prove somewhat differ gener theorem special case statement that independ g select expect over draw sampl accord g kullbackleibl diverg dgjjh bound follow hold without assum relat g prior p happen low complex simpl model f dgjjf small posterior densiti h small diverg g simpl model small diverg g dgjjh larg also unlik theorem vacuou continu model class observ also appli gener form appear catoni also give perform guarante model averag densiti estim continu model space use gibb posterior howev statement guarante quit involv relationship bound paper unclear yang consid model averag predict consid fix distribut pair hx yi x x f g consid countabl class condit probabl rule f f f f function x f x interpret p yjx f consid arbitrari prior model f construct posterior given sampl qf posterior model induc posterior h given x defin follow let gx true condit probabl p yjx defin distribut d function g x defin loss lg follow x denot select x margin x final defin ffi follow follow corollari yang theorem ia formula bound loss bayesian model averag without make assumpt relationship data distribut prior distribut p howev seem weaker impli even finit model class larg sampl loss posterior converg loss best model guarante vacuou continu model class observ appli gener statement weight model mixtur also wide use construct algorithm onlin guarante particular weight major algorithm variant prove compet well best expert arbitrari sequenc label data posterior weight use onlin algorithm gibb posterior q fi defin statement theorem one differ onlin guarante theorem algorithm one must know appropri valu fi see train data sinc aprior knowledg fi requir onlin algorithm guarante perform well optim perform well optim srm tradeoff requir tune fi respons train data anoth differ onlin guarante either formula theorem or theorem provid guarante even case incomplet search concept space feasibl onlin guarante requir algorithm find concept perform well train data find singl simpl concept fit well insuffici close relat earlier result theorem bound error rate stochast model select case model select stochast set u model probabl measur simpli renorm prior u theorem gener result case arbitrari posterior distribut proof theorem departur point proof theorem follow sampl size deltac abbrevi lemma prior distribut probabl measur p possibl uncount concept space c follow proof suffic prove follow lemma follow applic markov inequ prove suffic prove follow individu given concept given concept c probabl distribut sampl induc probabl distribut deltac chernoff bound distribut delta satisfi follow suffic show distribut satisfi must satisfi distribut delta satisfi maxim ee continu densiti f delta satisfi impli follow z e theorem consid select sampl s lemma impli probabl least gamma ffi select sampl follow prove theorem suffic show constraint function deltac impli bodi theorem interest comput upper bound quantiti s note deltac prove follow lemma lemma fi k q prove lemma note lemma togeth impli theorem see consid sampl satisfi arbitrari posterior probabl measur q concept possibl defin three infinit sequenc vector condit lemma satisfi follow take limit conclus lemma get e cq deltac prove lemma suffic consid valu drop indic chang valu enlarg feasibl set weaken constraint furthermor point theorem immedi assum without loss gener jensen inequ suffic prove consequ follow lemma origin version paper prove bound approxim form maxim subject constraint lemma fi k q n prove lemma take p q given use kuhntuck condit find vector maxim subject constraint function r n r maximum cy set satisfi f y c f continu differenti y either at y exist f f at y exist nonempti subset constraint f posit coeffici note lemma allow neg first step prove lemma show without loss gener work close compact feasibl set k difficult show exist feasibl point ie vector let c denot arbitrari feasibl valu ie point y without loss gener need consid point satisfi constrain optim problem object function set defin follow constraint version theorem form lq s prove bound applic jensen inequ idea maxim achiev theorem directli due robert schapir constraint impli upper bound constraint impli lower bound henc feasibl set close compact note continu object function close compact feasibl set must bound must achiev maximum valu point set constraint form fy call activ object function whose gradient nonzero everywher least one constraint must activ maximum sinc c feasibl valu object function constraint activ maximum kuhntuck lemma point achiev maximum valu must satisfi follow impli follow sinc constraint must activ maximum follow get follow sinc maximum valu lemma prove proof theorem wish find distribut q minim bq defin follow distribut p empir error lc given fix let k lnffiln m let fl object function rewritten follow k fl fix posit quantiti independ q simplifi analysi consid finit concept class let p prior probabl ith concept let l empir error rate ith concept problem becom find valu q satisfi minim follow p zero q nonzero dqjjp infinit minim bq assum q zero p zero assum without loss gener p nonzero p nonzero object function continu function compact feasibl set henc realiz minimum point feasibl set consid follow partial deriv note q zero p nonzero dqjjp q mean transfer infinitesim quantiti probabl mass q reduc bound minimum must occur boundari point satisfi assum without loss gener nonzero p nonzero two distribut support kuhntuck condit impli rb direct gradient one constraint case must exist singl valu bq yield follow henc minim distribut follow form r distribut q fi theorem proof theorem posterior distribut theorem first note follow dp c assum p l nondecreas interv m impli follow also theorem follow definit bq prove theorem first defin concept distribut u u induc uniform distribut error rate l let w subset valu l p l let ff denot size w measur uniform measur note ff defin concept distribut u follow total measur u written follow z du dp dp z henc u probabl measur concept let q arbitrari posterior distribut concept follow dp dp du impli follow third line follow jensen inequ min conclus pacbayesian learn algorithm combin flexibl prior distribut model perform guarante pac algorithm pac bayesian stochast model select given perform guarante superior analog guarante determinist pacbayesian model se lection perform guarante stochast model select natur handl continu concept class lead natur notion optim posterior distribut use stochast select model although optim posterior gibb distribut shown mild assumpt simpler posterior distribut perform nearli well open question whether better guarante given model averag rather stochast model select acknowledg would like give special thank manfr warmuth inspir paper emphas analog pac onlin set would also like give special thank robert schapir simplifi strengthen theorem avrim blum yoav freund michael kearn john langford yishay mansour yoram singer also provid use comment suggest r complex regular applic artifici neural network minimum complex densiti estim learn classif tree gibb estim univers aggreg rule sharp oracl inequali tie warmuth use expert advic adapt game play use multipl weight predict nearli well best prune decis tree experiment theoret comparison model select method result learnabl vapnikchervonenki dimens weight major algo rithm concept learn use complex regulariza tion pacbayesian theorem prune averag decis tree effici extens mixtur techniqu predict decis tree pac analysi bayesian estim learn nonparametr densiti tyerm finitedimension parametr hypothes adapt estim pattern recognit combin differ procedur mix strategi densiti estim tr ctr franoi laviolett mario marchand pacbay risk bound samplecompress gibb classifi proceed nd intern confer machin learn p august bonn germani matti kriinen john langford comparison tight gener error bound proceed nd intern confer machin learn p august bonn germani avrim blum john lafferti mugizi robert rwebangira rajashekar reddi semisupervis learn use random mincut proceed twentyfirst intern confer machin learn p juli banff alberta canada arindam banerje bayesian bound proceed rd intern confer machin learn p june pittsburgh pennsylvania ron meir tong zhang gener error bound bayesian mixtur algorithm journal machin learn research matthia seeger pacbayesian generalis error bound gaussian process classif journal machin learn research p