t mesh partit effici use distribut system a mesh partit homogen system studi extens howev mesh partit distribut system rel new area research ensur effici execut distribut system heterogen processor network perform must taken consider partit process equal size subdomain small cut set size result convent mesh partit longer primari goal paper address variou issu relat mesh partit distribut system issu includ metric use compar differ partit effici applic execut distribut system advantag exploit heterogen network perform present tool call part automat mesh partit distribut system novel featur part consid heterogen applic distribut system simul anneal use part perform backtrack search desir partit wellknown simul anneal comput intens describ parallel version simul anneal use part result parallel exhibit superlinear speedup case nearli perfect speedup remain case experiment result also present partit regular irregular finit element mesh explicit nonlinear finit element applic call whamsd execut distribut system consist two ibmsp differ processor result regular problem indic percent increas effici processor perform consid compar convent even partit result indic percent increas effici network perform consid compar consid processor perform signific given optim improv percent applic result irregular problem indic percent increas effici processor network perform consid compar even partit b introduct distribut comput regard futur high perform comput nationwid high speed network vbn becom wide avail interconnect highspe comput virtual environ scientif instrument larg data set project globu legion develop softwar infrastructur integr distribut comput inform resourc paper present mesh partit tool distribut system tool call part take consider heterogen processor network found distribut system well heterogen found applic mesh partit requir effici parallel execut finit element finit differ applic wide use mani disciplin biomed engin structur mechan fluid dynam applic distinguish use mesh procedur discret problem domain execut meshbas applic parallel distribut system involv partit mesh subdomain assign individu processor parallel distribut system mesh partit homogen system studi extens howev mesh partit distribut system rel new area research brought recent avail system ensur effici execut distribut system heterogen processor network perform must taken consider partit process equal size subdomain small cut set size result convent mesh partit longer desir part take advantag follow heterogen system featur processor speed number processor local network perform wide area network perform further differ finit element applic consider may differ comput complex differ commun pattern differ element type also must taken consider partit paper discuss major issu mesh partit distribut system particular identifi good metric use compar differ partit result present measur effici distribut system discuss optim number cut set remot commun metric use part identifi good effici estim execut time also present parallel version part significantli improv perform partit process simul anneal use part perform backtrack search desir partit howev well known simul anneal comput intens parallel part use asynchron multipl markov chain approach parallel simul anneal part use partit six irregular mesh subdomain use client processor ibm sp machin result show superlinear speedup case nearli perfect speedup rest result also indic parallel version part produc partit consist sequenti version part use partit part ran explicit d finit element code two geograph distribut ibm sp machin use globu softwar commun two sp compar partit part gener use widelyus partit tool meti consid processor perform result regular problem indic increas effici processor perform consid compar convent even partit result indic gamma increas effici network perform consid compar consid processor perform signific given optim applic result irregular problem indic increas effici processor network perform consid compar even partit remaind paper organ follow section provid background section discuss issu section describ part detail section experiment result section give previou work final conclus background meshbas applic finit element method fundament numer analysi techniqu solv partial differenti equat engin commun past three decad three basic procedur finit element method problem first formul variat weight residu form second step problem domain discret complex shape call element last major step solv result system equat procedur discret problem domain call mesh applic involv mesh procedur refer meshbas applic meshbas applic natur suitabl parallel distribut system implement finit element method parallel involv partit global domain element connect subdomain distribut among p processor processor execut numer techniqu assign subdomain commun among processor dictat type integr method solver method explicit integr finit element problem requir use solver sinc lump matrix which diagon matrix use therefor commun occur among neighbor processor common data rel simpl implicit integr finit element problem howev commun determin type solver use applic applic use paper explicit nonlinear finit code call whamsd use analyz elast plastic materi focu whamsd code concept gener implicit well meshbas applic distribut system distribut comput consist platform network resourc resourc may cluster workstat cluster person comput parallel machin further resourc mayb locat one site distribut among differ site figur show exampl distribut system distribut system provid econom altern costli massiv parallel comput research longer limit comput resourc individu site distribut comput environ also provid research opportun collabor share idea use collabor technolog distribut system defin group set processor share one interconnect network perform group smp parallel comput cluster workstat person comput commun occur within group group refer commun within group local commun processor differ group remot commun number group distribut system repres term s supercomput smp supercomput figur distribut system problem formul mesh partit homogen system view graph partit problem goal graph partit problem find small vertex separ equal size subset mesh partit distribut system howev variat graph partit problem goal differ regular graph partit problem equal size subset may desir distribut system partit problem state follow given graph e jv maximum cost function f v minim min paper cost function f estim execut time given applic distribut system function discuss section graph partit proven npcomplet mesh partit problem distribut system also npcomplet proven appendix therefor focu heurist solv problem major issu section discuss follow major issu relat mesh partit problem distribut system comparison metric effici number cut group comparison metric de facto metric compar qualiti differ partit homogen parallel system equal subdomain minimum interfac or cut set size although object counter exampl metric use extens compar qualiti differ partit obviou equal subdomain size minimum interfac valid compar partit distribut system one may consid obviou metric distribut system unequ subdomain pro portion processor perform small cut set size problem metric heterogen network perform consid given local wide area network use distribut system case big differ local remot commun especi term latenc argu use estim execut time applic target heterogen system alway lead valid comparison differ partit estim use rel comparison differ partit method henc coars approxim execut appropri comparison metric import make estim repres applic system estim includ paramet correspond system heterogen processor perform local remot commun also reflect applic comput complex effici effici distribut system equal ratio rel speedup effect number processor v ratio given below e sequenti execut time one processor e execut time distribut system term v equal summat processor perform rel perform processor use sequenti execut term follow k processor use sequenti execut exampl two processor processor perform f effici would processor use sequenti execut effici processor instead use sequenti execut network heterogen wellknown heterogen processor perform must consid distribut system section identifi condit heterogen network perform must consid distribut system recal defin group collect processor perform share local interconnect network remot commun correspond commun two group given processor requir remot local commun other requir local commun dispar execut time processor correspond differ remot local commun assum equal comput load ideal reduct execut time retrofit step use part tool reduc comput load processor local remot commun equal execut time among processor group step describ detail section reduct execut time occur retrofit demonstr consid simpl case stripe partit commun occur two neighbor processor assum exist two group processor local network perform group locat geograph distribut site requir wan interconnect figur illustr one case g processor local commun local commun remot commun figur commun pattern stripe partit processor as well processor local remot commun differ two commun time is x percentag differ cr cl total execut time e assum e repres execut time take consider processor perform sinc assum processor perform entail even partit mesh time written as consid case partit take consider heterogen network perform achiev decreas load assign processor increas load processor group appli processor j group amount load redistribut cr gamma cl xe amount distribut g processor illustr figur discuss retrofit step part execut time now g differ e e is x g g therefor take network perform consider partit percentag reduct execut time approxim xedenot delta g includ follow percentag commun applic differ remot local commun factor determin applic partit maximum number processor among group remot commun reduct execut time follow g delta exampl whamsd applic experi calcul ideal reduct regular mesh partit one processor group local remot commun therefor rel easi calcul ideal perform improv number processor group local remot commun major issu address reduct partit domain assign group maxim reduct particular issu entail tradeoff follow two scenario mani processor group local remot commun result small messag size execut time without retrofit step smaller case howev given mani processor group remot local commun fewer processor avail redistribut addit load illustr figur mesh size n theta n partit p block block oe oe figur size n theta n mesh partit p block n theta n assum processor equal perform mesh partit two group group p processor processor group boundari incur remot commun well local commun part comput load processor need move processor local commun compens longer commun time assum overlap commun messag messag aggreg use commun one node diagon processor commun time processor group boundari approxim localremot processor local commun commun time approxim again messag aggreg overlap assum local therefor commun time differ processor local remot commun processor local commun approxim local total p number processor local remot commun there fore use equat ideal reduct execut time group and group n figur size n theta n mesh partit p stripe is block one processor group local remot commun result larg messag size result execut time without retrofit step larger case howev processor avail redistribut addit load illustr figur mesh partit stripe one processor group local remot commun follow similar analysi figur commun time differ processor local remot commun processor local commun approxim local one processor remot commun group henc use equat ideal reduct execut time is stripe comm stripe therefor total execut time stripe block partit are stripe reduct block reduct differ total execut time block stripe partit is deltat blocksgammastrip ff l therefor differ total execut time block stripe partit determin term c posit sinc p term b neg block partit higher execut time ie stripe partit advantag p howev block partit still higher execut time unless n larg absolut valu term b larger sum absolut valu c note ff l ff r one two order magnitud larger fi l experi calcul block partit lower execut time n kb mesh use howev largest n kb descript part part consid heterogen applic system particular part take consider differ mesh base applic may differ comput complex mesh may consist differ element type distribut system part take consider heterogen processor network perform figur show flow diagram part part consist interfac program simul anneal program finit element mesh fed interfac program produc propos commun graph fed simul anneal program final partit comput partit graph translat requir input file format applic section describ initi interfac program step requir partit graph problem domain group processor per group comput model commun model partit data finit element mesh interfac program graph simul anneal partit graph interfac program input processor figur part flowchart mesh represent use weight commun graph repres finit element mesh natur extens commun graph commun graph vertic repres element origin mesh weight ad vertex repres number node within element commun graph edg repres connect element weight commun graph weight also ad edg repres number node inform need exchang two neighbor element partit method part entail three step partit mesh distribut system step are partit mesh subdomain group take consider heterogen processor perform element type partit subdomain g part g processor group take consider heterogen network perform element type necessari global retrofit partit among group take consider heterogen local network among differ group step describ detail follow subsect subsect includ descript object function use simul anneal key good partit simul anneal cost function cost function use part estim execut time one particular supercomput let e execut time ith processor p goal minim varianc execut time processor run simul anneal program found best cost function is instead sum actual cost function use simul anneal program cost function ecomm includ commun cost partit element need commun element remot processor therefor execut time balanc paramet need tune accord applic problem size partit first step gener coars partit distribut system group get subdomain proport number processor perform processor comput complex applic henc comput cost balanc across group cost function given by number group system step retrofit second step subdomain assign group step partit among processor within group simul anneal use balanc execut time step varianc network perform consid processor entail inter group commun reduc comput load compens longer commun time step illustr figur two supercomput sc sc sc four processor use two processor use sc comput load reduc p sinc commun remot processor amount reduc comput load repres ffi amount equal distribut three processor assum cut size remain unchang commun time chang henc execut time balanc shift comput load comm comp comm p p comp retrofit d figur illustr retrofit step two supercomput assum two nearest neighbor commun step entail gener imbalanc partit group take consider processor commun local remot processor commun local imbal repres term delta term ad processor requir local remot commun ad term result decreas ecomm compar processor requir local commun cost function given follow equat p number processor given group delta differ estim local remot commun time processor commun local step global retrofit third step address global optim take consider differ local interconnect perform variou group again goal minim varianc execut time across processor step element boundari partit move accord execut time varianc neighbor processor step execut larg differ perform differ local interconnect case signific number element move group step second step execut equal execut time group given new comput load step processor group balanc execut time howev execut time differ group may balanc may occur larg differ commun time differ group balanc execut among group take weight averag execut time group weight group equal comput power group versu total comput power comput power particular group multipl ratio processor perform respect slowest one among group number processor use group denot weight averag e assumpt commun time chang much ie separ step incur larg chang size e optim execut time achiev balanc execut time group execut time e first comput differ e e gamma ad e comp cost function commun cost ecomm remot commun cost group i cost function therefor given by number group system group whose domain increas group whose domain decreas step necessari step perform partit within group parallel simul anneal part use simul anneal partit mesh figur show serial version simul anneal algorithm algorithm use metropoli criteria line figur accept reject move move reduc cost function accept move increas cost function may accept probabl e gamma delta avoid trap local minima probabl decreas temperatur lower simul anneal comput intens therefor parallel version simul anneal use parallel version part three major class parallel simul anneal serial like parallel move multipl markov chain serial like algorithm essenti break move subtask parallel subtask parallel line figur parallel move algorithm processor gener evalu move cost function calcul may inaccur sinc processor awar move processor period updat normal use address effect cost function error parallel move algorithm essenti parallel loop figur line multipl markov chain algorithm multipl simul anneal process start variou processor differ random seed processor period exchang solut best select given processor continu anneal process multipl markov chain approach shown effect vlsi cell placement reason parallel version part use multipl markov chain approach given p processor straightforward implement multipl markov chain approach would initi simul anneal p processor differ seed processor perform move independ final best solut comput get initi solut get initi temperatur stop criteria met f number move per temperatur gener random move evalu chang cost function delta delta accept move updat solut g els f accept probabl updat solut accept g g end loop g end loop figur simul anneal processor select approach howev simul anneal essenti perform p time may result better solut speedup achiev speedup p processor perform independ simul anneal differ seed processor perform mp move m number move perform simul anneal temperatur processor exchang solut end temperatur exchang data occur synchron asynchron synchron multipl markov chain approach processor period exchang solut other asynchron approach client processor exchang solut server processor report synchron approach easili trap local optima asynchron therefor parallel version part use asynchron approach solut exchang client solut better server processor updat better solut server solut better client get updat better solut continu there processor exchang solut server processor end temperatur ensur subdomain connect check disconnect compon end part subdomain disconnect compon parallel simul anneal repeat differ random seed process continu disconnect subdomain number trial exceed three time warn messag given output disconnect subdomain experi section present result two differ experi first experi focus speedup parallel version part second experi focus qualiti partit gener part speedup result tabl parallel part execut time second partit proc barth barth inviscid labarr spiral viscou part use partit six irregular mesh triangular element barth labarr elem spiral elem viscou elem run time partit six irregular mesh parallel part speedup partit barth barth inviscid labarr spiral viscou figur parallel part speedup partit subdomain given tabl respect assum subdomain execut distribut system consist two ibm sp equal number processor differ processor perform further machin interconnect via vbn perform network given tabl discuss section tabl column number client processor use part column run time part second differ mesh solut qualiti use two client processor within use one client processor case solut qualiti estim execut time whamsd figur graphic represent speedup parallel version part rel one client processor figur show mesh partit subdomain superlinear speedup occur case mesh partit subdomain superlinear speedup occur case two smallest mesh spiral inviscid case show slightli less perfect speedup superlinear speedup attribut use multipl client processor conduct search processor benefit result good solut found one client inform given client quickli therebi reduc effort continu search solut superlinear tabl parallel part execut time second partit proc barth barth inviscid labarr spiral viscou speedup result consist report qualiti partit regular mesh part appli explicit nonlinear finit code call whamsd use analyz elast plastic materi code use mpi built top nexu interprocessor commun within supercomput supercomput nexu runtim system allow multipl protocol within applic comput complex linear size problem code execut ibm sp machin locat argonn nation laboratori cornel theori center two machin connect internet macro benchmark use determin network processor perform result network perform analysi given tabl further experi conduct determin cornel node time faster argonn node problem mesh consist regular mesh execut time given time step correspond second applic time gener applic may execut time step record execut time repres run take data run standard deviat less regular problem execut parallel part speedup partitionsbarth barth inviscid labarr spiral viscou figur parallel part speedup partit tabl valu ff fi differ network argonn sp vulcan switch ff machin configur processor anl ibm sp ctc ibm sp tabl present result regular problem column mesh configur column execut time result convent equal partit particular use chaco spectral bisect column result partit taken end first step varianc processor perform comput complex consid column execut time result partit taken end second step varianc network perform consid result tabl show approxim increas effici achiev balanc comput cost anoth gamma effici increas achiev consid varianc network perform small increas effici consid network perform tabl execut time use internet processor anl ctc case chaco proc perf local retrofit theta mesh effici effici theta mesh effici due commun small compon whamsd applic howev recal optim increas perform regular problem describ earlier global optim step last step part balanc execut time across supercomput give signific increas effici it includ tabl expect sinc two supercomput use argonn ibm sp cornel ibm sp interconnect network similar perform indic tabl result indic perform gain achiev step comparison convent method evenli partit mesh given obviou consid processor perform result signific gain follow section irregular mesh consid perform gain result consid network perform irregular mesh experi irregular mesh perform gusto testb avail experi regular mesh testb includ two ibm sp machin one locat argonn nation laboratori anl locat san diego supercomput center sdsc two machin connect vbn veri high speed backbon network servic use globu softwar allow multimod commun within applic macro benchmark use determin network processor perform result network perform analysi given tabl further experi conduct determin sdsc sp processor node time fast anl one tabl valu ff fi differ network anl sp vulcan switch ff sdsc sp vulcan switch ff part use partit five irregular mesh triangular element barth labarr elem viscou elem inviscid call part without restrict sightli modifi version part call part restrict use partit mesh one processor remot commun group meti use gener partit take consider processor perform each processor comput power use one input three partition use identifi perform impact consid heterogen network addit processor further three partition highlight differ forc remot commun occur one processor group versu multipl processor remot commun group consid configur two machin anl sdsc anl sdsc anl sdsc two group correspond two ibm sp anl sdsc use processor sp due limit coschedul comput resourc execut time given time step record execut time repres averag run standard deviat less tabl tabl show experiment result configur column one identifi irregular mesh number element mesh includ parenthesi column two execut time result partit part restrict one processor per group entail remot commun column number indic number processor remot commun group column three similar column two except partit restrict remot commun one processor column four execut time result meti take comput power consider each processor comput power use one tabl execut time use vbn processor anl sdsc mesh part w restrict part wo restrict proc perf meti effici viscou elem s s s effici labarr elem s s s effici effici inviscid elem s s s effici input meti program result show use part without restrict slight decreas execut time achiev compar meti forc remot commun one processor retrofit step achiev signific reduct execut time result tabl tabl show effici increas compar meti execut time reduc compar meti reduct come fact even high speed network vbn differ messag start cost remot local commun larg tabl see differ two order magnitud messag start compar approxim one order magnitud bandwidth restrict remot commun one processor allow part redistribut load among processor therebi achiev close ideal reduct execut time previou work problem domain partit finit element mesh equival partit graph associ finit element mesh graph partit proven tabl execut time use vbn processor anl sdsc mesh part w restrict part wo restrict proc perf meti effici viscou elem s s s effici labarr elem s s s effici effici inviscid elem s s s effici npcomplet problem mani good heurist static partit method propos kernighanlin propos local optim partit method farhat propos automat domain decompos base greedi algorithm berger bokhari propos recurs coordin bisect rcb util spatial nodal coordin inform nouromid et al propos recurs inerti bisect rib simon propos recurs spectral bisect rsb comput fiedler vector graph use lanczo algorithm sort vertic accord size entri fiedler vector recurs graph bisect rgb propos georg liu use sparspak rcm algorithm comput level structur sort vertic accord rcm level structur barnard et al propos multilevel version rsb faster hendrickson leland also report similar multilevel partit method karypi kumar propos new coarsen heurist improv multilevel method aforement decomposit method avail one three autom tool chaco meti topdomdec chaco versatil implement inerti spectral kernighanlin multilevel algorithm algorithm use tabl execut time use vbn processor anl sdsc mesh part w restrict part wo restrict proc perf meti effici viscou elem s s s effici labarr elem s s s effici effici inviscid elem s s effici recurs bisect problem equal size subproblem meti use method fast partit spars matric use coarsen heurist provid speed topdomdec interact mesh partit tool tool produc equal size partit tool applic system processor one interconnect network tool meti produc partit unequ weight howev none tool take network perform consider partit process reason tool applic distribut system crandal quinn develop partit advisori system network workstat advisori system three builtin partit method contigu row contigu point block given inform problem space machin speed network advisori system provid rank three partit method advisori system take consider varianc processor perform among workstat problem howev linear comput complex assum applic case implicit finit element problem wide use further varianc network perform consid conclus paper address issu mesh partit problem distribut system issu includ comparison metric effici cut set present tool part automat mesh partit distribut system novel featur part consid heterogen applic distribut system heterogen distribut system includ processor network perform heterogen applic includ comput complex also demonstr use parallel version part distribut system novel part parallel part use asynchron multipl markov chain approach parallel simul anneal mesh partit parallel part use partit irregular mesh subdomain use client processor ibm sp machin result show superlinear speedup case nearli perfect speedup rest use globu softwar run explicit d finit element code use mesh partit parallel part testb includ two geograph distribut ibm sp machin experiment result present regular mesh irregular finit element mesh whamsd applic execut distribut system consist two ibm sp result regular problem indic increas effici processor perform consid compar even partit result also indic addit gamma increas effici network perform consid result irregular problem indic increas effici processor network perform consid compar even partit experiment result irregular problem also indic increas effici compar use partit take processor perform consider improv come fact even high speed network vbn messag start cost remot local commun still larg differ appendix proof npcomplet mesh partit problem distribut system partit problem distribut system npcomplet proof transform proven npcomplet problem minimum sum squar partit problem distribut system let set arbitrari instanc minimum sum squar shall construct graph desir partit exist g sum squar basic unit minimum sum squar instanc n local replac substitut collect e edg shown figur therefor e defin follow figur local replac transform minimum sum squar partit problem distribut system easi see instanc partit problem distribut system construct polynomi time minimum sum squar instanc disjoint k partit sum squar minim correspond k disjoint partit v given take fa g everi subset a also restrict cost function f minimum sum squar ensur partit sum squar cost function convers disjoint k partit g minimum sum squar cost function correspond disjoint k partit set given choos vertic fa henc minimum sum squar cost function k disjoint partit ensur sum squar sa k disjoint set also minim conclud partit problem distribut system npcomplet appendix nomenclatur estim execut time processor i estim comput time processor i estim commun time processor i perform processor measur comput kernel ff l per messag cost local commun messag cost remot commun cost local commun cost remot commun size messag cl local commun time processor cr remot commun time processor differ cr cl one processor differ cr cl processor i maximum number processor group local remot commun coeffici comput complex paramet use equal contribut comput commun execut time number element partit i number processor system number processor group i g number processor particular group same p number group system ith group system ratio speed processor rel slowest processor system r parallel simul anneal algorithm cell placement hypercub multiprocessor fast multilevel implement recurs spectral bisect partit unstructur problem finit element procedur engin analysi partit strategi nonuniform problem multiproc sor evalu parallel simul anneal strategi applic standard cell placement whamsd project progress report pr data partit network parallel process problem decomposit parallel network block data partit partialhomogen parallel network evalu decomposit techniqu highspe cluster comput partit advisori system network dataparallel process simpl effici automat fem domain decompos automat partit unstructur mesh parallel solut problem comput mechan manag multipl commun method highperform network comput system softwar infrastructur iway metacomput experi comput intract guid theori np complet comput solut larg spars posit definit system parallel simul anneal techniqu legion team simul anneal base parallel state assign finit state machin chaco user guid multilevel algorithm partit graph finit element method internet fast lane research educ fast high qualiti multilevel scheme partit irregular graph fast high qualiti multilevel scheme partit irregular graph multilevel kway partit scheme irregular graph multilevel kway partit scheme irregular graph parallel multilevel kway partit scheme irregular graph effici heurist procedur partit graph placement simul anneal multiprocessor introduct parallel comput design analysi algorithm asynchron commun multipl markov chain parallel simul anneal solv finit element equat concurr comput partit spars matric eigenvector graph partit unstructur problem parallel process topdomdec softwar tool mesh partit parallel process parallel nari specul comput simul anneal studi factor fillin parallel implement finit element method retrofit base methodolog fast gener optim largescal mesh partit beyond minimum interfac size criterion tr partit strategi nonuniform problem multiprocessor partit spars matric eigenvector graph parallel simul anneal techniqu introduct parallel comput threedimension grid partit network parallel process legion vision worldwid virtual comput manag multipl commun method highperform network comput system simul anneal base parallel state assign finit state machin comput solut larg spars posit definit comput intract parallel simul anneal algorithm cell placement hypercub multiprocessor parallel nari specul comput simul anneal mesh partit distribut system problem decomposit parallel network ctr kyungmin lee dongman lee scalabl dynam load distribut scheme multiserv distribut virtual environ system highlyskew user distribut proceed acm symposium virtual realiti softwar technolog octob osaka japan zhile lan valeri e taylor greg bryan dynam load balanc samr applic distribut system scientif program v n p decemb zhile lan valeri e taylor greg bryan dynam load balanc samr applic distribut system proceed acmiee confer supercomput cdrom p novemb denver colorado