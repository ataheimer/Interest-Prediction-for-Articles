t improv gener activ learn a activ learn differ learn exampl learn algorithm assum least control part input domain receiv inform about situat activ learn provabl power learn exampl alon give better gener fix number train examplesin articl consid problem learn binari concept absenc nois describ formal activ concept learn call select sampl show may approxim implement neural network select sampl learner receiv distribut inform environ queri oracl part domain consid use test implement call sgnetwork three domain observ signific improv gener b introduct vs activ learn neural network gener problem studi respect random sampl train exampl chosen random network simpli passiv learner approach gener refer learn exampl baum haussler examin problem analyt neural network cohn tesauro provid empir studi neural network gener learn exampl also number empir effort le cun et al aim improv neural network gener learn exampl learn exampl not howev univers applic paradigm mani natur learn system simpli passiv instead make use least form activ learn examin problem domain activ learn mean form learn learn program control input train on natur system such human phenomenon exhibit high level eg activ examin object low subconsci level eg fernald kuhl work infant reaction motheres speech within broad definit activ learn restrict attent simpl intuit form concept learn via membership queri membership queri learner queri point input domain oracl return classif point much work formal learn theori direct studi queri see eg angluin valiant recent queri examin respect role improv gener behavior mani formal problem activ learn provabl power passiv learn randomli given exampl simpl exampl locat boundari unit line interv order achiev expect posit error less ffl one would need draw o train exampl publish machin learn preliminari version paper appear cohn et al one allow sequenti make membership queri binari search possibl and assum uniform distribut posit error ffl may reach oln queri one imagin number algorithm employ membership queri activ learn studi problem learn binari concept errorfre environ problem learner may proceed examin inform alreadi given determin region uncertainti area domain believ misclassif still possibl learner ask exampl exclus region paper discuss formal simpl approach call select sampl section describ concept learn problem detail give formal definit select sampl describ condit necessari approach use section describ sgnetwork neural network implement techniqu inspir versionspac search mitchel section contain result test implement sever differ problem domain section discuss limit select sampl approach section contain refer relat work field conclud discuss paper concept learn select sampl given arbitrari domain x defin concept c subset point domain exampl x might twodimension space c might set point lie insid fix rectangl plane classifi point x x membership concept c write otherwis popular use artifici neural network concept classifi x present input appropri train network activ design output node threshold x c is x instanc concept c formal concept class c set concept usual describ descript languag exampl class c may set twodimension axisparallel rectangl see figur case neural network concept class usual set concept network may train classifi figur concept class defin set axisparallel rectangl two dimens sever posit neg exampl depict sever consist concept class gener target concept t train exampl pair x tx consist point x usual drawn distribut p point classif tx x t say x tx posit exampl otherwis neg exampl concept c consist exampl x tx cx tx is concept produc classif point x target error c respect distribut p probabl c disagre random exampl drawn p write randomli accord p gener problem pose follow given concept class c unknown target t arbitrari error rate ffl confid ffi mani exampl draw classifi arbitrari distribut p order find concept c c consist exampl fflc t p ffl confid least problem formal valiant studi neural network baum haussler haussler figur region uncertainti rs set point x domain two concept consist train exampl yet disagre classif x region uncertainti consid concept class c set exampl classif region domain may implicitli determin figur concept c consist instanc may agre part interest area determin avail inform defin region uncertainti consist arbitrari distribut p defin size region increment learn procedur classifi train exampl ff monoton nonincreas point fall outsid rs leav unchang point insid restrict region thu ff probabl new random point p reduc uncertainti such rs serv envelop consist concept disagr concept must lie within rs thi rs also bound potenti error consist hypothesi choos error current hypothesi ffl ffl ff sinc basi chang current hypothesi without contradict point ff also bound probabl addit point reduc error select sampl activ learn let us consid learn sequenti process draw exampl one anoth determin much inform success exampl give us draw random whole domain probabl individu sampl reduc error ff defin abov decreas zero draw exampl mean effici learn process also approach zero eventu exampl draw provid us inform concept tri learn consid happen recalcul rs region uncertainti new exampl draw exampl within rs exampl reduc rs reduc uncertainti decreas effici draw exampl call process select sampl distribut p known eg p uniform perform select sampl directli randomli queri point accord p lie strictli insid rs frequent howev sampl distribut well target concept unknown case cannot choos point domain impugn risk assum distribut differ greatli actual underli p mani problem though still make use distribut inform without pay full cost draw classifi exampl rather assum draw classifi exampl atom oper as valiant blumer et al may divid oper two step first draw unclassifi exampl distribut second queri classif point cost draw point distribut small compar cost find point proper classif filter point drawn distribut draw random select classifi train fall rs approach well suit problem speech recognit unlabel speech data plenti classifi label speech segment labori process train set size random sampl pass sampl pass sampl pass sampl pass sampl pass sampl figur batch size select sampl approach one process yield diminish improv ad comput cost figur plot error vs train set size select sampl use differ batch size learn axisparallel rectangl two dimens sinc calcul rs may comput expens may want perform select sampl batch first pass draw initi batch train exampl p train it determin initi rs defin new distribut p sampl zero outsid maintain rel distribut p insid rs make second pass draw second batch train exampl ad first determin new smaller rs smaller batch size is pass made effici algorithm draw train exampl see figur howev sinc rs recalcul pass advantag must weigh ad comput cost incur calcul approxim select sampl even simpl concept class set axisparallel rectangl two dimens may difficult comput expens exactli repres region uncertainti class rectangl neg exampl lie along corner region add complex caus nick outer corner rs as figur realist complic class repres exactli easili becom difficult imposs task use good approxim rs may howev suffici allow select sampl practic implement select sampl possibl number approxim process includ maintain close superset subset rs assum abl maintain superset r point rs also superset select sampl insid r assur exclud part domain interest penalti pay effici may also train point interest effici approach compar pure select sampl measur ratio p rx abl maintain subset r sampl train algorithm must take addit precaut given iter part rs exclud sampl thi need ensur success iter choos subset cover entir region uncertainti an exampl techniqu discuss next section also need keep number exampl iter small prevent oversampl one part domain remaind paper denot arbitrari algorithm approxim true region uncertainti r s neural network select sampl select sampl approach hold promis improv gener mani trainabl classifi remaind paper concern demonstr approxim select sampl may implement use feedforward neural network train error backpropag backpropag algorithm rumelhart et al supervis neural network learn techniqu network present train set inputoutput pair x tx learn output tx given input x train neural network use standard backpropag take train exampl x tx copi x input node network as figur calcul individu neuron output layer layer begin first hidden layer proceed output layer output neuron j comput w ji connect weight neuron j neuron i squash function produc neuron output rang defin error output node n error valu propag back network see rumelhart et al detail neuron j error term x connect weight w ji adjust ad deltaw ji j constant learn rate adjust increment decreas error network exampl x tx present train exampl turn suffici larg network gener converg set weight assum input normal rang first hidden layer second hidden layer output layer input layer network output network input connect weight figur simpl feedforward neural network node comput weight sum input pass sum sigmoid squash function pass result output network accept small error train exampl concept learn model target valu train exampl depend whether input instanc concept learn pattern train error less threshold point need draw attent distinct neural network architectur configur architectur neural network refer paramet network chang train case network topolog transfer function configur network refer network paramet chang train case weight given connect neuron although network train algorithm involv chang network topolog train eg ash consid fix topolog train weight adjust theori method describ should modif equal applic trainabl classifi neural network architectur singl output node concept class c specifi set configur network take on configur implement map input x output mani configur may implement map set threshold such output may say particular configur c repres concept c x c cx see figur train train set then say network configur c implement concept c consist train set below use c denot concept c network c implement it below consid naiv algorithm select sampl neural network examin short come describ sgnet base versionspac paradigm mitchel overcom difficulti naiv neural network queri algorithm observ neural network implement concept learner may produc realvalu output threshold suggest naiv algorithm defin region uncertainti network train toler divid point domain one three classif greater less uncertain between may say last categori correspond region network uncertain may thu defin r s approxim region uncertainti figur problem appli approach measur uncertainti particular configur uncertainti among configur possibl given architectur fact terminolog judd figur threshold output train neural network c serv classifi repres concept c hope similar unknown target concept part rs full region compris differ possibl consist network configur limit exacerb induct bia learn algorithm includ backpropa gation backpropag algorithm attempt classifi set point tend draw sharp distinct becom overli confid region still unknown result r s chosen method gener small subset true region uncertainti patholog exampl behavior exhibit figur b figur a initi random sampl fail yield posit exampl triangl right train backpropag exampl yield region uncertainti between two contour concentr left half domain complet exclus right final result iter queri learn shown figur b strategi and relat one prone failur form whenev region detail target concept discov initi random sampl stage version space mitchel describ learn procedur base partial order gener concept learn concept c more gener anoth concept c c ae c c ae c c ae c two concept incompar concept class c set exampl version space subset consist g bound concept version space maintain two subset set most specif consist concept is cg similarli set most gener concept consist concept c must case c g g g one may activ learn version space examin instanc fall differ g is region where delta symmetr differ oper instanc region prove posit gener accommod new inform prove neg g g modifi exclud it either case version space space plausibl hypothes reduc everi queri implement activ versionspac search sinc entir neural network configur repres singl concept complet version space cannot directli repres singl neural network fact haussler point size figur naiv approach repres region uncertainti use network transit area repres part domain network uncertain g set could grow exponenti size train set repres set complet would requir keep track manipul exponenti number network configur can howev modifi versionspac search make problem tractabl done impos accord distribut p strict index order concept class defin concept c more gener concept c random point x drawn definit gener concept class compar make sens speak order repres singl most gener concept g singl most specif concept s may still mani concept gener impedi need know concept most gener case greater gener concept g chosen maintain two concept window version space r s sdeltag subset deltag thu point x guarante reduc size version space posit invalid leav us anoth s either gener one equal specif one includ new point similarli new point classifi neg invalid g proceed fashion approxim stepbystep travers g set use fix represent size sgnet neural network versionspac search algorithm sinc interest select exampl improv gener behavior given neural network architectur n defin concept class question set concept learnabl n learn algorithm manag obtain network configur repres g concept describ abov simpl matter implement modifi versionspac search follow two subsect first describ one may learn most specif most gener concept associ network describ two network may use select sampl r s defin region disagre implement most specificgener network below describ one may learn s most specif concept consist given data case learn g most gener concept analog specif network set exampl accord distribut p one classifi posit exampl point fact posit classifi neg much possibl rest domain requir amount choos c consist minim p rx c figur patholog exampl naiv network queri a left initi random sampl fail detect second disjoint region target concept b right success iter then naiv queri algorithm ignor region concentr region seen exampl dot line denot true boundari unknown target concept network may arriv employ induct bia induct bia predisposit learn algorithm solut other learn algorithm inher least form induct bia whether prefer simpl solut complex one tendenc choos solut absolut valu paramet remain small explicitli add new induct bia backpropag algorithm penal network part domain classifi posit add bia prefer specif concept gener one weight penalti must care adjust larg enough outweigh train exampl network converg train data must howev larg enough outweigh induct bia learn algorithm forc find specif configur consist neg bia may implement draw unclassifi point p or creat case p known arbitrarili label neg exampl add background exampl train set figur creat background bia domain weight input distribut p network least error background pattern one specif accord p order allow network converg actual train exampl spite background exampl must balanc influenc background exampl train data network learn train exampl x error term equat approach zero error term arbitrari background exampl may remain constant unless push random background exampl exert network weight deltaw ji y decreas match normal train exampl deltaw ji x background exampl domin network converg solut achiev balanc use differ learn rate train exampl background exampl dynam decreas background learn rate function network error train set time present train exampl x calcul new background learn rate error network x constant train singl background induct bias inher backpropag well studi appear tendenc fit data use smallest number unit possibl figur train larg number background point addit regular train data forc network most specif configur exampl use valu j repeat formal algorithm follow initi network random configur c actual train exampl x otherwis select next actual train exampl x tx calcul output error network c input x backpropag network adjust weight accord deltaw ji calcul new background learn rate draw point p creat background exampl y calcul output error backpropag network adjust weight accord modifi equat deltaw ji go step optim fl set weight updat background pattern alway infinitesim smaller weight updat actual train pattern allow network anneal specif configur howev requir prohibit amount train time empir found set provid adequ bia still allow converg reason number iter similar procedur use produc most gener network ad posit induct bia classifi background point drawn p posit implement activ learn sgnet repres concept g simpl matter test point x membership r s determin sx gx select sampl may implement follow point drawn distribut sdeltag if two network agre classif it point discard point sdeltag true classif queri ad train set practic merg input g network illustr figur train togeth import note techniqu somewhat robust failur mode degrad effici singl sampl iter rather caus overal failur learn process either typic network architectur split separ g network input merg g g figur construct sgnetwork equival origin g network fail converg train data point fail converg contain sdeltag region elig addit sampl next iter case found addit exampl suffic push network local minimum network converg train set settl solut near specificgener network consist data exampl glean next iter still use sinc chosen virtu lie area two network disagre point settl discrep two may lead oversampl region not itself caus techniqu fail effect two failur mode minim keep number exampl taken iter small increas effici learn process term number exampl classifi observ tradeoff comput resourc requir time new data ad train set network may complet readjust incorpor new inform found practic larg train set size often effici simpli retrain entir network scratch new exampl ad recent work pratt offer hope retrain may made effici use inform transfer strategi iter experiment result experi use select sampl run three type problem solv simpl boundari recognit problem two dimens learn input realvalu threshold function recogn secur region small power system triangl learner twoinput network two hidden layer unit singl output train uniform distribut exampl posit insid pair triangl neg elsewher task chosen intuit visual appeal requir learn nonconnect concept task demand train algorithm and sampl select scheme simpl convex shape baselin case consist network train randomli drawn exampl train set size point increment exampl eight test case run architectur data select four run sgnetwork use select sampl iter exampl figur b addit run naiv queri algorithm describ section run comparison network train select sampl data show mark consist improv randomli sampl network one train naiv queri figur naiv queri algorithm display much errat perform two algorithm possibl due patholog natur failur mode figur triangl learner problem learn random exampl a left learn exampl drawn pass select sampl b right dot line denot true boundari unknown target concept realvalu threshold function use bit realvalu threshold problem quantit measur network perform simpl higherdimension problem six run select sampl use iter exampl per iter train problem compar ident network train randomli sampl data result figur indic much steeper learn curv select sampl plot gener error number train exampl m network train randomli sampl data exhibit roughli polynomi curv would expect follow blumer et al use simpl linear regress ffl error data fit coeffici determin r network train select sampl data comparison fit indic fit polynomi good visual select sampl network exhibit steeper drop gener error would expect activ learn method use linear regress natur logarithm error select sampl network exhibit decreas gener error match error drop indic good fit exponenti curv comparison randomli sampl network fit domain sgnetwork appear provid almost exponenti improv gener increas train set size much one would expect good activ learn algorithm suggest sgnetwork repres good approxim region uncertainti in domain thu implement good approxim select sampl addit experi run use iter indic error decreas sampl process broken smaller frequent iter observ consist increas effici sampl new inform incorpor earlier sampl process power system secur analysi variou load paramet electr power system within certain rang system secur otherwis risk thermal overload brownout previou research aggoun et al determin problem amen neural network learn random sampl problem domain ineffici term exampl need rang paramet system run known distribut inform readili avail set paramet each point domain one analyt determin whether system secur must done solv train set size random sampl naiv queri select sampl figur gener error vs train set size random sampl naiv queri select sampl irregular naiv queri algorithm error may due intermitt failur find triangl intial random sampl timeconsum system equat thu sinc classif point much expens determin input distribut problem amen solut select sampl baselin case random sampl four dimens studi hwang et al use comparison experi ran six set network initi random train set with data point ad singl iter select sampl network train small second iter point for total well larg second iter for total point result compar baselin case point randomli sampl data estim network error test randomli drawn test point improv singl extra iter select sampl yield small set total error instead larg set result improv total instead differ signific greater confid limit select sampl approach number limit select sampl approach practic mention previou section discuss implement techniqu other theoret practic limit discuss earlier paper exact implement select sampl practic rel simpl concept class class becom complex becom difficult comput maintain accur approxim rs case maintain superset increas concept complex seem lead case r effect contain entir domain reduc effici select sampl random sam pling exampl section illustr nice bound box suffic approxim train set size sampl select sampl polynomi exponenti figur gener error vs train set size random sampl select sampl standard deviat error averag random case select sampl case rectangl two dimens nick box bound dimension figur could conceiv requir approxim contain domain space case maintain subset increas concept complex lead extrem r contain small subset rs case oversampl region becom critic problem due induct bia train algorithm even train set size one may omit larg region domain theoret limit select sampl draw power abil differenti region uncertainti bulk domain case represent complex concept larg as neural network mani hidden unit howev rs extend whole domain concept alreadi welllearn is even though maximum error may small due number place error may aris total uncertainti may remain larg thu depend desir final error rate select sampl may come effect longer need similarli input dimens larg bulk domain may uncertain even simpl concept one method avoid problem use bayesian probabl measur degre util queri variou part region uncertainti approach recent studi david mackay discuss briefli follow section relat work work describ paper extens result publish cohn et al prior work sinc then mani relat result activ learn larg bodi work studi effect queri strict learn theori viewpoint primarili respect learn formal concept boolean express finit state automata angluin show minim finit state automata polynomi learnabl in valiant sens exampl alon could learn use polynomi number queri oracl provid counterexampl valiant consid variou class learnabl use varieti form activ learn work eisenberg rivest put bound degre membership queri exampl help gener underli distribut unknown addit given certain smooth constraint distribut describ queri may use learn class initi segment unit line actual implement queri system learn recent explor work done hwang et al implement queri neural network mean invert activ train network determin uncertain approach show promis concept learn case rel compact connect concept alreadi produc impress result power system static secur problem is howev suscept patholog discuss section algorithm due baum lang use queri reduc comput cost train singl hiddenlay neural network algorithm make queri allow network effici determin connect weight input layer hidden layer seung et al independ propos similar scheme select queri base lack consensu committe learner freund et al show size committe increas beyond two learner use select sampl accuraci one util estim increas sharpli work david mackay pursu relat approach data select use bayesian analysi assign prior probabl concept or network configur one determin util queri variou part rs fact point lie within rs mean consist configur disagre classif point point edg rs though may configur disagre queri point decreas size rs infinitesim small amount use bayesian analysi one may effect determin number configur disagre given point thu determin part rs uncertain conclus paper present theori select sampl describ neural network implement theori examin perform result system sever domain select sampl rudimentari form activ learn benefit formal ground learn theori neural network implement test demonstr signific improv passiv random sampl techniqu number simpl problem paradigm suit concept learn problem relev input distribut known cost obtain unlabel exampl input distribut small compar cost label exampl limit select sampl becom appar complex problem domain approach open door studi sophist techniqu queri learn natur intuit mean activ learn acknowledg work support nation scienc foundat grant number ccr washington technolog center ibm corpor major work done david cohn dept comput scienc engin univers washington remaind done david cohn ibm t j watson research center yorktown height ny would like thank jai choi siri weerasooriya work run simul data power system problem would also like thank two anonym refere suggest earlier version paper r artifici neural network power system static secur assess learn regular set queri counterexampl dynam node creation backpropag network size net give valid gener construct hidden unit use exampl queri learnabl vapnikchervonenki dimens train node neural network npcomplet train connectionist network queri select sampl tight vapnikchervonenki bound neural comput volumevolumeissueissuepagespag sampl complex paclearn use random chosen exampl acoust determin infant prefer motheres speech learn conjunct concept structur domain gener pac model neural net learn applic queri learn base boundari search gradient comput train multilay perceptron complex load shallow neural network optim brain damag gener search learn intern represent error propag theori learnabl tr ctr gari m weiss ye tian maxim classifi util train data costli acm sigkdd explor newslett v n p decemb patricia g foschi huan liu activ learn detect spectral variabl subject color infrar imageri pattern recognit letter v n p octob traci hammond randal davi interact learn structur shape descript automat gener nearmiss exampl proceed th intern confer intellig user interfac januari februari sydney australia geoff hulten pedro domingo mine complex model arbitrarili larg databas constant time proceed eighth acm sigkdd intern confer knowledg discoveri data mine juli edmonton alberta canada a p engelbrecht r brit supervis train use unsupervis approach activ learn neural process letter v n p june prem melvil foster provost maytal saartsechanski raymond mooney econom activ featurevalu acquisit expect util estim proceed st intern workshop utilitybas data mine p august chicago illinoi rebecca hwa minim train corpu parser acquisit proceed workshop comput natur languag learn p juli toulous franc brigham anderson andrew moor activ learn hidden markov model object function algorithm proceed nd intern confer machin learn p august bonn germani rebecca hwa sampl select statist grammar induct proceed joint sigdat confer empir method natur languag process larg corpora held conjunct th annual meet associ comput linguist p octob hong kong sean p engelson ido dagan minim manual annot cost supervis train corpora proceed th annual meet associ comput linguist p june santa cruz california kiyonori ohtak analysi select strategi build dependencyanalyz corpu proceed colingacl main confer poster session p juli sydney australia georg k baah alexand gray mari jean harrold onlin anomali detect deploy softwar statist machin learn approach proceed rd intern workshop softwar qualiti assur novemb portland oregon jason baldridg mile osborn activ learn hpsg pars select proceed seventh confer natur languag learn hltnaacl p may edmonton canada prem melvil raymond j mooney divers ensembl activ learn proceed twentyfirst intern confer machin learn p juli banff alberta canada mark steedman rebecca hwa stephen clark mile osborn anoop sarkar julia hockenmai paul ruhlen steven baker jeremiah crim exampl select bootstrap statist parser proceed confer north american chapter associ comput linguist human languag technolog p may june edmonton canada hinrich schtze emr velipasaoglu jan o pedersen perform threshold practic text classif proceed th acm intern confer inform knowledg manag novemb arlington virginia usa rebecca hwa sampl select statist pars comput linguist v n p septemb jame f bowr jame m rehg mari jean harrold activ learn automat classif softwar behavior acm sigsoft softwar engin note v n juli a p engelbrecht sensit analysi decis boundari neural process letter v n p dec stephen soderland learn inform extract rule semistructur free text machin learn v n p feb michael lindenbaum shaul markovitch dmitri rusakov select sampl nearest neighbor classifi machin learn v n p februari steven a wolfman tessa lau pedro domingo daniel s weld mix initi interfac learn task smartedit talk back proceed th intern confer intellig user interfac p januari santa fe new mexico unit state gaurav pandey himanshu gupta pabitra mitra stochast schedul activ support vector learn algorithm proceed acm symposium appli comput march santa fe new mexico qi su dmitri pavlov jyhherng chow wendel c baker internetscal collect humanreview data proceed th intern confer world wide web may banff alberta canada atsushi fujii takenobu tokunaga kentaro inui hozumi tanaka select sampl examplebas word sens disambigu comput linguist v n p decemb sunita sarawagi anuradha bhamidipati interact dedupl use activ learn proceed eighth acm sigkdd intern confer knowledg discoveri data mine juli edmonton alberta canada leonardo franco sergio a canna gener select exampl feedforward neural network neural comput v n p octob aleksand kocz joshua alspector asymmetr missingdata problem overcom lack neg data prefer rank inform retriev v n p januari jianqiang shen thoma g dietterich activ em reduc nois activ recognit proceed th intern confer intellig user interfac januari honolulu hawaii usa francoi barbanon daniel p mirank sphinx schema integr exampl journal intellig inform system v n p octob yevgeniy vorobeychik michael p wellman satind singh learn payoff function infinit game machin learn v n p may kinh tieu paul viola boost imag retriev intern journal comput vision v n p januaryfebruari dilek hakkanitr giusepp riccardi gokhan tur activ approach spoken languag process acm transact speech languag process tslp v n p octob zhihua zhou ming li tritrain exploit unlabel data use three classifi ieee transact knowledg data engin v n p novemb pabitra mitra b uma shankar sankar k pal segment multispectr remot sens imag use activ support vector machin pattern recognit letter v n p juli joel ratsabi learn multicategori classif sampl queri inform comput v n p septemb yoram baram ran elyaniv kobi luz onlin choic activ learn algorithm journal machin learn research p mariaflorina balcan alina beygelzim john langford agnost activ learn proceed rd intern confer machin learn p june pittsburgh pennsylvania maytal saartsechanski foster provost activ sampl class probabl estim rank machin learn v n p februari david d lewi william a gale sequenti algorithm train text classifi proceed th annual intern acm sigir confer research develop inform retriev p juli dublin ireland huan liu hiroshi motoda lei yu select sampl approach activ featur select artifici intellig v n p novemb vijay s iyengar chidanand apt tong zhang activ learn use adapt resampl proceed sixth acm sigkdd intern confer knowledg discoveri data mine p august boston massachusett unit state hema raghavan omid madani rosi jone activ learn feedback featur instanc journal machin learn research p russel greiner adam j grove dan roth learn costsensit activ classifi artifici intellig v n p august raymond j mooney lorien roy contentbas book recommend use learn text categor proceed fifth acm confer digit librari p june san antonio texa unit state henrik jacobsson crystal substochast sequenti machin extractor cryssmex neural comput v n p septemb huan liu hiroshi motoda issu instanc select data mine knowledg discoveri v n p april xingquan zhu xindong wu costconstrain data acquisit intellig data prepar ieee transact knowledg data engin v n p novemb gedimina adomaviciu alexand tuzhilin toward next gener recommend system survey stateoftheart possibl extens ieee transact knowledg data engin v n p june a p engelbrecht sensit analysi select learn feedforward neural network fundamenta informatica v n p august andri p engelbrecht sensit analysi select learn feedforward neural network fundamenta informatica v n p decemb m hasenjg h ritter activ learn neural network new learn paradigm soft comput physicaverlag gmbh heidelberg germani