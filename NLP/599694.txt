t reinforc learn call admiss control rout qualiti servic constraint multimedia network a paper solv call admiss control rout problem multimedia network via reinforc learn rl problem requir network revenu maxim simultan meet qualiti servic constraint forbid entri certain state use certain action problem formul constrain semimarkov decis process show rl provid solut problem abl earn significantli higher revenu altern heurist b introduct number research recent explor applic reinforc learn rl resourc alloc admiss control problem telecommu nicat eg channel alloc wireless system network rout admiss control telecommun network nie haykin singh boyan littman marbach et al paper focus applic rl method call admiss control cac rout broadband multimedia commun network atm network broadband network carri heterogen traffic type simultan channel channel packetbas custom send vari rate time call arriv depart time network choos accept reject connect request new call accept network choos appropri rout deliv call sourc node destin node network provid qualiti servic qo guarante packet level eg maximum probabl congest call level eg limit call block probabil iti return network collect revenu payoff custom call accept network network want find cac rout polici maxim long term revenueutil meet qo constraint maxim revenu meet qo constraint suggest constrain semimarkov decis process smdp mitra et al rapid growth number state problem complex led rl approach prob lem marbach tsitsikli marbach et al howev rl applic ignor qo criteria work draw close relat fundament problem constrain optim semimarkov decis process studi research control theori oper re search artifici intellig commun see eg altman shwartz feinberg gabor et al unlik modelbas algorithm eg linear program mitra et al rl algorithm use paper stochast iter algorithm requir priori knowledg state transit probabl associ underli markov chain thu use solv real network problem larg state space cannot handl modelbas algorithm automat adapt real traffic condit work build earlier work author brown et al provid gener framework studi cac rout problem qo constraint also provid detail inform proof rl algorithm use studi contain result combin cac rout multimedia network report tong brown section describ problem model use studi section formul cac problem smdp give rl algorithm solv smdp section consid qo constraint detail simul cac singl link system present section combin cac network rout studi section simul result node link network section conclud paper problem descript section describ cac problem singlelink commun system substanti literatur cac one link multiservic network eg marbach tsitsikli mitra et al refer dziong mason singl link case signific sinc basic build block larger network and shown section paper combin cac rout multilink network system decompos singl link process thu first focu singlelink system user attempt access link time network immedi choos accept reject call accept call gener traffic term bandwidth function time later time call termin depart network call accept network receiv immedi revenu payment network measur qo metric transmiss delay packet loss ratio call reject probabl servic class compar guarante given call problem describ call arriv traffic departur process revenu payment qo metric qo constraint network model concret describ choic use later exampl call divid discret class index call gener via independ poisson arriv process arriv rate exponenti hold time mean hold time within call bandwidth onoff process traffic either on gener packet rate r rate zero mean hold time class call admit system collect reinforc learn call admiss control fix amount revenu interpret averag reward carri igammath class call dziong mason network element connect network fix bandwidth b total bandwidth use accept call vari time one import packetlevel qo metric fraction time total bandwidth exce network bandwidth caus packet loss ie congest probabl choos packetlevel qo guarante upper limit congest probabl p denot capac constraint previou work eg carlstrom nordstrom marbach et al call constant bandwidth time effect qo predict variabl rate traffic safe approxim assum alway transmit maximum peak rate peak rate alloc underutil network case order magnitud less possibl network effici improv statist multiplex statist bursti sourc unlik simultan commun peak rate thu possibl carri bursti variabl rate traffic would possibl alloc capac accord peak rate requir maintain servic qualiti stochast traffic rate real traffic desir high network utilizationrevenu result potenti qo violat character problem studi anoth import qo metric calllevel block probabl offer traffic class must cut back meet capac constraint import fairli denot fair constraint fair defin number differ way one intuit notion call everi class entitl admiss probabl equival reject probabl dziong mason precis defin section ultim goal find polici everi system state s choos correct control action a maxim revenu subject qo constraint formal consid follow problem find cac polici maxim j subject j j l fset policiesg k number qo constraint l real number character qo constraint j character averag network revenu polici j j character qo polici consid object form k action chosen state n accord polici reward function associ revenu for assum bound s n averag sojourn time state n action n index ngammath decis epoch decis made point time refer decis epoch semimarkov decis process reinforc learn follow section develop compon problem finish justifi particular method suitabl cac problem state action section develop state action model reduc state space represent suitabl cac problem cac problem formul semimarkov decis process smdp state transit control select take place discret time time one transit next continu random variabl given point time system particular configur x defin number type ongo call y number call state type random time event e occur onli one event occur time instant e gammavector indic either class call arriv call termin call turn on call turn event configur event togeth determin state system e iclass system dimension vector sinc number possibl choic e gener small compar x y size state space domin configur part state shown use nearli complet decompos approxim reduc state descriptor form stand call arriv departur event class i let configur e denot gammavector whose element equal zero except ith element whose valu uniti state associ class call arriv state associ class call departur reduct ignor number call state event call turn off give us enough accuraci cac problem shown experiment mitra et al give two reason simplif first moment call turn decis point admiss control therefor action need taken theorem appendix show ignor event call turn valid section also provid discuss similar simplif second intuit clear simplif good approxim process describ number call state reach equilibrium chang number call progress due call arrivaldepartur henc make call admiss decis number call class progress import number call class state not quantiti oscil rapidli rel call arriv departur view ignor state aggreg assum fix x qvalu chang much differ y reinforc learn call admiss control discuss section justifi reduct drop y note affect congest probabl assum process reach equilibrium correspond fix x assum sourc independ probabl configur x y given binomi distribut b fraction time class call spend state averag congest probabl class fix x thu x fdeltag indic function averag congest probabl depend x capac constraint associ conserv set set x long run averag packetlevel qo constraint alway satisfi never go state period time capac constraint violat stay forev set c c uniqu determin state space s i x e mitra et al consid aggress approach packetlevel qo constraint averag across allow configur x let xca x total system time x portion system spend x c set allow configur x xca less equal target p obvious c uniqu c c possibl c although gener conserv occas emphasi depend c c c p also write c c p c p summari choos state descriptor number class call progress e stand new class call arriv gamma class call departur event occur learner choos action feasibl event action set asfreject acceptg upon new call arriv call termin decis point action need taken symbol h tong tx brown state asfgammano action due call departuresg note action avail state s as gener depend s exampl ad new call state violat capac constraint action set state constrain fg subsequ random time anoth event occur cycl repeat revenu structur cac is task learner determin polici accept call given s maxim longrun averag revenu infinit horizon meet qo requir cac system constitut finit state space eg due capac constraint finit action space afgammag semimarkov decis process transit probabl section consid probabl model conclud larg state space classic approach base transit probabl model fea sibl theoret state transit probabl ps a probabl go state action next state deriv mitra et al depend configur x call arriv rate exact system model often infeas sever import reason first call arriv rate may depend call class also configur x dziong mason therefor call arriv rate class may constant gener second network reason size state space extrem larg exampl node link network servic type state marbach et al even possibl explicitli list state final fix model comput optim polici mean robust actual traffic condit depart assum model reason clear practic system larg state space difficult imposs determin exact transit model markov chain perform modelbas algorithm comput optim polici main motiv studi appli modelfre rl algorithm solv cac problem although explicitli comput transit probabl make follow assumpt studi let ss a continu random inter transit time state state action a probabl distribut f ss ja assumpt a assumpt a a expect ss a reinforc learn call admiss control z df ss ja particular exist assumpt a unichain condit everi stationari polici transit matrix ps s determin markov chain one ergod class possibl empti set transient state assumpt guarante transit probabl well defin assumpt guarante number transit finit time interv is almost sure finit guarante except initi transient state state reach state nonzero probabl qlearn section develop rl methodolog use paper unconstrain maxim revenu qo constraint consid section learn optim polici use watkin qlearn algorithm watkin dayan given optim qvalu q s a polici defin optim particular impli follow procedur call arriv qvalu accept call qvalu reject call determin reject higher valu drop call els accept higher valu accept call one action and qvalu exist call departur learn q s a updat valu function follow transit state action time ss a stepsiz learn rate k integ variabl index success updat ff chosen suffici close discount problem equival averag reward problem the tauberian approxim gabor et al well known qlearn robbinsmonro stochast approxim method solv socal bellman optim equat associ decis process let z e gammaff df ss ja s as assumpt guarante h contract map contract factor z e gammaff df ss ja respect maximum norm theorem suppos thatx stateact pair updat infinit number time then q k s a converg probabl q s a everi a proof see bertseka tsitsikli simplifi learn process practic issu concern implement qlearn discuss qlearn need execut everi state transit includ transit caus call departur feasibl action set one action state associ call departur necessari learn optim qvalu state induc optim polici state possibl avoid updat qvalu departur state still get optim polici reduc amount comput storag qvalu significantli sinc state space almost halv drop call departur state note interest state decis need made associ call arriv g decis point jump one arriv next arriv interarriv period may contain zero one departur given e j first arriv e case n departur two adjac arriv chapmankolmogorov equat bertseka gallag transit probabl actual decis process intermedi state correspond call departur shown appendix optim polici obtain qlearn state associ call arriv result reinforc learn call admiss control intuit sinc call departur random disturb affect state transit even though complic alreadi intract transit model smdp sinc qlearn depend explicit model asymptot converg optim polici follow explor order qlearn perform well potenti import stateact pair s a must explor specif converg theorem qlearn requir stateact pair s a tri infinit often section develop explor strategi suitabl cac problem common way tri stateact pair rl is small probabl ffl random action rather action recommend rl chosen decis point train socal fflgammarandom explor cac problem consid paper without explor state visit probabl sever order higher state experi shown fflgammarandom explor unlik help situat therefor train state visit mani time state visit time result qvalu function far converg optim polici cannot expect reason time see thi call arriv process model truncat independ mm queue system truncat system untrunc system except configur capac constraint violat elimin stationari distribut system assum greedi polici the polici alway accept new call capac constraint violat ad new call given bertseka gallag g normal constant allow set configur truncat system sinc state action determinist defin next configur x next state event part e arriv onli action need taken occur independ x probabl determin due memoryless assumpt stationari distribut state depend exampl consid experiment paramet shown tabl section below except simplifi calcul allow configur set c truncat system use peakrat alloc g use have visit state least visit state p ie five order differ stationari distribut stateact pair small system shown szepesvari converg rate qlearn approxim suitabl constant b k index defin overcom slow converg caus small valu p min pmax stationari distribut control explor scheme deriv base fact qlearn offpolici learn method sutton barto section smdp state transit thu state distri bution control choos appropri action state train one feasibl action probabl ffl control action chosen lead least visit configur fflgammadirect heurist effect reduc differ number visit state significantli speed converg valu function term qlearn formula action chosen accord explor scheme action b chosen accord current qvalu function approxim vs lookup tabl qlearn deal effect curs model an explicit state transit model need simul use instead anoth major difficulti smdp problem curs dimension the exponenti state space explos problem dimens treatment assum problem state space kept small enough lookup tabl use clearli number stateact pair becom larg lookup tabl represent infeas compact represent q repres function smaller set paramet use function approxim necessari paper choos approxim architectur correspond state aggreg consid partit state space disjoint subset gammadimension paramet vector oe whose mth compon meant approxim qvalu function state sm action a word deal piecewis constant approxim qs valu small lookup tabl use aggreg problem case shown bertseka tsitsikli qlearn converg optim polici aggreg problem function approxim use may perform well practic howev converg result state aggreg case wish avoid here proposit bertseka tsitsikli tauberian reinforc learn call admiss control approxim easi show perform loss due state aggreg bound j optim averag revenu per unit time origin aggreg problem respect defin cac state aggreg interpret featurebas architectur wherebi assign common valu oem a state s given a share common featur vector exampl featur vector may involv call class three valu indic specifi whether load call class high medium low system instead specifi precis number ongo call class x sinc state similar number call would expect similar qvalu expect small therefor state space greatli reduc lookup tabl use summari section formul cac problem smdp justifi qlearn approach solv cac problem show simplifi problem ignor detail within call process comput qvalu state decis standard fflgammarandom explor polici significantli slow learn problem simpl fflgammadirect explor strategi introduc aggreg state shown simplifi heurist follow readili problem structur next section develop method incorpor constraint framework constraint restrict maxim polici never violat qo guarante gener smdp problem constrain optim polici random stationari polici random k state problem k gammaconstraint feinberg howev modelbas linear program algorithm employ deriv polici impract cac number state larg sinc random need k state usual much smaller total number state nonrandom stationari polici learn rl often good approxim constrain optim polici gabor et al gener smdp due stochast state transit meet constraint may possibl eg state matter action taken possibl enter restrict state admiss control servic qualiti depend number call admit system ad call strictli control admiss control meet qo constraint possibl consid two import class qo constraint cac integr servic network one statedepend constraint past depend constraint conserv capac constraint exampl statedepend constraint statedepend constraint qo intrins state congest probabl function sole number call progress current state cf pastdepend constraint depend statist past histori exampl fair criterion fair depend statist reject ratio past histori address two constraint separ capac constraint simplic consid total packet congest probabl upper bound p conserv approach mean set c c p cf x state conserv capac constraint intrins properti state depend current state allow us collect qo statist state treat principl way eg comput confid interv estim current state action s n uniqu determin next configur xn project congest probabl next state n determin xn therefor forecast impact need evalu pxn expect congest probabl greater less constraint p action caus pxn action elimin feasibl action set asn cac ad new call violat capac constraint feasibl action reject new call request consid aggress capac constraint need determin set c c p allow configur defin implicitli but uniqu lim xca x total time system spend x xca x note distribut xt depend control polici again gener case differ servic type differ packetlevel qo requir easili made reinforc learn call admiss control state c c p serv possibl c usual conserv construct aggress set c p gradual decreas p c find seri set c c p c correspond chang p c clearli size c c p nonincreas decreas p c howev must alway contain c c p practic valu p c learn polici aggress congest probabl suffici close to still less constraint p search c p stop choos c p c aggress capac constraint essenc tri find correspond valu conserv threshold p c aggress threshold p construct c conserv approach way aggress capac constraint remain statedepend constraint conserv capac constraint implement constraint constrain action set state although c determin way may aggress one term revenu maxim loss optim expect small fair constraint measur reject ratio class upon nth call arriv befor nth decis made arbitrarili constraint r s n may abl find feasibl polici fair constraint involv comparison reject ratio type call formul fair constraint ii ii l maximum allow reject ratio discrep feasibl polici exist alway reject call type aggress fair constraint formul lim l sn sn a intertransit durat state n n action a formul constrain smdp problem capac constraint implement constrain feasibl action set state describ preced subsect deal fair constraint use lagrang multipli framework studi beutler ross sinc fair constraint pastdepend constraint the vector rsn depend reject ratio past hi tori fit framework need includ histori inform state descriptor new state descriptor s form gammavector req resp rej denot total number call request resp reject class current call arriv time interv last current call request origin state h tong tx brown descriptor obtain markov chain expans howev state space enlarg significantli specif due inclus req rej state space infinit must resort form function approxim solv smdp problem paper use state aggreg approxim architectur quantiz reject ratio r term lagrang multipli consid unconstrain optim parametr reward a origin reward function associ cost function associ constraint numer exist nonrandom polici solv bellman optim equat associ reward function mean time achiev equal beutler ross show constrain optim polici case optim polici exist shown constrain optim achiev random one state two nonrandom polici differ slightli undershoot resp overshoot l clearli case nonrandom constrain optim polici exist next best nonrandom polici loss optim minim reason avoid complic random polici concentr nonrandom polici studi summari section show constraint introduc problem either modul action space modifi reward function optim requir random polici sinc polici need random two state mani state greatli simplifi search restrict determinist polici simul result experi use follow model total bandwidth normal unit traffic per unit time target congest probabl p two sourc type consid properti shown tabl fair constraint averag reject ratio discrep two servic type differ l note befor hold time exponenti first concentr conserv approach capac constraint sinc explor employ ensur potenti import stateact reinforc learn call admiss control tabl experiment paramet sourc type paramet ii rate r mean period mean period call arriv rate call hold time immedi payoff pair tri natur enabl us collect statist use estim qo stateact pair emphas singl visit state suffici determin long run qo metric due variabl within call process number time stateact pair visit increas estim servic qualiti becom accur and confid gradual elimin stateact pair violat qo requir consequ valu function updat gradual correct subset stateact space sens qo requir met action within subspac state section capac constraint elimin stateact pair violat congest probabl upper limit experi use simpl way elimin stateact pair confid sinc target congest probabl total number visit configur x count number time step simul wx number congest x wx x wx conclud s a accept threshold provid close approxim confid interv brown sophist way estim px propos tong brown artifici neural network nn train base maximum likelihood principl nn estim px extrapol well p simul discount factor ff chosen gamma learn rate explor initi qvalu rl artifici set qlearn start greedi polici train complet appli test data set compar polici obtain rl altern heurist polici final qo measur obtain end rl train learn qo use test differ polici test rl polici new call arriv algorithm first determin accept call violat qo will call reject els action chosen accord arg max aa qs a rejectg qo constraint use three case peak rate alloc statist multiplex function learn onlin denot qo learn statist multiplex function given priori denot qo given examin six differ case rl qo given rl qo learn rl peak rate heurist accept call valuabl class ie type i qo given greedi qo given greedi peak rate result shown fig clear simultan qlearn qo learn converg correctli rl polici obtain give qo priori standard qlearn onli see signific gain about due statist multiplex vs vs gain due rl vs vs togeth yield increas revenu conserv peak rate alloc exampl also clear figur rl polici perform better heurist polici fig show reject ratio differ polici consid aggress approach capac constraint simul found valu p correspond aggress capac constraint p accept region ie c c c aggress conserv approach shown fig aggress accept region much larger conserv one figur number type ii user start two due insuffici measur data for confid level region that compar fig fig see aggress approach earn significantli revenu conserv approach greedi polici rl polici note peak rate alloc earn total amount reward unnorm approach fig qvalu initi rl polici start greedi polici exampl perform improv due rl signific improv due statist multiplex fair constraint impos case reject ratio two type call differ significantli fair constraint requir two reject ratio cannot differ averag test rl fair constraint set reward paramet type call type ii call keep paramet tabl unchang state befor use featurebas state aggreg cope difficulti larg state space caus fair constraint specif learn qh a instead qs a featur quantiz follow experi experienc reject ratio discrep fr quantiz level quantiz level correspond the approxim averag interarriv time although aggreg experi case complic also possibl aggreg simpler featur found simul learn rl polici compar greedi polici fair constraint accept call long fair constraint met otherwis fair constraint violat accept call class experienc highest reject ratio result shown fig fair strong constraint possibl polici gain due rl reduc expect fig see qlearn converg quickli fact rl curv figur show oscil connect learn rate reinforc learn call admiss control total reward comparison differ polici exponenti onoff rl qo given rl qo learn rl peak rate greedi type greedi qo given greedi peak rate figur comparison total reward rl learn qo capac constraint rl given qo measur rl peak rate greedi polici peak rate alloc normal greedi total reward rate greedi peak rate rl peak rate greedi qo given rl qo learn exponenti onoff figur comparison reject ratio polici learn fig a specif order qlearn converg fl k s a satisfi s a simul use small constant learn rate condit met reason adher typic prior knowledg a decreas learn rate becom small algorithm may stop make notic progress train process could becom long combin cac network rout gener issu cac rout close relat commun network combin cac rout also formul smdp howev exact character network state would requir specif number call progress class possibl rout network detail specif state intract comput assum statist independ link network dziong krishnan form decomposit network rout process singl link process usual employ dziong mason marbach et al base preced result singl link admiss control link state independ approxim propos decomposit rule allow decentr train decis make combin cac rout network also tri maxim network revenu number user class number user class ii comparison accept region aggress conserv figur comparison accept region total reward comparison differ polici exponenti onoff rl qo learn greedi qo given greedi peak rate figur comparison total reward rl learn qo capac constraint greedi polici peak rate alloca tion normal greedi total reward aggress reject rate greedi peak rate greedi qo given rl qo learn exponentialonoff figur comparison reject ratio polici learn fig reinforc learn call admiss control total reward comparison differ polici exponenti onoff rl qo learn greedi qo given figur comparison total reward obtain rl polici greedi polici capac constraint fair constraint impos normal greedi total reward reject rate figur comparison reject ratio capac constraint fair constraint polici learn fig let r denot predefin rout network action space system action due call departur rout new call rout r rg link i j node node j keep separ link state variabl whenev new call type k rout rout r contain link i j immedi reward associ link i j equal c ij satisfi ijr exampl number link along rout r qlearn perform link similarli singl link case arriv updat qvalu link i j arriv associ link new type k call origin node o destin node d decis made node follow way od set rout carri call without violat qo constraint ii defin net gain g r accept new call rout decis r ae p ijr theta figur network model admiss rout decis r ra od fg r decis make r reject call otherwis rout call rout r approach although network state simplifi link state link action space link simplifi acceptg dziong mason marbach et al import sinc link qfunction distinguish singlelink call multilink call avoid accept mani multilink call block singlelink call may bring amount revenu use less network resourc tabl experiment paramet sourc type paramet ii iii rate r call arriv rate call hold time immedi payoff present simul result obtain case network consist node unidirect link two differ class link total bandwidth unit respect indic thick thin arrow reinforc learn call admiss control fig assum three differ sourc type whose paramet given tabl call arriv node independ poisson process mean destin node randomli select among three node sourc destin node pair list possibl rout consist three entri direct path two altern hop rout emphas effect rl consid capac constraint assum peak rate alloc link simul use featurebas state aggreg approxim qvalu link learn qh r instead qs r ie number ongo call type aggreg eight level polici obtain rl compar commonli use heurist polici give direct path prioriti direct path reach capac heurist tri hop rout find one violat capac constraint rout exist call reject result given fig total fig call reject ratio fig rout behavior result show rl polici increas total revenu almost compar commonli use heurist rout polici conclus paper formul cac rout problem constrain smdp provid rl algorithm comput optim control polici incorpor two import class qo constraint statedepend pastdepend con straint rl solut maxim network revenu formul quit gener appli capac fair constraint approach experi singl link well network problem show signific improv even simpl exampl futur work includ studi combin cac rout studi function approxim neural network approxim qvalu function acknowledg work fund nsf career award ncr appendix proof simplifi learn process follow theorem show avoid learn qvalu state transit correspond call turn section call departur section let j ff cs asg set intermedi state g h tong tx brown total reward comparison differ rout polici exponenti onoff rl heurist figur comparison total reward node network normal heurist total reward reject ratio heurist rl exponenti onoff reject rate figur comparison reject ratio polici fig routingheurist portion call rout direct path direct st hop nd hop routingrl portion call rout direct path direct st hop nd hop figur comparison rout behavior polici fig reinforc learn call admiss control theorem assum as take sa step go state state optim stationari polici modifi decis process consid state also optim origin decis process proof optim polici origin problem z e gammaff df ss ja s optim polici ff modifi decis process is sinc one feasibl action as n sa due assumpt hi z e gammaff df ss ja oe first state defin z e gammaff df ss ja sinc sa finit without loss gener assum valu sa procedur similar summat term a becom hi deltad second summat second term formula due condit ps combin a a a a a z e gammaff df ss ja uniqu optim valu function easi verifi j ff therefor a a a proof eg a use memoryless properti transit process cac state serv sinc possibl call departur due capac constraint state s action a take finit number consecut call departur reach state like r adapt control constrain markov chain data network adapt statist multiplex broadband commun optim admiss control ensur qualiti servic multimedia network via reinforc learn control selfsimilar atm call traffic reinforc learn call admiss rout multiservic loss network ieee tran atm network resourc manag constrain semimarkov decis process averag reward intern confer machin learn markov decis algorithm dynam rout neurodynam approach admiss control atm network singl singl link case reinforc learn call admiss control rout integr servic network robust dynam admiss control unifi cell call qo statist multiplex reinforc learn dynam channel alloc cellular telephon system reinforc learn asymptot convergencer qlearn advanc nip estim loss rate integr servic network neural network adapt call admiss control qualiti servic con straint reinforc learn solut tr