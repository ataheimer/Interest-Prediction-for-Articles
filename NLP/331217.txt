t effici algorithm blockcycl array redistribut processor set a abstractruntim array redistribut necessari enhanc perform parallel program distribut memori supercomput paper present effici algorithm array redistribut cyclicx p processor cyclickx q processor algorithm reduc overal time commun consid data transfer commun schedul index comput cost propos algorithm base gener circul matrix formal algorithm gener schedul minim number commun step elimin node content commun step network bandwidth fulli util ensur equals messag transfer commun step furthermor time comput schedul index set significantli smaller take omaxpq time less percent data transfer time comparison schedul comput time use stateoftheart scheme which base bipartit match scheme percent data transfer time similar problem size therefor propos algorithm suitabl runtim array redistribut evalu perform scheme implement algorithm use c mpi ibm sp result show algorithm perform better previou algorithm respect total redistribut time includ time data transfer schedul index comput b introduct mani high perform comput hpc applic includ scientif comput signal process consist sever stage exampl applic includ multidimension fast fourier transform altern direct implicit adi method solv twodimension diffus equat linear algebra solver execut applic distribut memori supercomput data distribut need stage reduc perform degrad due remot memori access program execut proce one stage anoth data access pattern number processor requir exploit parallel applic may chang chang usual caus data distribut stage unsuit subsequ stage data redistribut reloc data distribut memori reduc remot access overhead sinc paramet redistribut gener unknown compil time runtim data redistribut necessari howev cost redistribut offset perform benefit achiev redistribut therefor runtim redistribut must implement effici ensur overal perform improv array data typic distribut blockcycl pattern onto given set processor blockcycl distribut block size x denot cyclicx block contain x consecut array element block assign processor roundrobin fashion distribut pattern cyclic block distribut special case blockcycl distribut gen eral blockcycl array redistribut problem reorgan array one blockcycl distribut anoth ie cyclicx cyclici import case problem redistribut cyclicx cyclickx aris mani scientif signal process applic type data redistribut occur within processor set differ processor set data redistribut given initi layout final layout consist four major step index comput commun schedul messag pack unpack final data transfer four step contribut total array redistribut cost briefli explain four cost associ data redistribut array element belong comput local memori locat local index array element local index use pack array element messag similarli destin processor determin sourc processor indic receiv messag comput local indic find locat receiv messag store total time comput indic denot index comput cost schedul comput cost commun schedul specifi collect senderreceiv pair commun step sinc commun step processor send one messag processor receiv one messag care schedul requir avoid content minim number commun step time comput commun schedul signific reduc cost import criteria perform runtim redistribut messag packingunpack cost sender messag consist word differ memori locat need gather buffer send node typic requir memori read memori write oper gather data form compact messag buffer time perform data gather sender messag pack cost similarli receiv side messag unpack data word need store appropri memori locat data transfer cost data transfer cost commun step consist startup cost transmiss cost startup cost incur softwar overhead commun oper total startup cost reduc minim number messag transfer step transmiss cost incur transfer bit network depend network bandwidth tabl summar key featur well known data distribut algorithm literatur known algorithm ignor one cost scheme focu effici index set comput complet ignor schedul commun event base index block scheme focu find destin processor gener messag destin commun schedul consid lead node content perform commun inturn lead higher data transfer cost node incur addit delay scheme elimin node content explicitli schedul commun event although scheme effici schedul algorithm design data redistribut processor set redistribut differ processor set caterpillar algorithm propos use simpl round robin schedul avoid node content key featur schedul index comput commun pitfal commun schedul index comput use line segment node content occur minim transmiss cost commun schedul effici index comput sourc destin set node content occur minim transmiss cost caterpillar simpl schedul algorithm index comput scan array segment node content minim transmiss cost number commun step bipartit match scheme larg schedul comput overhead schedul comput time node content stepwis strategi minim number commun step greedi strategi minim transmiss cost scheme fast schedul index comput schedul comput time node content minim number commun step data transfer cost tabl comparison variou scheme array redistribut howev algorithm fulli util network bandwidth ie size data sent node commun step vari node node lead increas data transfer cost scheme reduc data transfer cost howev schedul comput cost signific bipartit graph match use take op state oftheart workstat time rang s msec p q interest problem interest schedul comput cost larger data transfer cost algorithm optim data transfer cost number commun step non alltoal commun case which one three case occur perform redistribut consid here algorithm optim data transfer cost alltoal commun case differ messag size optim data transfer cost necessari transfer messag equal size commun step paper propos novel effici algorithm data redistribut cyclicx p processor cyclickx q processor algorithm use optim number commun step fulli util network bandwidth step commun schedul determin use gener circul matrix framework schedul comput cost q implement show schedul comput time rang s sec p q rang processor comput index set commun schedul use set equat deriv gener circul matrix formul experiment result show schedul comput time neglig compar data transfer cost array size interest messag packingunpack cost scheme gener optim commun schedul thu scheme minim total time data redistribut make scheme attract runtim well compiletim data redistribut techniqu use implement scalabl redistribut librari implement direct hpf develop parallel algorithm supercomput applic particular techniqu lead effici distribut corner turn oper commun kernel need parallel signal process applic redistribut scheme implement use mpi c easili port variou hpc platform perform sever experi illustr improv perform compar stateoftheart experi perform determin data transfer schedul index comput cost one experi use processor ibm sp partit sourc processor destin processor expans factor set array size vari mbyte mbyte compar caterpillar algorithm data transfer time lower ratio data transfer time algorithm caterpillar algorithm schedul comput time propos algorithm much less bipartit match scheme p q schedul comput time bipartit match scheme s msec algorithm s sec exampl schedul comput time use bipartit match scheme msec time use algorithm sec rest paper organ follow section explain tablebas framework also discuss gener circul matrix formal deriv conflict free commun schedul section explain redistribut algorithm index comput section report experiment result ibm sp conclud remark made section a array a n c cyclic q processor b cyclic p processor figur blockcycl redistribut array point view a array element b p processor c cyclicx p processor cyclickx q processor exampl approach redistribut section present approach blockcycl redistribut problem subsect discuss two view redistribut illustr concept superblock follow subsect explain tablebas framework redistribut use destin processor tabl column row reorgan subsect discuss gener circul matrix formal allow us comput commun schedul effici array processor point view blockcycl distribut cyclicx array defin follow given array n element p processor block size x array element partit contigu block x element each th block b consist array element whose indic vari ix i x block distribut onto processor roundrobin fashion block b assign processor j p j paper studi problem redistribut cyclicx p processor cyclickx q processor denot x array point view element array shown along singl horizont axi processor indic mark block redistribut x q period found p processor cyclic q processor c initi distribut tabl f final distribut tabl f b b b b a initi layout b d final layout figur blockcycl redistribut cyclicx p processor cyclickx q processor processor point view exampl block movement pattern exampl figur b b b b initi assign p move q respect that b p move q again find commun pattern b b repeat block collect block call superblock period block movement pattern kq size superblock figur superblock size lcm delta next superblock block b b move fashion processor point view blockcycl distribut repres dimension tabl column correspond processor row index local block index entri tabl global block index therefor element i j tabl repres th local block j th processor figur show exampl global block index destin processor index figur exampl destin processor tabl t processor point view block distribut tabl roundrobin fashion tabl correspond sourc processor denot initi layout repres block initi assign sourc processor similarli final layout repres block assign destin processor problem redistribut block initi layout final layout layout shown figur a d respect initi layout partit collect row size l similarli final layout partit disjoint collect row collect row note collect correspond superblock block locat rel posit within superblock move way redistribut block transfer singl commun step mpi deriv data type handl block singl block without loss gener consid first superblock follow illustr algorithm refer tabl repres indic block within first superblock initi final layout initi distribut tabl final distribut tabl f shown figur c f respect cyclic redistribut problem essenti involv reorgan block within superblock initi distribut tabl final distribut tabl f tablebas framework redistribut given redistribut paramet p k q block locat f determin redistribut block move initi locat final locat f thu processor ownership local memori locat block chang redistribut redistribut conceptu consid tabl convers process f decompos independ column row reorgan column row reorgan reorgan column reorgan figur tabl convers process redistribut column reorgan block rearrang within column tabl therefor local oper within processor memori row reorgan block within row rearrang oper therefor lead chang ownership block requir interprocessor commun destin processor block initi distribut tabl determin redistribut paramet global block index send commun event tabl construct replac block index initi distribut tabl destin processor index shown figur denot destin processor tabl dpt t i th entri destin processor index th local block sourc processor j consid one superblock l theta p matrix row correspond commun step algorithm commun step processor send data atmost one destin processor q p atmost p processor destin processor set receiv data destin processor remain idl commun step therefor commun step p commun pair hand q destin processor receiv data time maximum number commun pair commun step minp q without loss gener follow discuss assum q p figur show tablebas framework redistribut convert initi distribut tabl final distribut tabl f dpt use but use commun schedul effici lead node content sinc sever processor tri send data destin processor commun step exampl figur step sourc processor tri commun destin processor howev everi row consist p distinct destin processor indic among node content avoid commun step motiv column reorgan elimin node content dpt reorgan column reorgan reorgan tabl call send commun schedul tabl s section discuss reorgan perform l theta p matrix well entri destin processor index row correspond contentionfre commun step maintain correspond t set column reorgan appli result distribut tabl correspond s commun step block row transfer destin processor specifi correspond entri s refer figur first commun step sourc processor transfer block destin processor respect specifi s step call row reorgan distribut tabl f correspond receiv block destin processor reorgan final distribut tabl f anoth set column reorgan for exampl need oper receiv block store memori locat destin processor key idea choos requir row reorganizationscommun event perform effici support easytocomput contentionfre commun schedul far discuss redistribut problem cyclicx p processor cyclickx q processor dual relationship exist problem cyclicx p processor cyclickx q processor problem cyclickx p processor cyclicx processor redistribut cyclickx p processor cyclicx q processor redistribut revers direct redistribut x q send receiv commun schedul tabl receiv send commun schedul tabl q therefor scheme x extend redistribut problem cyclickx p processor cyclicx q processor commun schedul use gener circul matrix framework commun schedul perform local rearrang data within processor well interprocessor commun local rearrang data call column reorgan result send commun schedul tabl s show p k q send commun schedul inde gener circul matrix avoid node content matrix circul matrix satisfi follow properti n row row circularli right shift k time k m shift l time note definit extend block circul matric chang row row block matrix gener circul matrix matrix partit block size theta n result block matrix form circul matrix block either circul matrix gener circul matrix figur illustr gener circul matrix two observ gener circul matrix i block along block diagon ident ii element row distinct row element distinct show approach destin processor tabl transform gener circul matrix distinct element row effici redistribut algorithm discuss commun schedul algorithm redistribut classifi commun pattern class redistribut problem x altern formul cyclicx cyclici problem accord follow lemma let g denot kq gener circul matrix circul matrix submatrix figur gener circul matrix lemma commun pattern induc x p k q requir i non alltoal commun commun fix messag size k ffg ff integ greater iii alltoal commun differ messag size g k k ffg among three case case alltoal processor commun messag size optim schedul use trivial roundrobin schedul howev non trivial achiev messag size pair node commun step alltoal case differ messag size therefor focu two case redistribut requir schedul non alltoal commun alltoal commun differ messag size non alltoal commun given redistribut paramet p q k get l theta p initi distribut tabl dpt t let g dpt t everi k th row similar pattern differ destin processor indic shuffl row row similar pattern adjac result shuffl dpt shuffl dpt divid q slice row direct divid p slice column direct now dpt consid k theta p block matrix made q theta g submatric block matrix convert gener circul matrix reorgan block column block matrix reorgan individu column within submatrix appropri amount result gener circul matrix commun schedul matrix s a b c figur step column reorgan procedur k ident valu row dpt distribut k distinct row henc row distinct valu sinc gener circul matrix element row distinct achiev conflictfre schedul rigor proof fact dpt transform gener circul matrix use column reorgan found schedul commun step p sourc processor transfer equals messag p distinct destin processor ensur network bandwidth fulli util number commun step also minim therefor data transfer cost minim reorgan element move within column so incur interprocessor commun figur show exampl dpt x convert gener circul matrix form column reorgan exampl figur a show initi distribut tabl figur d show correspond dpt t row shuffl shown figur b e partit shuffl tabl submatric size theta diagon submatric diagon element submatrix shown figur c f figur f gener circul matrix give commun schedul dpt convert send commun schedul tabl s set reorgan appli initi data distribut tabl convert shown figur expens reorgan larg amount data within local memori instead reorgan done maintain pointer element array sourc processor tabl point data block pack commun step denot send data locat tabl entri local block index correspond entri entri s si j point destin processor correspond entri i j scheme comput schedul data index set time algebra manipul procedur give follow two equat directli comput individu entri equat denot quotient integ divis remaind integ divis similarli n solut nk proof correct mathemat formul found formula comput commun schedul index set redistribut extrem effici compar method present use bipartit match algorithm furthermor use formula processor comput entri need send commun schedul tabl henc schedul index set comput perform distribut way total cost comput schedul index set q amort cost comput step commun schedul index set comput o scheme minim number commun step avoid node content commun step equals messag transfer therefor scheme minim total data transfer cost alltoal commun differ messag size alltoal commun case aris g g g state lemma g q first superblock dpt construct dpt therefor column entri q destin processor column sever block transfer destin processor tabl send commun schedul tabl s transform dpt messag messag figur exampl illustr alltoal case differ messag size x destin column reorgan state section appli dpt t result gener circul matrix k theta p circul block matrix block q theta g submatrix also circul matrix block matrix first g block column distinct block everi g th row entri differ circularshift pattern block fold onto block first row therefor first g row block matrix use determin send commun schedul tabl s q theta p gener circul matrix sinc block everi g th row fold onto block first row alltoal commun case differ messag size block first k mod g row size k e whole block remain row size b k c figur show exampl send commun schedul tabl x gener alltoal case differ messag size exampl processor entri destin processor correspond dpt l theta p matrix l appli column reorgan result gener circul matrix consid k theta p block matrix k block first g use tabl rd row fold onto st row henc messag size st row nd row k multipl g messag size everi row same therefor network bandwidth fulli util send equal size messag commun step data transfer cost distribut memori model commun cost two paramet startup time transfer time startup time incur commun event independ messag size gener startup time consist transfer request acknowledg latenc context switch latenc latenc initi messag header unit transmiss time cost transfer messag unit length network total transmiss time messag proport size thu total commun time send messag size unit one processor anoth model model reorgan data element among processor processor unit data anoth processor also take time model assum node content ensur commun schedul redistribut use distribut memori model perform algorithm analyz follow assum array n element initi distribut cyclicx p processor redistribut cyclickx q processor use algorithm commun cost perform x i l case non alltoal commun ii case alltoal commun proof analysi found experiment result experi conduct ibm sp algorithm written c mpi tabl show comparison propos algorithm caterpillar algorithm bipartit match scheme respect data transfer cost schedul index comput cost alltoal commun case equals messag data transfer cost commun step three algorithm also schedul comput perform simpl way henc consid tabl tabl size array assign sourc processor non alltoal commun case p algorithm well bipartit match scheme perform less number commun step compar caterpillar algorithm alltoal commun case differ messag size messag transmit commun step size bipartit match scheme well algorithm therefor network bandwidth fulli util total transmiss cost caterpillar algorithm transmiss cost commun step domin largest messag transfer step let denot size largest messag sent commun step i note total startup cost algorithm qt sinc number commun step same hand total transmiss cost tabl comparison data transfer cost schedul index comput cost caterpillar algorithm bipartit match scheme algorithm note where l q non alltoal commun case maximum transfer data size commun step i non alltoal commun alltoal commun differ messag size data transfer cost schedul index comput cost data transfer cost schedul index comput cost caterpillar algorithm bipartit match scheme algorithm p q q bipartit match scheme algorithm less caterpillar algorithm caterpillar algorithm well algorithm perform schedul index comput oq time howev schedul index comput cost bipartit match scheme op evalu total redistribut cost data transfer cost consid differ scenario correspond rel size p q scenario p q scenario scenario experi choos array consist singl precis integ size element byte array size chosen multipl size superblock avoid pad use dummi data rest section organ follow subsect report experiment result overal redistribut time algorithm caterpillar algorithm subsect show experiment result data transfer time algorithm caterpillar algorithm subsect compar algorithm bipartit match scheme respect schedul comput time j jn j ts redistribut routin comput schedul index set i in i sourc processor sourc processor pack messag send messag destin processor els destin processor receiv messag sourc processor unpack messag ts comput tavg node_tim node comput figur step measur redistribut time total redistribut time subsect report experiment result total redistribut time algorithm caterpillar algorithm total redistribut time consist schedul comput time index comput time packingunpack time data transfer time experi sourc destin processor set disjoint commun step sender pack messag send receiv unpack messag receiv it pack oper sourc processor unpack oper destin processor overlap ie send messag commun step i sender start pack messag commun step i receiv start unpack messag receiv step i methodolog measur total redistribut time shown figur time measur use mpiwtim call n number run run execut redistribut number commun step processor measur nodetimej j th run gener sourc destin processor perform interprocessor commun last step complet redistribut earlier processor receiv messag unpack it barrier synchron mpibarri perform end redistribut measur nodetim averag nodetim p processor total array sizetot redistribut time msec ibm sp algorithm caterpillar algorithm total array sizetot reditribut time msec ibm sp algorithm caterpillar algorithm total array sizetot redistribut time msec ibm sp algorithm caterpillar algorithm total array sizetot redistribut time msec ibm sp algorithm caterpillar algorithm a maximum time b averag time c median time d minimum time figur maximum averag median minimum total redistribut time comput save tavg measur valu store array t shown figur redistribut perform n time maximum minimum median averag total redistribut time comput n run experi n set figur total redistribut time algorithm caterpillar algorithm compar ibm sp experi node use sourc processor destin processor total number array element in singl precis vari mbyte mbyte k set figur a show maximum time tmax figur observ larg varianc measur valu figur b show result averag time tavg figur figur c show result use median time tmed figur still varianc measur valu howev smaller varianc found averag maximum time figur d show minimum time redistribut tmin figur plot accur observ redistribut time sinc minimum time smallest compon due os interfer effect relat environ remain plot section show tmin onli redistribut non alltoal commun case non alltoal commun case messag commun step size total number commun step algorithm caterpillar algorithm therefor redistribut time algorithm theoret caterpillar algorithm experiment result shown figur d redistribut time algorithm caterpillar algorithm figur show sever experiment result non alltoal commun case figur a b c show result use number commun step use algorithm respect number commun step use caterpillar algorithm therefor redistribut time algorithm expect reduc compar caterpillar algorithm experiment result confirm these similar reduct time achiev experiment result shown figur figur compar overal redistribut time alltoal commun case differ messag size figur a report experiment result array size vari mbyte mbyte case total redistribut time msec ibm sp algorithm caterpillar algorithm total array size total redistribut time msec ibm sp algorithm caterpillar algorithm total array size total redistribut time msec ibm sp algorithm caterpillar algorithm total array size total redistribut time msec ibm sp algorithm caterpillar algorithm total array sizetot redistribut time msec ibm sp algorithm caterpillar algorithm total array size total redistribut time msec ibm sp algorithm caterpillar algorithm a c figur total redistribut time non alltoal commun case total redistribut time msec ibm sp algorithm caterpillar algorithm total array sizetot redistribut time msec ibm sp algorithm caterpillar algorithm total array size total redistribut time msec ibm sp algorithm caterpillar algorithm total array size total redistribut time msec ibm sp algorithm caterpillar algorithm c figur total redistribut time alltoal commun case differ messag size j jn j redistribut routin comput schedul index set i in i sourc processor sourc processor pack messag ts send messag destin processor ts els destin processor ts receiv messag sourc processor ts unpack messag comput tavg node_tr node comput figur step measur data transfer time algorithm number step within superblock half messag two block half one block algorithm equals messag transfer commun step therefor half step two block messag sent half one block messag sent caterpillar algorithm attempt schedul commun oper send equals messag therefor redistribut time step determin time transfer largest messag theoret total redistribut time algorithm reduc compar caterpillar algorithm experi achiev reduct redistribut time array size small algorithm approxim perform sinc startup cost domin overal data transfer cost array size increas reduct time perform distribut use algorithm improv scenario obtain similar result see figur b c d data transfer time subsect report experiment result data transfer time algorithm caterpillar algorithm experi perform manner discuss subsect data set use experi use previou subsect data transfer time commun step first measur total data transfer time comput sum measur time commun step methodolog measur time shown figur figur data transfer time algorithm caterpillar algorithm report experi perform ibm sp figur a report maximum data transfer time tmax figur larg variat measur valu observ figur b c show averag time tavg figur median time tmed figur data transfer time respect valu comput use maximum timetmax figur d show minimum data transfer timetmin plot accur observ data transfer time sinc minimum time smallest compon due os interfer effect relat environ therefor accur comparison rel perform redistribut algorithm remaind section show plot correspond tmin onli redistribut non alltoal commun case messag commun step size total number commun step use algorithm total number step use caterpillar algorithm therefor data transfer time algorithm theoret caterpillar algorithm experiment result see figur d redistribut time algorithm caterpillar algorithm figur show sever experiment result non alltoal commun case similar reduct time achiev experi figur report experiment result alltoal commun case differ messag size data transfer time alltoal commun case sensit network content sinc everi sourc processor commun everi destin processor algorithm number step within superblock half messag two block half one block algorithm equals messag transfer commun step therefor half step two block messag sent one block messag sent half caterpillar algorithm attempt send equals messag commun step therefor data transfer time step determin time transfer largest messag theoret data transfer time algorithm reduc compar caterpillar data transfer time msec ibm sp algorithm caterpillar algorithm total array sizedata transfer time msec ibm sp algorithm caterpillar algorithm total array sizedata transfer time msec ibm sp algorithm caterpillar algorithm total array size data transfer time msec ibm sp algorithm caterpillar algorithm a maximum time b averag time c median time d minimum time figur maximum averag median minimum data transfer time data transfer time msec ibm sp algorithm caterpillar algorithm total array size data transfer time msec ibm sp algorithm caterpillar algorithm total array size data transfer time msec ibm sp algorithm caterpillar algorithm total array size data transfer time msec ibm sp algorithm caterpillar algorithm total array sizedata transfer time msec ibm sp algorithm caterpillar algorithm total array size data transfer time msec ibm sp algorithm caterpillar algorithm a c figur data transfer time non alltoal commun case data transfer time msec ibm sp algorithm caterpillar algorithm total array sizedata transfer time msec ibm sp algorithm caterpillar algorithm total array sizedata transfer time msec ibm sp algorithm caterpillar algorithm total array sizedata transfer time msec ibm sp algorithm caterpillar algorithm c figur data transfer time alltoal commun case differ messag size tabl comparison schedul comput time sec procedur use bipartit match p q k scheme bipartit match scheme algorithm experi larg messag size achiev reduct small messag algorithm approxim perform sinc startup time domin data transfer time experiment result report figur b c d schedul comput time time comput schedul caterpillar algorithm well algorithm neglig compar total redistribut time even though schedul caterpillar algorithm simpler our caterpillar algorithm need time index comput identifi block pack commun step time approxim schedul comput time schedul comput time bipartit match scheme much higher caterpillar algorithm algorithm rang hundr msec quit signific schedul comput cost bipartit match scheme increas rapidli number processor increas hand algorithm comput commun schedul effici processor comput entri send commun schedul tabl thu schedul comput distribut way schedul comput time rang s sec comparison scheme bipartit match scheme respect schedul comput time shown tabl here time scheme includ index comput time bipartit match scheme time shown schedul comput time onli conclus paper show effici algorithm perform redistribut cyclicx p processor cyclickx q processor propos algorithm repres use gener circul matrix formal algorithm minim number commun step avoid destin node content commun step network bandwidth fulli util ensur messag size transfer commun step therefor total data transfer cost minim schedul index comput cost also import perform runtim redistri bution algorithm schedul index set comput omaxp q time comput extrem fast compar bipartit match scheme take op schedul index comput time small enough neglig compar data transfer time make algorithm suitabl runtim data redistribut acknowledg would like thank staff mhpcc assist evalu algorithm ibm sp also would like thank manash kirtania assist prepar manuscript r redistribut blockcycl data distribut use mpi processor map techniqu toward effici data redistribut schedul blockcycl array redistribut parallel matrix transpos algorithm distribut memori concurr comput parallel implement synthet apertur radar high perform comput platform fast runtim block cyclic data redistribut multipro cessor messag pass interfac forum runtim array redistribut hpf program effici algorithm array redistribut automat gener effici array redistribut routin distribut memori multicomput compil techniqu blockcycl distribut multiphas array redi tribut model evalu multiphas array redi tribut model evalu approach commun effici data redistribut commun issu heterogen embed system basiccycl calcul techniqu effici dynam data redistribut scalabl portabl implement spacetim adapt process effici algorithm blockcycl redistribut array shortest augment path algorithm dens spars linear assign problem tr ctr stavro souravla mano roumelioti pipelin techniqu dynam data transfer multiprocessor grid intern journal parallel program v n p octob wang minyi guo dame wei divideandconqu algorithm irregular redistribut parallel compil journal supercomput v n p august jihwoei huang chihp chu effici commun schedul method processor map techniqu appli data redistribut journal supercomput v n p septemb ian n dunn gerard g l meyer qr factor share memori messag pass parallel comput v n p novemb chinghsien hsu shihchang chen chaoyang lan schedul contentionfre irregular redistribut parallel compil journal supercomput v n p june chinghsien hsu spars matrix blockcycl realign distribut memori machin journal supercomput v n p septemb emmanuel jeannot frdric wagner schedul messag data redistribut experiment studi intern journal high perform comput applic v n p novemb minyi guo yi pan improv commun schedul array redistribut journal parallel distribut comput v n p may chinghsien hsu kunm yu compress diagon remap techniqu dynam data redistribut band spars matrix journal supercomput v n p august minyi guo ikuo nakata framework effici data redistribut distribut memori multicomput journal supercomput v n p novemb