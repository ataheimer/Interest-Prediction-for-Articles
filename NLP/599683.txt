t model select error estim a studi model select strategi base penal empir loss minim point tight relationship error estim databas complex penal good error estim may convert databas penalti function perform estim govern qualiti error estim consid sever penalti function involv error estim independ test data empir vc dimens empir vc entropi marginbas quantiti also consid maxim differ error first half train data second half expect maxim discrep close relat capac estim calcul mont carlo integr maxim discrep penalti function appeal pattern classif problem sinc comput equival empir risk minim train data label flip b introduct consid follow predict problem base random observ one estim y predict rule measur bound loss function data consist sequenc independ ident distribut sampl distribut x n independ x goal choos predict rule f n restrict class f loss lf close possibl best possibl loss inmum taken predict rule empir risk minim evalu perform predict rule f f term empir loss b l n provid estim whose loss close optim loss l class f i sucient larg loss best function f close l ii sucient small nding best candid f base data still possibl two requir clearli con ict tradeo best understood write rst term often call estim error second approxim error often f larg enough minim l possibl distribut x f larg empir risk minimiza tion case common x advanc sequenc smaller model whose union equal f given data n one wish select good model one class problem model select denot b f k function f k minim empir risk one hope select model class fk excess error el close min idea structur risk minim also known complex regular izat add complex penalti b f k s compens overt eect penalti usual close relat distributionfre upper bound sup ff k penalti elimin eect overt thu structur risk minim nd best tradeo approxim error distributionfre upper bound estim error unfortun distributionfre upper bound may conserv specic distribut critic led idea use datadepend penalti next section show approxim upper bound error includ datadepend bound use dene possibl data depend complex penalti c n k model select algorithm excess error close min section give sever applic perform bound section section consid estim provid independ test sampl disadvantag cost data section consid distributionfre estim base vc dimens datadepend estim base shatter coecient unfortun dicult comput section brie consid marginbas error estim view easili comput estim quantiti analog shatter coecient section look estim provid maxim discrep error rst half sampl second half classic estim conveni comput simpli minim empir risk half label ip section look complex estim expect maximum discrep estim calcul mont carlo integr lead better perform bound section review concentr inequ central proof final section oer experiment comparison propos method clariti includ tabl notat use throughout paper work complex regular see akaik barron bar ron birg massart barron cover birg massart buescher kumar devroy gyor lugosi gallant geman hwang kearn mansour ng ron krzy_zak linder lugosi nobel lugosi zeger mallow meir modha masri rissanen schwarz shaw taylor bartlett williamson anthoni shen wong vapnik f predict rule set predict rule model class f union model class f k f k element f k minim loss element f k minim empir loss predict rule f minim loss loss minim loss function f k l b l n empir loss r nk estim high condenc upper bound loss l b penal loss estim l loss optim predict rule tabl notat vapnik chervonenki yang barron datadepend penalti studi bartlett freund kolt chinskii koltchinskii panchenko lozano lugosi nobel massart shawetaylor bartlett williamson anthoni penal error estim class f k let b f k denot predict rule select f k base data goal select among rule one approxim minim loss key assumpt analysi true loss b f k estim k assumpt everi n posit number c k estim r nk l b avail satis ce notic c might depend sampl size n dene databas complex penalti r log k last term requir technic reason becom appar shortli typic small dierenc r nk simpli estim right amount penal l b final dene predict rule r log k follow theorem summar main perform bound f n theorem assum error estim r nk satisfi posit constant c m ce moreov k b minim empir loss model class f k r logc second part theorem show predict rule minim penal empir loss achiev almost optim tradeo approxim error expect complex provid estim r nk complex base approxim upper bound loss particular knew advanc class f k contain optim predict rule could use error estim r nk obtain upper bound el upper bound would improv bound theorem rang loss function innit set inmum empir loss might achiev case could dene b f k suitabl good approxim inmum howev conveni assum throughout minimum alway exist suce thi variou proof assum n close proof breviti introduc notat sup by union bound r log j by denit ce by assumpt ce sinc prove second inequ k decompos lf n l k rst term may bound standard integr tail inequ shown see eg page e logcem choos f k lf k second term may bound directli by denit minim empir loss f k last step follow fact e sum obtain bound term yield k logcem impli second statement theorem sometim bound tighter assumpt avail assumpt below bound may exploit decreas term log km denit complex penalti assumpt everi n posit number c k estim r nk l b avail satis ce dene modi penalti dene predict rule trivial modic proof theorem obtain follow result theorem assum error estim r nk satisfi assumpt posit constant c m moreov k b minim empir loss model class f k logec far concentr expect loss penal estim howev easi modic proof obtain exponenti tail inequ work one inequ scenario theorem theorem assum error estim r nk satisfi posit constant c m k b minim empir loss model class f k r log k proof note r log k r log k sup r log k by rst inequ theorem r log k by union bound denit r log k minim empir loss f k e n log kn by hoed inequ conclud proof exampl shown concentr expect loss penal empir error minim tail probabl estim may obtain case simpl applic theorem abov applic independ test sampl assum independ sampl pair avail simpli remov sampl train data cours attract may small rel n case estim l b appli hoed inequ show assumpt satis appli theorem give follow result corollari assum model select algorithm section perform holdout error estim min r log k word estim achiev nearli optim balanc approxim error quantiti may regard amount overt inequ recov main result lugosi nobel much simpler estim fact bound corollari may substanti improv main result squar root bound corollari remov increas penalti term small constant factor use bernstein inequ place hoed follow choos modi estim r nk m posit constant bernstein inequ see eg yield thu satis replac m therefor dene obtain perform bound estim complex remain exampl consid error estim r nk avoid split data simplic concentr section case classic loss dene argument may carri gener case well recal basic vapnikchervonenki inequ sup n empir shatter coecient f k is number dierent way n point classi element f k easi show inequ impli estim r log es k x n assumpt need estim quantiti log es k x n simplest way use fact es k x n vc dimens f k substitut theorem min r log r rn type distributionfre result mention introduct interest result involv estim es k x n k x n theorem assum model select algorithm section use r min r e log k x n r log k key ingredi proof concentr inequ random vc entropi log k x n proof need check valid assumpt shown n satis condit theorem below first note es k x n log es k x n log last inequ theorem therefor r e log k x n sup r log es k x n use vapnikchervonenki inequ follow r r e log k x n r log k x n r e log k x n r log k x n r e log k x n last term may bound use theorem follow r r e log k x n log expb expb log c exp summar therefor assumpt satis appli theorem nish proof eectiv vc dimens margin practic may dicult comput valu random shatter coecient k x n altern way assign complex may easili obtain observ k x n empir vc dimens class f k is vc dimens restrict point immedi estim r log assumpt way estim theorem in fact care analysi possibl get rid log n factor price increas constant unfortun comput k gener still dicult lot eort devot obtain upper bound k simpl comput bound handi framework sinc upper bound may immedi convert complex penalti particular marginsbas upper bound misclass probabl neural network support vector machin convex combin classier immedi give complex penalti and theorem perform bound recal fact basi theori support vector machin see bartlett shawetaylor cristianini shawetaylor vapnik refer therein model class f call class gener linear classier exist function f class linear classier r p is class predict rule form w r p weight vector satisfi much theori support vector machin build fact eectiv vc dimens gener linear classier minim distanc correctli classi data point separ hyperplan larger certain margin may bound independ linear dimens p function margin constant say linear classier correctli classi x margin recal follow result lemma bartlett shawetaylor let f n arbitrari possibl data depend linear classier form w n r p weight vector satisfi kw posit random variabl let k n posit integ valu random variabl k x k r correctli classi k n data point x margin sn assum b f minim empir loss class f gener linear classier correctli classi least n k data point margin applic lemma show take sn obtain f sn rm log sn use inequ inequ show model class f k class gener linear classier class error estim r nk dene abov condit satis theorem may use result obtain follow perform bound theorem sn kk r log k k r k random variabl k correspond class f k import result lie fact give comput feasibl way assign datadepend penalti linear classier hand estim r nk may much inferior estim studi previou section penal maxim discrep section propos altern way comput penalti improv perform guarante new penalti may still dicult comput ecient better chanc obtain good approxim quantiti dene solut optim problem assum simplic n even divid data two equal halv dene predictor f empir loss two part l use notat section dene error estim r nk l loss function loss ie maximum discrep l may comput use follow simpl trick rst ip label rst half data thu obtain modi data set x next nd f k f k minim empir loss base l clearli function f k maxim discrep therefor algorithm use comput empir loss minim b may use nd f k comput penalti base maximum discrep appeal although empir loss minim often comput dicult approxim optim algorithm use nding predict rule estim appropri penalti particular algorithm approxim minim empir loss class f k minim proper subset f k theorem still applic et al consid similar quantiti case pattern classic motiv bound similar elf n b dene eectiv vc dimens obtain choos valu vc dimens give best bound experiment estim elf n b show linear classier xed dimens varieti probabl distribut good suggest model select strategi estim elf n use bound follow theorem justi direct approach use discrep train data directli rather use discrep rang sampl size estim eectiv vc dimens show independ test sampl necessari similar estim consid although error bound present theorem nontrivi maximum discrep neg theorem penalti dene use maximumdiscrep error estim min l r log k proof again check assumpt appli theorem introduc ghost sampl x n independ data distribut denot empir loss base sampl proof base simpl observ k thu k l sup l sup l sup l now dierenc supremum maximum satis condit mcdiarmid inequ see theorem below c probabl exp n thu assumpt satis proof nish random complex estim section introduc altern way estim quantiti may serv eectiv estim complex model class f maximum discrep estim previou section split data two halv oer altern allow us deriv improv perform bound consid expect random split data two set maxim discrep koltchinskii consid similar estim prove bound analog theorem below improv bound theorem sequenc iid random variabl pf s independ data n introduc quantiti sup n use nk measur amount overt class f k note nk known may comput arbitrari precis montecarlo simul case pattern classic comput integr involv minim empir loss sampl randomli ip label oer two dierent way use estim model select rst base theorem second slight modic theorem start simpler version theorem let dene error estim r nk choos f n minim penal error estim r log k r log k proof introduc ghost sampl proof theorem recal symmetr trick gine zinn sup sup n sup sup sup rest proof assumpt follow easili concentr in equal k sup sup sup by last step use mcdiarmid inequ it easi verifi dierenc supremum nk satis condit theorem c assumpt hold theorem impli result concentr inequ concentrationofmeasur result central analysi inequ guarante certain function independ random variabl close mean recal three inequ use proof theorem mcdiarmid let x independ random variabl take valu set a assum f n r satis sup c mcdiarmid inequ conveni f varianc situat varianc f much smaller follow inequ might appropri theorem boucheron lugosi massart suppos independ random variabl take valu set a r exist function r x moreov log experiment comparison empir penal criteria learn problem section report experiment comparison propos model select rule setup propos kearn mansour ng ron toy problem x s drawn uniform distribut interv class f k dene class function f f k exist partit f constant interv straightforward check vcdimens f k k follow assum target function f belong f k unknown k label exampl x obtain ip valu denot nois level clearli function g make simpl learn problem especi conveni experiment studi fact comput minima empir loss min ff k perform time on log n use dynam program algorithm describ lozano also report experiment comparison model select method problem paper studi sever penal model select techniqu holdout or crossvalid method base independ test sampl penal base empir vc entropi maximum discrep estim random complex estim investig learn problem easi see empir vc entropi log k x n class f k almost sure constant equal therefor penal base empir vc entropi essenti equival guarante risk minim grm procedur propos vapnik thu investig empir method note lozano compar grm procedur method base rademach penalti similar random complex estim nd rademach penalti systemat outperform grm procedur grm compar minimum descript length principl independ test sampl techniqu regard simpli crossvalid techniqu main messag penal techniqu take account empir loss structur properti model cannot compet crossvalid sampl size contrari conclus base experi databas penalti perform favor compar penalti base independ test data gure shown report experi three method holdout method holdout base select independ sampl describ section maximum discrep md method select model accord method section rademach penal rp perform random complex method propos section use maximum discrep section experi penalti were l found multipli penalti dene section provid superior perform use random complex estim sec tion penalti were sup n note experi log k log k term omit penalti reason comparison perform oracl select also shown pictur method select model minim true loss l b among empir loss minim b f k class f k train error minim algorithm describ implement use templat prioriti queue doubli link list provid leda librari result result illustr gure below gener conclus may observ gener error ie true loss obtain method mdp rp favor compar holdout even sampl size datadepend penal techniqu perform well holdout data depend penal techniqu exhibit less varianc holdout main messag paper good error estim procedur provid good model select method hand except holdout method datadepend penal method tri estim directli l b gure show accur nois level high becom rather inaccur nois level decreas strong incent explor datadepend penal techniqu take account fact part f k equal elig minim empir loss acknowledg thank vincent mirelli alex smola fruit convers thank anonym review use suggest r new look statist model identi logic smooth densiti estim complex regular applic arti minimum complex densiti estim sampl complex pattern classi gener perform support vector machin pattern classi sharp concentr inequ applic random combinator learn learn canon smooth estima tion learn canon smooth estima tion introduct support vector machin bound learn algorithm nonlinear statist model nonparametr maximum likelihood estim method siev experiment theoret comparison model select method rademach penalti structur risk minim rademach process bound risk function learn radial basi function network complex regular function learn model select use rademach penal adapt model select use empir com plexiti nonparametr estim via empir risk minim concept learn use complex regulariza tion comment c p improv gener explicit optim margin applic concentr inequ stati tic method bound di platform combinatori geometr comput perform bound nonlinear time seri predict minimum complex regress estim weakli depend observ univers prior integ estim minimum descript length boost margin estim dimens model structur risk minim datadepend hierarchi converg rate siev estim best constant khintchin inequ concentr measur isoperimetr inequ product space estim depend base empir data natur statist learn theori statist learn theori uniform converg rel frequenc event probabl theori pattern recognit measur vcdimens learn machin weak converg empir process asymptot properti model select criteria tr ctr david anguita sandro ridella fabio rivieccio rodolfo zunino quantum optim train support vector machin neural network v n p june clayton d scott robert d nowak learn minimum volum set journal machin learn research p leila mohammadi sara van de geer asymptot empir risk minim journal machin learn research p peter l bartlett shahar mendelson rademach gaussian complex risk bound structur result journal machin learn research andr anto balz kgl tam linder gbor lugosi datadepend marginbas gener bound classif journal machin learn research p joel ratsabi complex hyperconcept theoret comput scienc v n p octob ron meir tong zhang gener error bound bayesian mixtur algorithm journal machin learn research joel ratsabi learn multicategori classif sampl queri inform comput v n p septemb magali fromont model select bootstrap penal classif machin learn v n p march shie mannor ron meir tong zhang greedi algorithm classificationconsist converg rate adapt journal machin learn research p ron meir gunnar rtsch introduct boost leverag advanc lectur machin learn springerverlag new york inc new york ny