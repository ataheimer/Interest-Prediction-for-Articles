t knowledg extract transduc neural network a previous neural network shown interest perform result task classif still suffer insuffici focu structur knowledg repres therein paper analyz variou knowledg extract techniqu detail develop new transduc extract techniqu interpret recurr neural network learn first provid overview differ possibl express structur knowledg use neural network then analyz type recurr network rigor appli broad rang differ techniqu argu analysi techniqu weight analysi use hinton diagram hierarch cluster analysi princip compon analysi may use provid certain view underli knowledg howev demonstr techniqu static lowlevel interpret recurr network classif contribut paper particularli broad analysi knowledg extract techniqu furthermor propos dynam learn analysi transduc extract two new dynam interpret techniqu dynam learn analysi provid better understand network learn transduc extract provid better understand network repres b introduct lot interest late knowledg structur represent artici neural network holldobl kurf sperduti et al wermter hallam medsker sun wermter et al elman et al craven wermter artici neural network or connectionist network alreadi demonstr interest learn result variou classic task howev continu dicult understand underli represent within connectionist network lead perfor manc better understand connectionist represent learn import improv credibl comput techniqu also improv network perform integr possibl symbol represent sever attempt made interpret connectionist network focus feedforward network particular andrew diederich abe et al shavlik in stanc visual intern activ weight strength use get impress intern knowledg hinton gorman sejnowski eort also made reduc network size order simplifi knowledg express therein elimi wermter nate small weight furthermor group similar weight replac averag strength shavlik addit techniqu hierarch cluster analysi use interpret connectionist network never theless often interpret dynam learn process underli knowledg neglect especi case dynam recurr neural network interpret recurr network dicult nonrecurr feedforward network sinc previou context recurr network import dynam uenc within network intern state recurr network depend input also intern state local memori base previou input elman gile omlin omlin gile reason date focu primarili smaller recurr network artici gener data instanc interest current approach interpret train srn network two input two output two intern element learn sequenc n b n wile elman discov network behav like spiral move x point wherea seem plausibl interpret behavior recurr network train learn sequenc n b n dierent interpret requir move dierent task data set closer realworld scenario past develop larg real world system spoken languag analysi make extens use srn network wermter weber wermter meurer spoken input recogn speech recogn analyz syntact semant dialog level base increment anali si parallel syntact semant interpret robust process error date howev yet possibl focu interpret learn process interpret connectionist knowledg paper primarili concern detail interpret learn behavior well symbol interpret learn knowledg train order carri detail analysi concentr syntact transform task repres task largescal speechlanguag system task recurr network process sentenc associ syntact class phrasal level eg noun phrase preposit phrase etc use task analyz recurr neural network use mani dierent techniqu structur paper follow first introduc repres syntact transform task then dene illustr a dynam learn analysi b weight analysi c hierarch activ analysi d compon activ analysi e transduc extract rigor compar techniqu network data set argu dierent techniqu provid mutual complementari interpret contribut paper particularli broad concret analysi knowledg extract process done befor furthermor propos dynam learn analysi transduc extract two new interpret techniqu dynam learn analysi provid better understand network learn transduc extract provid better understand network repres extract structur knowledg use syntact analysi task order examin number dierent techniqu extract structur knowledg connectionist network rigor manner focu particular task spoken languag environ wermter lochel wermter weber wermter meurer train mani variat srn network elman mani sentenc use variou corpora sever thousand word each base corpu sentenc domain schedul appoint word tabl summar accuraci label assign unknown test set relat experi result report elsewher detail wermter lochel wermter weber wermter meurer wermter want illustr realworld network perform tabl focu how knowledg extract transduc neural network ever analysi process extract explicit knowledg implicitli learn knowl edg paper concentr syntact phrasal assign mark tabl tabl perform network test set appoint schedul corpu task accuraci test set basic syntact disambigu basic semant disambigu syntact phrasal assign semant phrasal assign dialog act assign word repair detect phrase repair detect demonstr process knowledg ex traction use sentenc contain word domain appoint schedul illustr purpos concentr learn syntact phrasal assign task sequenc basic categori word associ sequenc abstract syntact categori actual occur syntact basic categori noun n verb v adverb a adject j preposit r determin d pronoun u abstract phrasal categori noun group ng verb group vg preposit group pg task recurr network learn assign phrasal categori basi basic syntact categori order support robust understand spontan spoken languag below show exampl utter corpu togeth syntact categori basic phrasal level u ng thought v vg r pg u ng v vg d ng thursday n ng r pg easter n base seven basic syntact three phrasal syntact categori use srn network seven input unit three intern unit three output unit the network actual system contain categori train sever thousand word illustr purpos restrict smaller network learn rate momentum weight updat perform increment train pattern train pattern consist basic syntact categori input layer abstract phrasal categori output layer figur show simpli exampl recurr network task syntact phrase assign output input context layer aaaa aaaa aaaa noun group preposit group verb group pronoun noun adject verb adverb preposit determin fig recurr network knowledg extract syntact phrase assign activ output element j t time srn network comput basi weight activ h t incom connect limit logist function f activ element intern layer h l t comput similar manner activ input layer k t time use activ intern layer previou time step wermter dynam learn analysi knowledg structur lazi learn past work knowledg structur connectionist network focus static connectionist network represent howev import insight gain examin certain knowledg structur emerg develop time certain task learn complet frequent interpret learn behavior demonstr mean learn curv overal error reduct time howev learn curv rst step detail analysi provid preliminari hint perform network train time figur show learn curv overal sum squar error time pattern error fig learn curv syntact phrasal assign learn curv show speed learn dier substanti time further more see dierent stage learn process begin learn proce fast later learn slower take longer make signic improv instanc seem learn nish nal signic improv examin network reach perform start analysi directli random initi weight state learn start want give overview overal perform input pattern dierent time step ef fect show error pattern demonstr set dierent time step figur show individu error pattern trainingindividu pattern fig perform individu pattern learn ing base random initi pattern show rel high error point expect valu output element dier desir valu therefor expect error individu pattern three output element expect error valu conrm gure shown gure error decreas quickli start train state pattern train set shown gure first observ train pat tern error pattern shown could reduc signicantli pattern still show high error obvious network start learn pattern select knowledg extract transduc neural network individu pattern pattern pattern fig perform individu pattern train pattern detail analysi reveal pattern lower error exactli pattern belong noun group ng pattern network recogn global error minim signicantli focus ng pattern sinc pattern occur frequent than instanc preposit group verb group therefor rst network learn constant map pattern noun group sinc reduc overal error stage explain certain pattern gure still exhibit high error other low error pattern low error exactli pattern classi correctli noun group figur show detail perform pattern network learn constant map ng observ perform ng pattern improv even further howev also observ v g pattern learn detail analysi output prefer reveal stage addit ng v g pattern also learn correctli also demonstr gure remain error pattern stage pattern belong preposit group pg still categor noun group ng ng pattern v g pattern classi correctli network learn frequent ng pattern second frequent v g pattern learn thu one could state network pursu conserv lazi learn strategi learn frequent occur simpl regular rst individu pattern pattern pattern pattern fig perform individu pattern train pattern afterward network attempt improv pattern especi remain pattern preposit group pg occur noun pronoun determin adject either part pg pattern ng pattern order resolv potenti ambigu previou context must use learn correct class assign again exampl conserv lazi learn strategi network individu pattern except pg pattern pattern ng pattern vg pattern fig perform individu pattern train pattern wermter sinc rst network learn pattern need previou context knowledg categori assign simpl noncontextdepend categori assign learn pattern learn requir context previou pattern assign state network pattern shown gure pattern classi correctli except three compar g ure remain error individu pattern could reduc signicantli learn pg pattern necessari network integr local preced con text pattern regular learn shown gure comparison gure point smaller scale vertic axi stage pattern learn even though dierenc error rate individu pattern order reach correct train set may necessari give reason good state certain stage order reach even better stage later also ect global learn curv gure individu pattern ng patternsvg pattern pg pattern correct fig perform individu pattern train pattern gener network pursu conserv lazi learn strategi first simpl frequent occur gener one categori learn network cannot minim error signicantli more frequent occur categori integr fur thermor pattern learn requir previou local context pattern learn requir context correct categori assign otherwis ambigu input final remain except learn conserv learn process may possibl overal error increas brie order reach better overal state later weight analysi knowledg extrac tion visual intern weight strength use get impress intern knowl edg experi train set learn correctli pattern start analysi start weight analysi sinc weight provid lowest level interpret connectionist represen tation figur show weight network three dierent time step illustr weight chang time learn gure identi sourc connectionist element shown horizont identi goal connectionist element shown vertic start horizont axi left right see weight threshold element s input connectionist element syntact basic categori n j v a r u d three intern element three context element c vertic axi top bottom see weight three intern element output element repres abstract syntact categori vg ng pg knowledg extract transduc neural network pattern pattern pattern fig weight analysi begin train pattern train pattern train pattern white box repres posit weight black box neg weight size box correspond size weight copi connect intern layer context layer chang therefor shown sinc alway equal start analysi rst third gure random initi rst third show weight network pattern point ng pattern classi correctli pattern learn yet network learn constant output order reduc overal error much possibl see gure network produc constant ng class wermter see weight input element syntact basic categori n j v a u d intern element rel small similar hold weight context element c c c intern element due random initi begin train ing weight intern element output element abstract syntact categori neg v g pg intern element ng close reason network produc constantli ng categori stage focu state network present pattern also shown gure point ng v g pattern assign correctli also ect weight observ posit weight n intern element posit weight intern element ng howev see neg weight v intern element intern element v g pg pattern categor correctli point one reason pg pattern depend signicantli previou con text howev point network learn obviou prefer start chang weight context layer network state pattern shown bottom gure intern layer distribut represent develop therefor direct interpret easili po sibl howev observ rst intern element primarili import pg detec tion second intern element play import role v g assign third intern element import ng nevertheless distribut rather local represent addit uenc ele ment furthermor weight context layer from c h chang necessari order learn pg group assign gener speak explain certain phenomena use type weight analysi lowest interpret level network howev dicult extract explicit knowledg deeper understand behavior network directli weight reason diculti includ static represent weight show dynam recurr network distribut weight activ number weight especi case larger network there fore eort could made reduc size network elimin small weight furthermor group similar weight could replac averag strength noneth less weight analysi still detail larger network compon activ analysi knowledg extract weight analysi focus weight provid lowlevel analysi one way address problem move toward activ analysi activ intern element analyz sinc intern element receiv activ number weight connect activ intern element integr sever weight connect provid higher abstract level analysi order demonstr analysi perform use srn network introduc previou section store vector represent intern layer pattern vector represent constitut input cluster algorithm provid hierarch represent form dendrogram vector similar vector represent end cluster figur show initi part pattern cluster accord intern activ clearli observ intern represent ect classic accord three class pg is base weight intern layer learn represent particularli support classic singl word appear dierent context lead differ intern represent instanc word the shown two dierent represen tation one represent use part ng class part pg class therefor nd represent dierent posit within dendrogram knowledg extract transduc neural network allung iung iung usung weung wednesdaynng morningnng inrpg weeknpg isvvg comevvg mustvvg isvvg fig hierarch cluster analysi intern classic represent for visibl purpos portion shown princip compon analysi knowledg extract anoth kind analysi use interpret intern represent cla sicat princip compon analysi figur show result analysi current task vector intern layer correspond identi provid input princip compon analysi vector dier substanti depict gure larg distanc also observ intern represent ect prefer map learn three categori class ng v g pg pattern distribut across dierent area thu classic intern represent clearli seen show network actual learn classic task well learn complet intern represent character prefer map ping accord cluster analysi princip compon analysi similar intern vector represent respons represent similar prefer assign equal cate gori howev interpret weight mean hinton diagram activ via cluster analysi princip compon analysi provid limit form structur extract knowledg wermter iung inrpg thursdaynng morningnng weung unsung onapg thedng isvvg wednesdaynng iung comevvg mustvvg iung thatung isvvg allung tuesday couldvvg usung letvvg usung dasung isvvg makevvg tillrpg weung inrpg marchnpg otherjng thatung isvvg fig princip compon analysi intern classic represent transduc extract word sequenc word repres syntact semant pragmat categori prefer input instanc srn network input repres sequenc categori prefer associ sequenc correspond output prefer simpl descript sequenc analysi similar function synchron sequenti machin booth kohavi shield although prefer learn yet consid machin therefor shall focu extens synchron sequenti machin repres sequenti knowl edg especi synchron moor machin start basic denit synchron sequenti machin also call transduc denit synchron sequenti ma chine transduc synchron sequenti machin tupl nite nonempti set input output nonempti set state function f state transit function function f output function output depend state input machin socal meali machin output function f o output depend state machin later socal moor machin output function f synchron sequenti machin sometim call transduc sequenti machin assign output new state input old state done whole sequenc input state discret time set necessarili nite booth although assum case nite machin wherea automata knowledg extract transduc neural network acceptor languag decid whether certain input belong correspond grammar sequenti machin transduc chang intern state dynam depend input previou state also provid output input meali moor machin slightli dier ent other moor machin determin state rst afterward state use provid output contrast output meali machin depend also directli current input howev shown moor machin equival meali machin vice versa booth hopcroft ullman case concentr moor machin sinc output certain neural network base intern state hold in stanc feedforward network srn network wherea sometim sun sequenti machin use model singl element neural network want use sequenti machin descript whole network also motiv fact real neuron system seen physic entiti perform state transit churchland sejnowski specifi languag knowledg describ moor machin state transit function f output function f also integr f f function f os f correspond instanc transform within srn network specic moor machin could perform use state tabl potenti entri task assign syntact phrasal categori syntact basic categori could be verb current state prep group new state verbal group output verbal group may possibl assign direct interpret state reason simpl identi may use verb current state new state output verbal group possibl dene state transit tabl assign combin input current state output new state way symbol synchron sequenti machin speci clear regular known beforehand number limit tabl compos manual howev number input state combin quickli get larg automat procedur becom necessari abovement state transit tabl discret symbol therefor support gradual represent instanc input state could ambigu dierent gradual prefer could exist dierent inter pretat instanc meet could stronger prefer syntact interpret noun smaller prefer verb form consequ want use prefer input output state machin prefer type abl take valu multipl prefer repres integr extend singl categori as in verb ndimension prefer input mdimension prefer output obtain new synchron machin call prefer moor machin want describ synchron sequenti prefer moor machin transform sequenti input prefer sequenti output prefer see simpl recurr network feedforward network interpret neural prefer moor machin furthermor show symbol neural knowledg integr quit natur use prefer moor machin wermter denit prefer moor machin prefer moor machin pm synchron sequenti machin character tupl nonempti set input output state sequenti prefer map contain state transit function f output function f n m ldimension prefer valu n l respect gener version prefer moor machin shown gure left prefer moor machin realiz sequenti prefer map use current state prefer input prefer assign output prefer new state prefer prefer map state aaaa aaaa output input fig neural prefer moor machin relationship srn network describ new techniqu extract knowledg within recurr network form transduc symbol transduc extract recurr network assign input vector basic syntact categori new output vector phrasal categori depend previou context network intern state context repres threedimension vector simplic strict symbol interpret threedimension vector take state order acquir symbol interpret network present pattern train set store intern state vector hidden layer network output vector state vector next corner prefer determin use euclidean distanc metric thu euclidean distanc metric assign one three symbol abstract syntact phrase categori output vector one eight state number identi state vector knowledg extract transduc neural network nng rpg vvg dng dng nng rpg ung dng nng nng dpg jpg npg vvg dng ang apg jng nng vvg vvg rpg vvg ung rpg nng nng dng ung nng rpg apg vvg vvg ung fig transduc extract recurr network exampl sentenc that ung vvg dng thursday nng rpg easter nng figur show knowledg learn network extract symbol transduc corner node repres eight strict state center node repres start state tran ducer edg nd symbol singl transduct input output categori separ colon eg ng mean start sourc state edg determin prefer assign noun group prefer ng transduct made end state edg extract transduc see clear regular certain state instanc transduct state primarili respons assign preposit group pg exampl transduct state state primarili respons verbal group vg assign furthermor gure show exampl transduct sentenc that thursday easter begin start state center see transduct ng word that assign noun group ng pronoun u then assign verb group vg verb is transduct ng ng assign noun group ng the thursday final transduct assign preposit group pg sequenc after easter dierent abstract syntact categori ng pg assign categori n depend learn previou context nng rpg vvg dng dng nng rpg ung dng nng nng dpg jpg npg vvg dng ang apg jng nng vvg vvg rpg vvg ung rpg nng nng dng ung nng rpg apg vvg vvg ung fig transduc extract recurr network exampl sentenc i ung thought vvg rpg dpg next jpg week npg detail less detail transduc obtain state output vector map fewer node thu gener abstract level symbol transduc quit variabl symbol transduc repres abstract detail network knowledg abstract also hide numer complex allow direct symbol interpret provid summari network behavior give exampl gure show transduct exampl sentenc i thought next week begin start state center see transduct ng word i assign noun group ng pronoun u then assign verb group vg verb thought final transduct r pg assign preposit group pg word sequenc in next week one advantag transduc extract higher abstract level use represent recurr network lead better understand function origin network contain detail knowledg numer weight activ possibl see declar sequenti symbol knowledg network repres extract symbol transduc allow better understand learn sequenti knowledg repres explicit manner knowledg extract transduc neural network discuss analysi comparison knowledg extract techniqu previou work use individu techniqu isol interpret neural network extract structur knowledg them paper analyz dierent techniqu use train network order interpret network knowl edg extens comparison detail network knowledg need order gain better understand knowledg extract repres neural network also introduc two new techniqu here dynam learn analysi transduc extract dynam learn analysi examin format develop categori time learn thu provid much deeper understand neural network arriv learn represent transduc extract develop repres sequenti process recurr network higher level abstract gener found dierent interpret techniqu provid dierent view knowledg contain neural network thu singl best techniqu dier ent aspect knowledg extract use particular techniqu depend rather requir interpret tabl illustr summar gener properti dierent techniqu dynam learn analysi dla base output represent provid high level understand base known output represent techniqu easi interpret use network type hand particularli support recurr network symbol integr exibl knowledg structur furthermor structur relationship cannot extract transduc extract te new techniqu use output represent well intern activ main advantag techniqu high level understand form extract symbol transduc specic support sequenti recurr network possibl extract structur relationship extract transduc integr symbol knowledg eg code symbol transduc further more dierent transduc gener exibl base number state use intern activ layer lead rel straightforward interpret network involv compar techniqu also requir addit eort extract symbol transduc intern activ output represent compar dla caa see dla techniqu specic provid high level interpret dynam learn process argu wa haa caa techniqu tendenc toward gener detail lowlevel interpret dla te howev techniqu special highlevel dynam interpret focus output interpret dynam recurr network provid new level understand wherea lot previou work focus lowlevel in terpret believ futur higher level interpret knowledg extract requir relat work transduc extract relat work finit state automata transduc wide use variou form within tradit eg hopcroft ullman basic automata transduc alway certain context state analyz certain word symbol move new state potenti gener new word sym bol use chang state possibl encod sequenti context although nite automata regular languag sucient describ possibl construct natur languag complet see eg winograd automata still constitut central minim requir represent natur languag thu occupi lowest level chomski hierarchi languag hopcroft ullman furthermor possibl design ecient realiz nite automata dierent domain kaplan eg wermter tabl comparison dierent knowledg extract techniqu dynam learn analysi dla weight analysi wa hierarch activ analysi haa compon activ analysi caa transduc extract te abbrevi activationsweightsoutput lowmediumhigh network represent use w ao gener level understand h l h specic support recurr network l l l l h degre structur relationship l l h integr symbol knowledg l l h flexibl level knowledg structur l l h comput eort l l easi interpret h l h gener portabl network h h h h morpholog lexicon access inform extract sentenc syntact tag etc recurr network potenti learn sequenti prefer map f automat base input output exampl see gure wherea tradit moor machin fuzzysequentialfunct santo involv manual encod recent illustr srn network emul symbol moor machin nite automaton kremer kremer also shown howev goudreau gile goudreau et al recurr network singl input layer one context layer one output layer socal singl layerrstordernetwork sucient realiz arbitrari nite automata natur languag process represent least power nite au tomata consequ singlelayerrstord network appropri use srn network here recurr network contain nite transduc special case also support much power properti base gradual mdimension prefer represent instanc could shown srn network emul certain restrict properti pushdown automaton particular recurs represent structur limit depth elman wile elman apart tradit symbol regular rep resent gradual learn represent also repres furthermor number input state output prefer necessarili nite therefor neural prefer moor machin power nite transduc er recurr neural network seen learn augment simpl nite symbol transduc respect learn within gradual prefer space perspect symbol knowledg special abstract region neural prefer space import line research automata recurr network report gile et al goudreau gile tino et al gile colleagu studi nite state automata neural network substanti dierenc research start often known nite state automaton use gener sequenc it sequenc use train secondord neural network use partit algorithm nite state automaton extract network activ minim compar origin known nite state automaton way gile colleagu could studi comput properti extract particularli well nite state automata also frequent reli rel simpl sequenc knowledg extract transduc neural network motiv methodolog dierent sever respect assum initi nite state automaton transduc known especi realworld problem interest case one automaton known advanc wherea interest comparison sequenc gen erat gener sequenc nite state automaton alreadi introduc certain regular train set thu sequenc gener import uenc learn behav ior someth want rule out fact interest situat know machin ex tract especi noisi realworld learn data underli regular may quit dispar regularli gener sequenc furthermor task network quit dierent secondord network employ gile colleagu train recognit output layer repres state represent fed back input layer next step recurr network perform assign task sequenc input associ sequenc output determin whether certain sequenc belong certain automaton simpl structur sequenc is is interest transduc extract rather recogn ex traction gener design nal state network sinc network extract symbol transduc produc output long input provid transduc behavior therefor quit dierent recognit perform report gile omlin base acceptor artici languag conclus main contribut paper particularli broad analysi knowledg extract recurr network addit propos dynam learn analysi transduc extract two new dynam interpret tech niqu dynam learn analysi provid better understand network learn transduc extract provid better understand network repres learn conserv lazi learn strategi lead connectionist represent describ symbol transduc transduc allow much better interpret sequenti network knowledg compar standard analysi use hierarch cluster hinton diagram weight analysi cluster analysi princip compon analysi detail static contrast new method extract symbol transduc describ learn classic perform much bet ter sinc transduc extract consid sequenti charact learn represent recurr network allow better symbol inspect possibl direct integr symbol classier explor futur work conclud dynam learn analysi transduc extract lot potenti improv knowledg structur base recurr network r extract algorithm pattern classi rule network sequenti machin automata theori comput brain extract comprehens model train neural network distribut repr sentat languag dynam system learn extract recurr neural network repres hybrid prob lem learn distribut represent concept introduct automata theori finit state technolog switch finit automata theori comput power elmanstyl recurr network theori grammat induct connectionist paradigm hybrid intellig system extract rule discretetim recurr neural network fuzzi sequenti func tion framework combin symbol neural learn introduct automata theori learn distribut represent classi finit state machin recurr neural network hybrid connectionist natur languag process hybrid approach arti prefer moor machin neural fuzzi integr build lexic represent dynam use arti screen learn syntact semant spoken languag analysi use arti learn count without counter case studi dynam activ landscap recurr net work languag cognit process tr