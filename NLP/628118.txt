t simpl strategi encod tree automata sigmoid recurs neural network a abstractrec number author explor use recurs neural net rnn adapt process tree treelik structur one import languagetheoret formal process treestructur data determinist finitest tree automata dfsta may easili realiz rnn use discretest unit threshold linear unit recent result sima neural network world pp show threshold linear unit oper binari input implement analog unit use continu activ function bound real input construct proof find scale factor weight reestim bia accordingli paper explor applic result simul dfsta sigmoid rnn that is analog rnn use monoton grow activ function also present altern scheme onehot encod input yield smaller weight valu and therefor work lower satur level b introduct last decad number author explor use analog recurs neural net rnn adapt process data laid tree treelik structur direct acycl graph arena frasconi gori sperduti recent establish rather gener formul adapt process structur data focus direct order acycl graph which includ tree sperduti starita studi classic structur direct order graph includ cyclic graph sperduti studi comput power recurs neural net structur processor one import languagetheoret formal process treestructur data determinist nitest tree automata dfsta also call determinist frontiertoroot ascend tree automata may easili realiz rnn use discretest unit threshold linear unit tlu sperduti fact recent shown elmanstyl rnn use tlu may simul dfsta provid intuit explan similar express kremer special case determinist nite automata also work sigmoid network increment gain sigmoid function lead arbitrarili precis simul step function are howev unawar attempt establish nite valu gain exact simul may inde perform analog rnn recent result sma show tlu oper binari input simul analog unit use continu activ function bound real input tlu neuron comput output appli threshold step activ function bias linear combin binari input correspond analog neuron work activ function gx two dierent nite limit b x given input output toler construct proof nd scale factor weight basic valu gain analog activ function use scale factor shift valu bia paper dene three possibl way encod dfsta discretest rnn use tlu explor applic sma result turn discretest rnn sigmoid rnn simul origin with sigmoid mean analog activ function monoton grow addit present altern scheme analog simul yield smaller weight valu sma scheme discretest case therefor work lower satur level goal nd smallest possibl scale factor guarante correct behavior last approach assum onehot encod input gener approach use stabl encod famili nite state machin fsm varieti sigmoid discretetim recurr neural network similar spirit previou work omlin gile determinist nite automata a class fsm particular discretetim recurr neural network dtrnn architectur the secondord dtrnn use gile et al follow section tree automata recurs network intro duce section describ three dierent scheme encod recurs neural network discretest rnn use tlu main result sma present section togeth similar construct case exclus also call onehot encod input section describ convers discretest rnn sigmoid counterpart dierent scheme evalu compar magnitud result weight valu final present conclus last section tree automata recurs neural network explor neural network simul tree automata need specifi notat tree describ architectur recurs neural network tree nitest machin denot rank alphabet is nite set symbol associ function give rank symbol subset symbol rank denot set tree dene set string made symbol augment parenthesi comma repres order label tree or recurs ani symbol rank singlenod tree root node label f rank children valid tree belong rank may dene gener relat r n formul equival symbol one possibl rank split determinist nitest tree automaton dfsta vetupl nite set state alphabet label rank function r f q subset accept state nite collect transit function form maximum rank valenc dfsta tree result t q oper dfsta tree dene unden otherwis word state t associ given tree depend label root node f also state ndfsta associ children t convent unden transit lead unaccept tree is maximum number children node tree la usual languag la recogn dfsta subset dene one may gener denit dfsta produc output label node visit act like structurepreserv nitest tree transduc two gener possibl correspond class nitest string transduc known meali moor machin meali tree transduc obtain replac subset accept state f denit dfsta collect output function g one possibl rank moor tree transduc obtain replac f singl output function whose argument new state convers dfsta regard particular case meali moor machin oper tree whose output function return two valu neural architectur dene two recurs neural architectur similar use relat work frasconi gori sperduti sperduti sperduti starita nd conveni talk meali moor neural network dene way network comput output use analog correspond nitest tree transduc rst architectur highord meali rnn second one rstorder moor rnn highord meali recurs neural network highord meali recurs neural network consist two set singlelay neural network rst one comput next state play role collect transit function nitest tree transduc second one comput output play role collect output function meali nitest tree transduc nextstat function realiz collect singlelay network one possibl rank nx neuron m input port input subtre state vector remain two combin highord moor rnn which may easili shown comput power meali counterpart rstorder meali machin which need extra layer comput arbitrari output function see dimension nx one input node label repres vector dimension n u node label input port take input vector equal dimension number input symbol n particular node tree label l u input vector associ node compon u k equal input symbol node k input symbol onehot exclus encod node label next state x comput correspond m th order singlelay neural net follow repres bia network rank leaf ie l express compon x reduc ik is set jj weight type w nxk play role initi state recurr network output function realiz collect network n unit input structur output function node rank evalu rstorder moor recurs neural network rstorder moor recurs neural network collect nextstat function one rank m form ik structur input port highord counterpart singl output function form take nx input produc n output encod tree automata discretest recurs neural network present three dierent way encod nitest recurs transduc discretest rnn use tlu activ function rst two use discretest version highord meali rnn describ section third one use discretest version rst order moor rnn describ section rst two encod straightforward third one explain detail encod base exclus onehot encod state nitest transduc n rnn said state compon x nx dimension state vector x take high valu compon take low valu addit exclus encod input n output n use one discretest rnn encod convert section sigmoid rnn encod use two strategi describ section highord meali encod use bias assum meali nitest tree transduc then discretest highord meali rnn weight weight bia v behav stabl simul nitest tree transduc note uppercas letter use design weight discretest neural net would also case bias set valu valu happen best valu convers sigmoid rnn highord meali encod use bias second possibl encod the treetransduc counterpart string transduc encod describ use bia nextstat weight bias w output weight bias v as case bias construct encod also work bias set valu optim valu convers sigmoid rnn rstorder moor encod consid moor nitest tree transduc form encod dfsta rstorder rnn need split state rst statesplit found necessari implement arbitrari nite state machin rstorder discretetim recurr neural network also recent describ sperduti encod rnn easili shown equival easili construct use method describ sperduti follow new set state q subset dene nextstat function new set dene follow shorthand notat use final new output function dene follow split encod discretest rnn eq choos paramet follow exist q jm zero otherwis exist q l zero otherwis zero otherwis dicult show oper discretest rnn equival correspond therefor as case previou construct dierent valu bias also possibl one shown happen optim convers sigmoid rnn stabl simul discretest unit analog unit use sma theorem follow restat theorem sma includ conveni notat slightli chang order adapt present studi rememb uppercas letter use denot weight discretest rnn threshold linear unit tlu neuron comput output g h threshold activ function w j j realvalu weight binari input vector consid analog neuron weight w activ function g two dierent limit lim x gx magnitud max call maximum input toler final let also map dene as unden otherwis map classi output analog neuron three categori low high forbidden unden then let r map r kg sma theorem state that input toler max output toler exist analog neuron activ function g weight w r that x f g n accord construct proof theorem set sucient condit equat hold b b jgx aj x jgx bj x jj jj small possibl is sma prescript simpli scale weight tlu get analog network bia shift conveni avoid zero valu argument activ function note input analog unit allow within wherea output allow within b construct recurs network output one analog unit normal use input anoth analog unit therefor natur choic choic compat instanc use logist function g l whose limit exactli activ function hyperbol tangent tanhx particular case eq reduc becom follow reder simpl sucient condit stabl simul tlu analog unit suitabl strictli grow activ function restrict exclus encod input wherea sma construct valid binari input vector simplic prescript allow altern straightforward worstcas analysi lead weight are common situat smaller obtain direct applic sma theorem use simpl scheme exclus encod input condit stabl simul nitest machin fsm dtrnn studi follow approach relat sma carrasco et al see also condit assum special usual case onehot exclus encod input strictli grow activ function assumpt togeth worstcas analysi allow one obtain prescript choic suitabl weight stabl simul work lower satur level gener scheme eq usual prescript realiz singleparamet scale weight tlu includ bia scale equival nding nite valu gain sigmoid function ensur correct behavior note that case exclus encod as one use section n possibl input binari vector b vector whose ith compon one rest zero therefor argument may take n dierent valu w w for binari input howev analog neuron input toler may receiv input vector r b ig properti exclus encod make possibl formul condit hold possibl input two case distinguish case hold g strictli grow may also write w obvious minimum valu w jg therefor sucient condit analog neuron simul correspond tlu input b similar argument lead sucient condit instanc choos w eq fulll either order compar eq last two condit may written singl restrict pair condit simpl choic w adequ case w case appear encod propos section encod tree automata sigmoid recurs neural network mention befor theorem section lead natur choic addit appli neuron recurs neural network due widespread use consid compar section variou possibl encod use logist function g although result dierent activ function may also obtain inde monoton growth function along real line enough follow deriv as case eq case want simul sigmoid rnn consid rst highord meali rnn architectur input x j is case rnn describ product output x jm one rang product alway rang word forbidden region dicult show equal hold therefor condit max suce purpos want use scale factor weight possibl rank m use consid rstorder moor rnn architectur case product condit max sucient previou section describ two dierent scheme simul discretest neuron take exclus input vector sigmoid neuron section describ applic two scheme three recurs neural network architectur describ section use sima prescript sma construct section give bias highord meali rnn section follow and therefor w condit appli togeth condit ensur posit valu h shown minimum valu allow h depend nx given architectur nx maximum valu m xed chang exist least one valu allow one choos minimum valu h need stabl simul sens minim h choos appropri perform lead valu shown tabl minimum requir h function nx weight obtain grow slower logmn x seen inordin larg lead therefor satur analog rnn appli sma construct biasless highord meali rnn sec tion get use equat nu nx term but due exclus encod input nu n term ident zero uncertainti all togeth posit valu h weight obtain search minimum h satisfi condit shown tabl seen weight which show asymptot behavior one previou construct smaller still larg avoid satur final appli sma construct rstorder moor rnn section have nextstat function rank m and accordingli weight are exist q jm zero otherwis exist q l zero otherwis where use togeth output function accordingli weight zero otherwis sma construct appli provid consid possibl w ik u k nextstat function dierent bia w u f g n choos safest prescript valid possibl valu bia present case bia alway valu m number addit term provid use want singl valu h assign weight nextstat function output function use weight obtain search minimum h satisfi condit shown tabl grow slower logmnx and seen equal smaller one bias highord construct larger bias construct howev fact state split lead larger valu nx automata transit function taken account use encod exclus input choos w includ bias obtain bias highord meali encod section substitut eq togeth happen express one obtain previou section use sma construct biasless encod section result shown tabl result obvious ident report secondord discretetim recurr neural network use bias construct instead appli altern encod biasless highord meali construct section get togeth suitabl minim h lead best possibl weight encod weight grow nx slower logmn result shown tabl previou case result obvious ident report secondord discretetim recurr neural network use bias final appli altern encod scheme rstorder moor construct section the particular form case valid combin w take valu w take valu minimum valu jw exclus valu state vector equal therefor togeth which suitabl minim lead weight grow nx slower logmnx valu shown tabl valu smaller one obtain sma construct rstorder network still larg especi one consid split lead larg valu nx conclus studi four strategi encod determinist nitest tree automata dfsta highord sigmoid recurs neural network rnn two strategi encod rstorder sigmoid rnn six strategi deriv three dierent strategi encod dfsta discretest rnn that is rnn use threshold linear unit appli two dierent weight map scheme convert one sigmoid rnn rst map scheme one describ sma second one altern scheme devis us strategi yield analog rnn simpl weight alphabet contain three weight proport singl paramet h best result ie smallest possibl valu h would desir derivativebas learn set obtain appl altern scheme biasless discretest highord rnn it mention sma map yield larger weight case gener would also work distribut encod allow construct smaller rnn construct valu h suggest that even though principl rnn nite weight abl simul exactli behavior dfsta practic dicult learn exact nitest behavior exampl small gradient present weight reach adequ larg valu smaller weight obtain cost enlarg size rnn due exclus encod state input sma result also work distribut encod acknowledg work support spanish comis interministeri de ciencia tecnologa grant tic r find structur time learn initi state secondord recurr neural network regularlanguag infer gener framework adapt data structur process learn extract syntact pattern recognit introduct automata theori comput power elmanstyl recurr net work construct determinist stabl encod larg formal languag comput power neural network struc ture supervis neural network classi tree automata inform survey tr ctr barbara hammer alessio mich alessandro sperduti marc strickert recurs selforgan network model neural network v n p octobernovemb barbara hammer peter tio recurr neural network small weight implement definit memori machin neural comput v n p august henrik jacobsson rule extract recurr neural network taxonomi review neural comput v n p june