t algebra geometr method hierarch learn machin a hierarch learn machin layer perceptron radial basi function gaussian mixtur nonidentifi learn machin whose fisher inform matric posit definit fact show convent statist asymptot theori cannot appli neural network learn theori exampl either bayesian posteriori probabl distribut converg gaussian distribut gener error proport number paramet purpos paper overcom problem clarifi relat learn curv hierarch learn machin algebra geometr structur paramet space establish algorithm calcul bayesian stochast complex base blowingup technolog algebra geometri prove bayesian gener error hierarch learn machin smaller regular statist model even true distribut contain parametr model b introduct learn artifici neural network understood statist estim unknown probabl distribut base empir sampl white watanab fukumizu let pyx w condit probabl densiti function repres probabilist infer artifici neural network x input output paramet w consist lot weight bias optim infer pyx w approxim true condit probabl densiti train sampl taken let us reconsid basic properti homogen hierarch learn machin map paramet w condit probabl densiti pyx w onetoon model call identifi otherwis call nonidentifi word model identifi paramet uniqu determin behavior standard asymptot theori mathemat statist requir given model identifi exampl identifiabl necessari condit ensur distribut maximum likelihood estim bayesian posteriori probabl densiti function converg normal distribut number train sampl tend infin cramer approxim likelihood function quadrat form paramet select optim model use inform criteria aic bic mdl implicitli assum model identifi howev mani kind artifici neural network layer perceptron radial basi function boltzmann machin gaussian mixtur nonidentifi henc either statist properti yet clarifi convent statist design method appli fact failur likelihood asymptot normal mixtur shown viewpoint test hypothesi statist hartigan research artifici neural network point aic correspond gener error maximum likelihood method hagiwara sinc fisher inform matrix degener paramet repres smaller model fukumizu asymptot distribut maximum likelihood estim nonidentifi model analyz base theorem empir likelihood function converg gaussian process satisfi donsker condit dacunhacastel gassiat proven gener error bayesian estim far smaller number paramet divid number train sampl watanab watanab paramet space conic sym metric gener error maximum likelihood method dierent regular statist model amari ozeki log likelihood function analyt paramet set paramet compact gener error maximum likelihood method bound constant divid number train sampl watanab b let us illustr problem caus nonidentifi layer learn machin pyx w threelay perceptron k hidden unit w paramet pyx w equal machin k hidden unit set true paramet consist sever submanifold paramet space moreov fisher inform matrix log pyx w log pyx wpyx wqxdxdi qx probabl densiti function input space posit semidefinit posit definit rank rank iw depend paramet fact indic artifici neural network mani singular point paramet space figur typic exampl shown exampl section reason almost homogen hierarch learn machin boltzmann machin gaussian mixtur competit neural network singular paramet space result mathemat foundat analyz learn previou paper watanab b watanab watanab a order overcom problem prove basic mathemat relat algebra geometr structur singular paramet space asymptot behavior learn curv construct gener formula calcul asymptot form bayesian gener error use resolut singular base assumpt true distribut contain parametr model paper consid threelay perceptron case true probabl densiti contain parametr model clarifi singular paramet space aect learn bayesian estim employ algebra geometr method show follow fact learn curv strongli aect singular sinc statist estim error depend estim paramet learn ecienc evalu use blowingup technolog algebra geometri gener error made smaller singular bayesian estim appli result clarifi reason bayesian estim use practic applic neural network demonstr possibl algebra geometri play import role learn theori hierarch learn machin dierenti geometri regular statist model amari paper consist section section gener framework bayesian estim formul section analyz parametr case true probabl densiti function contain learn model deriv asymptot expans stochast complex use resolut singular section also studi nonparametr case true probabl densiti contain clarifi eect singular paramet space section problem asymptot expans gener error consid final section devot discuss conclus bayesian framework section formul standard framework bayesian estim bayesian stochast complex schwarz akaik levin tishbi solla mackay amari fujita shinomoto amari murata let pyx w probabl densiti function learn machin input x output y paramet w n dimension vector respect let qyxqx true probabl densiti function input space train sampl x independ taken paper mainli consid bayesian framework henc estim probabl densiti n w paramet space defin expnh n ww log z n normal constant w arbitrari fix probabl densiti function paramet space call priori distribut h n w empir kullback distanc note posteriori distribut n w depend qy x constant function w henc written form infer p n yx train machin new input x defin averag condit probabl densiti function gener error gn defin kullback distanc p n yx qyx qyx log qxdxdi repres expect valu overal set train sampl one import purpos learn theori clarifi behavior gener error number train sampl sucient larg well known levin tishbi solla amari amari murata gener error gn equal increas stochast complex f n arbitrari natur number n f n defin stochast complex f n gener concept sometim call free energi bayesian factor logarithm evid seen statist inform theori learn theori mathemat physic schwarz akaik rissanen mackay opper haussler meir merhav haussler opper yamanishi exampl bayesian model select hyperparatemet optim often carri minim stochast complex averag call bic abic import practic applic stochast complex satisfi two basic inequ firstli defin hw f n respect qxdxdi note hw call kullback inform then appli jensen inequ hold arbitrari natur number n opper haussler watanab a secondli use notat f n f n f n f n explicitli show priori probabl densiti w f n f n understood gener stochast complex case w nonneg function w w satisfi immedi follow therefor restrict integr region paramet space make stochast complex smaller exampl defin expnhwwdw sucient small two inequ eq eq give upper bound stochast com plexiti hand support w compact lower bound proven moreov learn machin contain true distribut hold watanab b watanab a paper base algebra geometr method prove rigor upper bound f n constant olog n function n satisfi olog n log n mathemat speak although gener error gn equal f n natur number n deriv asymptot expans gn howev section show that gn asymptot expans satisfi inequ sucient larg n eq main result paper upper bound stochast complex howev also discuss behavior gener error base eq parametr case section consid parametr case true probabl distribut qyxqx contain learn machin pyx wqx show relat algebra geometr structur machin asymptot form stochast complex algebra geometri neural network subsect briefli summar essenti result previou paper mathemat proof subsect see watanab b watanab a strictli speak need assumpt log pyx w analyt function w analyt continu holomorph function w whose associ converg radii posit uniformli arbitrari x y satisfi qyxqx watanab watanab a paper appli result previou paper threelay perceptron threelay perceptron redund approxim true distribut set true paramet w union sever submanifold paramet space gener set zero point analyt function call analyt set analyt function hw polynomi set call algebra varieti well known analyt set algebra varieti complic singular gener introduc state densiti function vt t dirac delta function sucient small constant definit use vt f n rewritten expnhwwdw dt henc vt asymptot expans f n n asymptot expans n order examin vt introduc kind zeta function jz sato kullback inform hw priori probabl densiti w function one complex variabl z hw z wdw jz analyt function z region rez well known theori distribut hyperfunct that hw analyt function w jz analyt continu meromorph function entir complex plane pole neg part real axi atiyah bernstein sato shintani bjork moreov pole jz ration number kashiwara let largest pole order jz respect note eq show jz z c mellin transform vt use invers mellin transform show vt satisfi c posit constant eq f n asymptot expans o bound function n henc eq moreov support w compact set eq obtain asymptot expans f n first theorem theorem watanab b watanab a assum support w compact set stochast complex f n asymptot expans respect largest pole order function analyt continu hw z wdw hw kullback inform w priori probabl densiti function remark that support w compact theorem give upper bound f n import constant calcul algebra geometr method defin set paramet w proven hironaka resolut theorem hironaka atiyah exist manifold u resolut map arbitrari neighborhood arbitrari u u satisfi au strictli posit function k nonneg even integ figur let decomposit w finit union suitabl neighborhood w appli resolut theorem function jz hw z wdw hw z wdw given recurs blowingup jacobian g u direct product local variabl u cu posit analyt function h j nonneg integ neighborhood u au gu set constant function calcul pole jz take u small enough henc set loss gener then k j depend neighborhood u find jz pole h j ration number neg part real axi sinc resolut map gu found use finit recurs procedur blowingup found algorithm also proven d w w d theorem watanab b watanab a largest pole function jz algorithm calcul hironaka resolut theorem moreov ration number natur number w w dimens paramet note that learn machin regular statist model alway also note that jerey prior employ neural network learn equal zero singular assumpt w w satisfi even fisher metric degener watanab c exampl regular model let us consid regular statist model exp set paramet assum true distribut exp priori distribut uniform distribut w then subset w defin introduc map z pole z show jw z pole way henc result f n log n coincid well known result bayesian asymptot theori regular statist model map eq typic exampl blowingup exampl nonidentifi model let us consid learn machin pyx a b c exp assum true distribut eq priori probabl distribut uniform one set then kullback inform let us defin two set paramet use blowingup recurs find map defin use transform obtain therefor hw z dw z largest pole jw z order one also shown jww z largest pole order one henc result log n o applic layer perceptron appli theori forego subsect threelay perceptron threelay perceptron paramet defin k b k x y fx w h n dimension vector x b h dimension vector c h real number k number input unit output unit hidden unit paper consid machin estim standard deviat s constant assum true distribut say true regress function special case analysi case import follow section true regress function contain model theorem assum learn machin given eq eq train use sampl independ taken distribut eq priori distribut satisfi w neighborhood origin proof theorem use notat kullback inform b c hp kp purpos find pole function let us appli blowingup techniqu kullback inform ha b c firstli introduc map defin let u variabl u except u word jacobian g u map g defin set paramat assumpt exist order obtain upper bound stochast complex restrict integr region paramet space use eq assumpt w gu calcul pole jz assum constant gu du db dc pole function respect largest pole jz then sinc h zero point interv larger z nk pole jz otherwis jz larger pole nk henc nk secondli consid anoth blowingup g defin then method first half exist analyt function impli therefor comb two result largest pole jz satisfi inequ complet proof theorem end proof theorem moreov gn asymptot expans see section obtain inequ gener error hand well known largest pole regular statist model equal d number paramet threelay perceptorn input unit hidden unit output unit employ regular statist model number paramet emphas gener error hierarch learn machin far smaller regular statist model use bayesian estim adopt normal distribut priori probabl densiti shown result theorem direct calcul watanab a howev theorem show systemat result hold arbitrari priori distribut moreov easi gener result case learn machin input unit k first hidden unit k second hidden unit k p pth hidden unit n output unit assum hidden unit output unit bia paramet use blowingup gener proof theorem cours result hold true regress function special case howev follow section show result necessari obtain bound gener regress function nonparametr case previou section studi case true probabl distribut contain parametr model section consid nonparametr case true distribut contain parametr model illustr figur let w paramet minim hw point c figur main purpos clarifi eect singular point b figur contain neighborhood w let us consid case threelay perceptron given eq eq train use sampl independ taken true probabl distribut gx true regress function qx true probabl distribut input space let ek minimum function approxim error use threelay perceptron k hidden unit assum that k k exist paramet w attain minimum valu theorem assum learn machin given eq eq train use sampl independ taken distribut eq priori distribut satisfi w arbitrari w d proof theorem jensen inequ eq hw kullback distanc natur number satisfi k k divid paramet also let real number satisfi then arbitrari u v r n therefor arbitrari x w henc inequ use definit f n increas function hw function satisfi choos w w compact support function firstli evalu f n let w paramet minim h w then eq theorem number paramet threelay perceptron k hidden unit secondli appli theorem f n combin eq eq take sucient close obtain arbitrari given theorem end proof base theorem gn asymptot expans see section gn satisfi inequ n n sucient larg n henc ek n n sucient larg n figur illustr sever learn curv correspond k k k gener error gn smaller everi curv well known barron murata that gx belong kind function space sucient larg k cg posit constant determin true regress function gx then asymptot properti gener error n k sucient larg then choos inequ hold n sucient larg n sucient larg extens larg gn bound gener error middl size model n becom larger bound larger model n extens larg bound largest model complex hierarch learn machin contain lot smaller model paramet space analyt set singular choos appropri model adapt number train sampl bayesian estim appli properti caus fact model nonidentifi quantit eect evalu use algebra geometri asymptot properti gener er ror section let us consid asymptot expans gener error eq f n equal accumul gener error g defin f henc gn asymptot expans f n also asymptot expans howev even f n asymptot expans gn may asymptot expans forego section prove f n satisfi inequ constant determin singular true distribu tion order mathemat deriv inequ gn eq need assumpt asymptot properti gener error assumpt a assum gener error gn asymptot expans q q n a q real constant q n posit nonincreas function n satisfi base assumpt follow lemma lemma gn satisfi assumpt a eq hold gn satisfi inequ proof assumpt a show eq hold ks k eqeq eq tk tk c c tk then arbitrari exist k henc contradict eq henc tn c c end proof lemma paper proven inequ eq theorem without assumpt a then obtain correspond inequ adopt assumpt a word gn asymptot expans eq hold gn satisfi eq conjectur natur learn machin satisfi assumpt a sucient condit assumpt a f n asymptot expans r exampl learner pyx a exp priori distribut standard normal distribut true distribut then shown direct calcul stochast complex asymptot expans henc gn asymptot expans c n expect that gener case gn asymptot expans assumpt a howev mathemat speak necessari sucient condit yet establish import problem statist learn theori futur discuss section univers phenomena observ hierarch learn machin bia varianc singular consid cover neighborhood paramet space w w j sucient small neighborhood paramet w j number j eq finit compact then upperbound stochast complex rewritten exphwwdw function approxim error paramet w j hw v w j statist estim error neighborhood w j log n mw j c constant valu w j mw j respect largest pole multipl meromorph function note bw j v w j call bia varianc respect bayesian estim neighborhood paramet w j minim select largest probabl regular statist model varianc depend paramet word w j arbitrari paramet w j henc paramet minim function approxim error select hand hierarch learn machin varianc v w j strongli depend paramet w j paramet minim sum bia varianc select number train sampl larg extens larg paramet among singular point figur repres middl size model automat select result smaller gener error n increas larger largest model b select last n becom extens larg paramet c minim bia select univers phenomenon hierarch learn machin indic essenti dierenc regular statist model artifici neural network neural network overcomplet basi singular hierarch learn machin origin homogen structur learn model set function use artifici neural network exampl set overcomplet basi word coecient ab c wavelet type decomposit given function gx uniqu determin gx chui murata practic applic true probabl distribut seldom contain parametr model howev adopt model almost approxim true distribut compar fluctuat caus random sampl k b k x appropri number sampl choos appropri learn model expect model almost redund state output function hidden unit almost linearli depend expect paper mathemat foundat studi learn machin state conclus consid case true distribut contain parametr model made hierarch learn machin show paramet among singular point select bayesian distribut result small gener error quantit eect singular clarifi base resolut singular algebra geometri even true distribut contain parametr model singular strongli aect improv learn curv univers phenomenon hierarch learn machin observ almost artifici neural network r likelihood bay procedur univers theorem learn curv four type learn curv neural comput statist theori learn curv entrop loss resolut singular divis distribut commun pure appli mathemat approxim estim bound artifici neural network analyt continu gener function respect paramet mathemat method statist introduct wavelet test local conic model gener function problem appli aic determin structur layer feedforward neural network failur likelihood asymptot normal mixtur mutual inform resolut singular algebra varieti field characterist zero statist approach learn gener layer neural network bayesian interpol stochast complex learn realiz unrealiz rule integr represent ridg function approxim bound threelay network bound predict error statist mechan supervis learn stochast complex model zeta function associ prehomogen vector space optim method layer neural network base modifi inform criterion essenti di gener error layer statist model bayesian estim algebra analysi nonregular learn machin neural comput probabilist design layer neural network base unifi framework learn artifici neural network statist prespect neural comput decisiontheoret extens stochast complex applic learn tr bayesian interpol four type learn curv univers theorem learn curv introduct wavelet statist theori learn curv entrop loss criterion approxim estim bound artifici neural network stochast complex learn realiz unrealiz rule regular condit inform matrix multilay perceptron network integr represent function use threelay network approxim bound algebra analysi singular statist estim ctr miki aoyagi sumio watanab stochast complex reduc rank regress bayesian estim neural network v n p septemb keisuk yamazaki sumio watanab singular mixtur model upper bound stochast complex neural network v n p septemb sumio watanab shunichi amari learn coeffici layer model true distribut mismatch singular neural comput v n p may shunichi amari hiroyuki nakahara difficulti singular popul code neural comput v n p april haikun wei jun zhang florent cousseau tomoko ozeki shunichi amari dynam learn near singular layer network neural comput v n p march shunichi amari hyeyoung park tomoko ozeki singular affect dynam learn neuromanifold neural comput v n p may