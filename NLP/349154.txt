t upper lower bound learn curv gaussian process a paper introduc illustr nontrivi upper lower bound learn curv onedimension guassian process analysi carri emphasis effect induc bound smooth random process describ modifi bessel squar exponenti covari function present explan earli linearlydecreas behavior learn curv bound well studi asymptot behavior curv effect nois level lengthscal tight bound also discuss b introduct fundament problem system learn exampl estim amount train sampl need guarante satisfactori generalis capabl new data theoret interest also vital practic import exampl algorithm learn data use safetycrit system reason understand generalis capabl obtain recent year sever author carri analysi issu result present depend theoret formalis learn problem approach analysi generalis includ base asymptot expans around optim paramet valu eg aic akaik nic murata et al probabl approxim correct converg approach eg vapnik bayesian method pac uniform converg method concern frequentist style confid interv deriv random introduc respect distribut input nois target function central concern result identifi flexibl hypothesi class f approxim function belong exampl vapnikchervonenki dimens f note bound independ input nois densiti assum train test sampl drawn distribut problem understand generalis capabl system also address bayesian framework fundament assumpt concern kind function system requir model word bayesian perspect need put prior target function context learn curv bound analys averag probabl distribut function paper use gaussian prior function advantag gener simpl linear regress prior analyt tractabl prior function obtain neural network neal shown fix hyperparamet larg class neural network model converg gaussian process prior function limit infinit number hidden unit hyperparamet bayesian neural network defin paramet correspond gaussian process gp william calcul covari function gp correspond neural network certain weight prior transfer function investig gp predictor motiv result rasmussen compar perform obtain gp obtain bayesian neural network rang task conclud gp least good neural network although present studi deal regress problem gp also appli classif problem eg barber william paper mainli concern analysi upper lower bound learn curv gp plot expect generalis error number train sampl n known learn curv mani result avail concern lean curv differ theoret scenario howev mani concern asymptot behaviour curv usual great practic import unlik enough data reach asymptot regim main goal explain earli behaviour learn curv gaussian process structur paper follow gp regress problem introduc section shown whole theori gp base choic prior covari function c p x x section present covari function use studi section learn curv gp introduc present properti learn curv gp well problem may aris evalu it upper lower bound learn curv gp nonasymptot regim present section bound deriv two differ approach one make use main properti generalis error wherea deriv eigenfunct decomposit covari function asymptot behaviour upper bound also discuss set experi run order assess upper lower bound learn curv section present result obtain investig link tight bound smooth stochast process model gp summari result open question present last section gaussian process collect random variabl fy x jx xg index set x defin stochast process gener domain x might r dimens although could even gener joint distribut characteris statist random variabl give complet descript stochast process gp stochast process whose joint distribut gaussian fulli defin give gaussian prior distribut everi finit subset variabl follow concentr regress problem assum valu target function x gener underli function x corrupt gaussian nois mean varianc oe given collect n train data where observ output valu input point x would like determin posterior probabl distribut p yjx n order set statist model stochast process set n random variabl model function valu respect introduc similarli collect target valu denot set train input also denot vector whose compon test valu point x distribut p yjx n infer use bay theorem order so need specifi prior function well evalu likelihood model evid data choic prior distribut stochast vector gaussian prior distribut gamma prior describ distribut true underli valu without refer target valu t covari matrix sigma partit element k covari ith jth train point ie k compon vector k x covari test point train data covari test point itself gp fulli specifi mean e y covari function set valid assumpt provid known offset trend data remov also deal x introduc extra notat complex discuss possibl choic covari function c p x x given section moment note covari function assum depend upon input variabl x x thu correl function valu depend upon spatial posit input vector usual chosen closer input vector higher correl function valu likelihood relat underli valu function target data assum gaussian nois corrupt data write likelihood i likelihood refer stochast variabl repres data t r n andomega n theta n matrix given prior distribut valu function p bay rule specifi distribut p yjx n term likelihood model p tji evid data p d n given assumpt standard result eg whittl deriv analyt form predict distribut marginalis y predict distribut turn x n mean varianc gaussian function probabl valu x regard predict gp test point x k covari matrix target t i estim varianc oe x posterior distribut consid error bar x follow alway omit subscript take understood sinc estim linear combin train target gp regard linear smoother hasti tibshirani covari function choic covari function crucial one properti two gp differ choic covari function remark divers due role covari function incorpor statist model prior belief underli function word covari function analyt express prior knowledg function model misspecifi covari function affect model infer influenc evalu equat formal everi function produc symmetr posit semidefinit covari matrix k set input space x chosen covari function applic point view interest function contain inform structur underli process model choic covari function link priori knowledg smooth function x connect differenti covari function meansquar differenti process relat smooth process covari function given follow theorem see eg adler exist finit x x stochast process x mean squar differenti ith cartesian direct x theorem relev link differenti properti covari function smooth random process justifi choic covari function depend upon prior belief degre smooth x work mainli concern stationari covari func tion stationari covari function translat invari ie c p x x depend upon distanc two data point follow covari function use present order simplifi notat consid case stationari covari function squar exponenti se defin lengthscal process paramet defin characterist length process estim distanc input space function x expect vari significantli larg valu indic function almost constant input space wherea small valu lengthscal design function vari rapidli graph covari function shown continu line figur se function infinit mani deriv give rise smooth random process y x poss meansquar differenti order possibl tune differenti process introduc modifi bessel covari function order k mb k defin exp k delta modifi bessel function order see eg equat gradshteyn ryzhik set constant c p factor k constant depend order bessel function matern show function mb k defin proper covari stein also note process covari function mb k differenti studi deal modifi bessel covari function order note mb correspond ornsteinuhlenbeck covari function describ process mean squar differenti k mb k behav like se covari function easili shown consid power spectra mb k se se exp sinc lim mb k behav like se larg k provid rescal accordingli modifi bessel covari function also interest describ markov process order k ihara defin x strict sens markov process order k differenti everi x r p y state gaussian process note definit markov process discret continu time rather differ discret time markov process order k depend previou k time continu time depend deriv last time howev function valu previou time clearli allow approxim comput markov process order k strict sens autoregress model order k ark power spectrum in fourier domain form power spectrum mb k form power spectrum ark model stochast process whose covari function mb k strict sens kple markov process characterist mb k covari function import ultim affect evalu generalis error as shall see section figur show graph four discretis random function gener use mb k covari function with se func tion note smooth random function specifi depend choic covari function particular roughest function gener ornsteinuhlenbeck covari function figur wherea smoothest one produc se figur d intermedi level regular characteris function figur b c correspond mb mb respect note number zerolevel upcross denot n u weakli depend order process mb mb en u deriv eg via finit differ thu one would expect continuoustim situat previou k process valu contain inform need predict next time note ornsteinuhlenbeck process depend previou observ t respect see papouli eqn detail se process en u ornsteinuhlenbeck process nondifferenti formula given en u cannot appli case learn curv gaussian process learn curv model function relat generalis error amount train data independ test point well locat train data depend upon amount data train set learn curv gp evalu estim generalis error averag distribut train test data regress problem measur generalis capabl gp squar differ e g target valu test point x predict made use equat bayesian generalis error point x defin expect dn x t actual distribut stochast process t assumpt data set actual gener gp possibl read equat bayesian generalis error x given train data n see thi let us consid n dimension distribut target valu x x zeromean multivari gaussian predict test point x i henc expect generalis error x given theta theta theta tt use theta tt k equat ident oe x given equat addit nois varianc oe sinc deal noisi data varianc also calcul vivarelli covari matrix pertin calcul true prior gp predictor differ incorrect covari function use express generalis error becom c indic c denot correct incorrect covari function respect shown vivarelli alway larger equat anoth properti generalis error deriv follow observ ad data point never increas size error bar predict oe n x prove use standard result condit multivari gaussian see vivarelli also understood inform theoret argument condit addit variabl never increas entropi random variabl consid x random variabl observ distribut gaussian varianc independ although mean depend t entropi gaussian log log monoton assert prove argument extens qazaz et al inequ deriv gener linear regress dn x similar inequ appli also bayesian generalis error henc remark appli section evalu upper bound learn curv equat calcul generalis error point x averag dn x densiti distribut test point p x expect generalis error e dn particular choic p x c p x comput express reduc n theta n matrix comput e x theta theta k x k x also note equat independ test point x still depend upon choic train data n order obtain proper learn curv gp e g dn need averag possibl choic train data n howev difficult obtain analyt form e g gp function n presenc k equat matrix k vector k x depend locat train point calcul averag respect data point seem hard motiv look upper lower bound learn curv gp bound learn curv noiseless case lower bound generalis error n observ due michelli wahba let order eigenvalu covari function domain input space x show e g n bound learn curv noisi case sinc bound use observ consist project random function onto first eigenfunct expect tight observ consist function evalu result awar pertain asymptot properti n ritter shown optim sampl input space asymptot generalis error hansen show linear regress model possibl averag distribut train set random process obey sacksylvisak condit order see ritter et al detail sacksylvisak condit gener sacksylvisak order mb k covari function exampl mb process henc generalis error show n gamma asymptot decay case x ae r asymptot optim design input space uniform grid silverman prove similar result random design haussler opper develop gener asymptot bound expect loglikelihood test point see n train point follow introduc upper lower bound learn curv gp nonasymptot regim upper bound particularli use practic provid overestim number exampl need give certain level perform lower bound similarli import contribut fix limit outperform model bound present deriv two differ approach first approach make use particular form assum generalis error x e g x error bar gener one data point greater gener n data point former consid upper bound latter sinc observ hold varianc due one data point envelop surfac gener loos speak stochast process possess meansquar deriv said satisfi sacksylvisak condit order s varianc due data point also upper bound oe n x particular oe dn x cf equat envelop upper bound generalis error gp follow argument assert upper bound e g dn x one gener everi gp train subset n larger subset n tighter bound two upper bound present differ number train point consid evalu covari deriv onepoint upper bound e u n twopoint upper bound e u n present section section respect section report asymptot expans e u n term oe second approach base expans stochast process term eigenfunct covari function within framework opper propos bound train generalis error opper vivarelli term eigenvalu c p x x lower bound e l n obtain present section order tractabl analyt express bound deriv introduc three assumpt input space x restrict interv ii probabl densiti distribut input point uniform iii prior covari function c p x x stationari onepoint upper bound e u deriv onepoint upper bound let us consid error bar gener one data point x sinc c equat becom x far away train point x oe confid predict test point lie far apart data point x quit low error bar larg closer x x smaller error bar x irrespect valu c p r vari normal c p ae oe thu oe far use hypothesi concern dimens variabl x thu observ hold regardless dimens input space effect one data point help introduc first upper bound interv split n subinterv theta where b centr around ith data point x let us consid ith train point error bar oe x x theta relat illustr figur envelop surfac error due datapoint denot e g x upper bound overal generalis error sinc deal posit function upper bound expect generalis error interv theta written p x distribut test point sum contribut come train datapoint side equat set interv contribut varianc due x contribut equat also shown figur assumpt stationar covari function integr right hand side equat depend upon differ adjac train point ie x right hand side equat rewritten dx equat deriv chang variabl two integr equat equat upper bound e g still depend upon choic train data n interv integr note argument integr delta equat differ adjac train point denot differ model probabl densiti distribut use theori order statist david given uniform distribut n train data interv densiti distribut differ adjac point p sinc true differ omit superscript thu expect integr equat p n integr calcul follow similar procedur let us consid second line obtain integr part last line follow fact i abl write upper bound learn curv calcul integr express straightforward though involv evalu hypergeometr function becaus evalu function comput intens found prefer evalu equat numer twopoint upper bound e un second bound introduc natur extens previou idea use two data point rather one construct expect tighter one introduc section let us consid two adjac data point x x i interv argument present previou section follow inequ hold x varianc predict x gener data point x x i similarli equat sum contribut side equat get upper bound generalis error defin calcul see appendix a obtain calcul integr respect e u n complic determin delta denomin distribut n prefer evalu numer e u asymptot upper bound equat expans e u n term oe limit larg amount train data obtain expans depend upon covari function deal with expand covari function around asymptot form e u n mb n wherea function mb mb se asymptot valu e u depend neither lengthscal process order covari function mb k k function ratio r lim point section minimum generalis error achiev gp train one datapoint n scenario correspond situat everi test point close datapoint mention begin section asymptot learn curv mb k se covari function delta respect although expans decay asymptot faster learn curv reach asymptot plateau oe also note asymptot valu get closer true nois level r ie unrealist case oe smooth process enter asymptot factor factor affect rate approach asymptot valu oe e u n notic larger lengthscal nois level increas rate decay e u n asymptot plateau asymptot form e u n mb mb mb se covari function vivarelli valu depend upon choic covari function similarli expans e u n decay rate n faster asymptot decay actual learn curv reach asymptot plateau lim straightforward verifi asymptot plateau e u n lower one e u n correspond error bar estim gp two observ locat test point lower bound e l n opper opper vivarelli propos bound learn curv train error base decomposit stochast process x term eigenfunct covari c p x x denot k set function satisfi integr equat z bayesian generalis error e where x true underli stochast function x gp predic tion written term eigenvalu c p x x particular averag distribut input data e g d n written e g d n infinit dimens diagon matrix eigenvalu v matrix depend train data ie v use jensen inequ possibl show lower bound learn curv upper bound train error opper paper mean compar lower bound actual learn curv gp bound rather y must add oe express obtain equat give actual lower bound result point section analyt calcul learn curv gp infeas sinc generalis error complic function train data which insid element k x k gamma problemat perform integr distribut train point compar learn curv gp bound found need evalu expect integr equat distribut data e edn theta dn estim e g n obtain use mont carlo approxim expect use gener train data sampl uniformli input space gener expect generalis error gp evalu use datapoint use gener train data obtain estim learn curv e g n confid interv sinc studi focus behaviour bound learn curv gp assum true valu paramet gp known chose valu constant covari function equat c p allow lengthscal nois level oe assum sever valu begin with studi smooth process affect behaviour learn curv empir learn curv figur obtain process whose covari function mb notic learn curv exhibit initi linear decreas explain consid without train data generalis error maximum allow model c introduct train point x creat hole error surfac volum hole proport valu lengthscal depend covari function addit new data point x effect gener new hole surfac data point like two data lie far apart one other give rise two distinct hole thu effect small dataset exert pull error surfac proport amount train point explain initi linear trend concern asymptot behaviour learn curv verifi agre theoret analysi carri ritter particular loglog plot learn curv mb k covari function show asymptot behaviour similar remark appli se covari function asymptot decay rate opper also note smoother process describ covari function smaller amount train data need reach asymptot regim behaviour learn curv affect also valu lengthscal process nois level illustr figur learn curv shown figur a obtain mb covari function set nois level oe vari valu paramet intuit figur a suggest decreas lengthscal stretch earli behaviour learn curv approach asymptot plateau last longer due effect induc differ valu lengthscal stretch compress input space verifi rescal amount data n ratio two lengthscal two curv figur a lay top other variat nois level shift learn curv prior valu c p offset equal nois level cf equat order see signific effect nois learn curv figur b show loglog graph e obtain stochast process mb covari function set notic two main effect nois varianc affect actual valu generalis error sinc learn curv obtain high nois level alway one obtain low nois level second effect concern amount data necessari reach asymptot regim learn curv characteris high nois level need fewer datapoint attain asymptot regim stochast process differ covari function differ valu lengthscal nois varianc behav similar way follow discuss result two main subsect result bound e u n present section wherea lower bound section shown section result obtain experi show common characterist show bound learn curv obtain set upper bound e un e un graph figur show empir learn curv confid interv two upper bound e u n curv shown mb mb mb se covari function limit amount train data possibl notic upper error bar associ edn e g n lie actual upper bound effect due variabl generalis error small data set suggest bound quit tight small n effect disappear larg n estim generalis error less sensit composit train set expect twopoint upper bound e u n tighter onepoint upper bound e u note tight upper bound depend upon covari function tighter rougher process such mb get wors smoother process explain recal covari function mb k correspond markov process order k cf section although markov process actual hidden presenc nois e g n still depend train data lie close test point x distant point sinc bound e u calcul use local inform name closest datapoint test point closest datapoint left right respect natur varianc x depend local data point tighter bound becom instanc let us consid mb covari function first order markov process noisefre process knowledg datapoint lie beyond left right neighbour x reduc generalis error x although noisi case distant datapoint process valu train point test point form markov chain knowledg process valu left right test point block reduc generalis error becaus term oe covari matrix k like local inform still import bound learn curv comput mb mb confirm remark looser mb se covari function effect still hold actual enlarg section shown asymptot behaviour bound depend covari function plot upper bound confirm analysi carri section show e u approach asymptot plateaux particular e u tend oe tend qualiti bound process characteris differ length scale differ nois level compar one describ far tight e u still depend smooth process explain begin section variat lengthscal effect rescal number train data observ explicitli asymptot analysi equat decay rate depend factor n fix covari function note bound tighter lower nois varianc due fact lower nois level better hidden markov process manifest itself smaller nois level influenc remot observ learn curv becom closer bound generalis error reli local behaviour process around test data contrari larger nois level hide underli markov process thu loosen bound bound e l n also run experi comput lower bound obtain equat process gener covari prior mb mb mb se equat show evalu e l n involv comput infinit sum term truncat seri consid term add signific contribut sum ie j k oe machin precis sinc contribut seri posit quantiti comput still lower bound learn curv figur show result experi set graph lower bound lie empir learn curv tighter larg amount data particular smoothest process larg amount data confid interv lay actual lower bound lower bound tend nois level oe empir learn curv loglog plot e l n show asymptot decay zero on gammakgammak delta mb k se covari function respect graph figur show also tight bound depend smooth stochast process particular smooth process characteris tight lower bound learn curv e g n explain observ e l n lower bound learn curv upper bound train error valu smooth function larg variat train point thu model infer better test data reduc generalis error pull closer train error sinc two error sandwich bound equat e l n becom tight smooth process also notic tight lower bound depend nois level becom tight high nois level loos small nois level consist gener characterist e l n monoton decreas function nois varianc opper vivarelli paper present nonasymptot upper lower bound learn curv gp theoret analysi carri onedimension gp characteris sever covari function support numer simul start observ increas amount train data never worsen bayesian generalis error upper bound learn curv estim generalis error gp train reduc dataset mean given train set envelop generalis error gener one two datapoint upper bound actual learn curv gp sinc expect generalis error distribut train data analyt tractabl introduc two upper bound e u n n amen averag distribut test train point studi evalu expect valu futur direct research also deal evalu varianc order highlight behaviour bound respect smooth stochast process investig bound modifi bessel covari function order k describ stochast process differenti squar exponenti function describ process mean squaredifferenti order experiment result shown learn curv bound characteris earli linearli decreas behaviour due effect exert datapoint pull surfac prior generalis error also notic tight bound depend smooth stochast process due fact bound reli subset train data ie one two datapoint modifi bessel covari function describ markov process order k although simul markovian process hidden nois learn curv depend mainli local inform bound becom tighter rougher process also investig behaviour curv respect variat correl lengthscal process varianc nois corrupt stochast process notic lengthscal stretch behaviour curv effect rescal number train data nois level effect hide underli markov process upper bound becom tighter smaller nois varianc expans bound limit larg amount data highlight asymptot behaviour depend upon covari function approach asymptot plateau for mb covari smoother process rate decay plateau e u n numer simul support analysi one limit analysi dimens input space bound made analyt tractabl use order statist result split one dimension input space gp higher dimension space partit input space replac voronoi tessel depend data n averag distribut appear difficult one suggest approxim evalu upper bound integr ball whose radiu depend upon number exampl volum input space bound hold case expect effect due larger input dimens loosen upper bound note recent work sol lich deriv good approxim learn curv method appli one dimens also ran experi use lower bound propos opper base knowledg eigenvalu covari function process sinc bound e l n also upper bound train error observ bound tighter smooth process learn curv becom closer train error also nois vari tight e l n low nois level loosen lower bound unlik upper bound lower bound appli also multivari problem easili extend high dimens input space howev verifi opper vivarelli bound becom less tight input space higher dimens appendix a twopoint upper bound e u appendix deriv equat start equat start calcul oe x covari matrix gener two data point theta matrix straightforward evalu oe consid two train data x x i covari matrix refer sollich ad manuscript revis april gp evalu determin k covari vector test point x k varianc assum form chang variabl covari c p as turn upper bound gener oe x interv theta when n d notic that similarli equat also integr delta delta determin delta depend upon length interv integr evalu contribut upper bound interv theta x x n integr varianc oe gener x x n theta x x n respect henc right hand side equat rewritten delta defin equat equat still depend distribut train data function distanc adjac train point similarli equat obtain upper bound independ train data integr equat distribut differ gammac acknowledg research form part valid verif neural network system project fund jointli epsrc grk british aerospac thank dr manfr opper dr andi wright bae help discuss also thank anonym refere comment help improv paper f v support studentship british aerospac r geometri random field new look statist model identif gaussian process bayesian classif via hybrid mont carlo order statist tabl integr stochast linear learn exact test train error averag gener addit model mutual inform inform theori design problem optim surfac interpol network inform criteriondetermin number hidden unit artifici neural network model bayesian learn neural network lectur note statist regress gaussian process averag case per formanc gener bound bay error regress gaussian process upper bound bayesian error bar gener linear regress evalu gaussian process method nonlinear regress almost optim differenti use noisi data multivari integr approxim random field satisfi sack ylvisak condit aspect spline smooth approach nonparametr regress curv filter learn curv gaussian process theori learnabl natur statist learn theori studi generalis gaussian process bayesian neural network predict regul linear least squar method comput infinit network figur a figur figur a tr ctr peter sollich anason hale learn curv gaussian process regress approxim bound neural comput v n p june