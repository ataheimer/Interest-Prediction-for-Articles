t perform predict larg parallel applic use parallel simul a accur simul larg parallel applic facilit use direct execut parallel discret event simul paper describ use compass direct executiondriven parallel simul perform predict program includ commun io intens applic simul use predict perform applic distribut memori machin like ibm sp sharedmemori machin like sgi origin paper illustr use compass versatil perform predict tool use realworld applic synthet benchmark studi applic scalabl sensit commun latenc interplay factor like commun pattern parallel file system cach applic perform also show simul accur predict also effici abil use parallel simul reduc execut time which case yield nearlinear speedup b introduct accur effici perform predict exist parallel applic multipl target architectur challeng problem analyt simul approach use success purpos wherea analyt solut advantag effici also suffer limit mani complex system analyt intract although simul wide applic tool major limit extrem long execut time largescal system number simul includ parallel proteu laps simo wisconsin wind tunnel mpisim develop control execut time simul model parallel program simul typic use direct execut reduc cost simul sequenti instruct use parallel discret event simul exploit parallel within simul reduc impact scale target configur simul exist program simul design studi cpuintens parallel program howev inadequ parallel io perform becom signific deterr overal perform mani applic number solut propos improv parallel io perform abil includ io cpuintens applic unifi perform predict environ thu appear signific valu develop compass componentbas parallel system simul portabl execut driven asynchron parallel discret event simul use predict perform largescal parallel program includ comput io intens applic target execut sharednoth share memori architectur well smp cluster particular simul modul develop predict perform applic function commun latenc number avail processor machin interest differ cach strategi parallel i parallel file system characterist altern implement collect commun i command simul use detail program simul within poem project poem perform orient endtoend model system collabor multiinstitut project whose goal creat experiment evalu problem solv environ endtoend perform model complex paralleldistribut system paper describ simul use evalu perform larg scale complex applic function variou system characterist demonstr simul accur also fast due abil run parallel can use real world applic howev case use synthet benchmark highlight particular featur simul show simul portabl accuraci valid tool two platform the distribut memori ibm sp share memori sgi origin rang synthet real world applic instanc show predict execut time asci kernel call sweepd within measur execut time architectur second demonstr scalabl tool itself major impedi widespread use program simul execut ineffici show compass effect exploit parallel model execut dramat reduc execut time simul model without sacrif accuraci particular show that configur applic kernel call sweepd target machin processor simul reduc slowdown factor use sequenti simul low use parallel simul run processor further larger amount memori avail parallel platform allow us conduct scalabl studi target configur least two order magnitud larger obtain sequenti machin instanc sweepd applic memori constraint sequenti simul would limit us simul target architectur processor problem size use memori avail us node sp abl predict perform sweepd processor establish simul accuraci scalabl demonstr capabl use simul predict scalabl properti applic use standard measur scalabl includ isoeffici scaleup function number processor analyz behavior applic function commun latenc target architectur demonstr applic sweepd sensit latenc variat impli execut applic network workstat rather massiv parallel system reason altern show compass model new architectur consist cluster smp such newest ibm sp even though hardwar smp cluster exist mpi softwar yet avail exploit faster commun avail among processor smp node use compass show applic would perform new architectur fast intranod mpi commun made avail particular use synthet benchmark identifi type applic run faster use four way smp rather sixteen processor use synthet benchmark demonstr sensit differ commun pattern variat commun latenc target architectur parallel file system becom complex allow comput ionod cach demonstr variou cach polici affect perform benchmark particular io intens benchmark see network latenc degrad gain cooper cach becom neglig next section give brief descript simul section describ benchmark target host architectur use perform studi section present result valid scalabl simul section showcas featur simul describ point abov section discuss relat work conclud discuss futur research direct compass goal simul enabl simul largescal parallel applic written use mpi mpiio varieti high perform architectur applic program simul refer target program architectur perform predict refer target architectur machin simul execut refer host machin may sequenti parallel simul environ compos sever distinct yet tightli coupl componentsth simul kernel mpi commun librari simul mpisim parallel io simul piosim parallel file system simul pf sim success compon build upon extend capabl previou compon expand breadth depth perform issu may investig simul simul kernel provid framework implement simul protocol provid support schedul execut thread mpisim provid capabl simul individu collect mpi commun routin piosim extend mpisim capabl includ io routin well provid sever implement collect io abil handl user defin data type need support complex io oper simpl io servic time model pfssim complet simul environ provid detail simul parallel file system multipl cach algorithm simul portabl run varieti parallel platformsth ibm sp origin intel paragon simul kernel heart simul environ gener number processor host machin less number processor target architectur simul simul must support multithread kernel processor schedul thread ensur event processor execut correct timestamp order target thread simul follow local code simul direct execut commun io command trap simul use appropri model predict execut time correspond activ target architectur correspond commun io command also execut consist target program physic time taken execut oper ignor use direct execut simul local code requir processor host target machin similar howev interconnect network parallel io system file system two architectur may differ compass support commonli use mpi commun routin pointtopoint collect commun simul collect commun function implement term pointtopoint commun function pointtopoint commun function implement use set core nonblock mpi function interconnect network model current ignor content network detail model develop given excel valid obtain simpler model varieti benchmark previou work consid seriou limit parallel io compon compass simul individu collect io construct provid mpiio construct includ creat open close delet file data access readwrit oper local datatyp constructor introduc part mpiio specif file system compon compass simul parallel file system use servic io request gener mpiio program compon selfcontain may replac simpl disk access model order speed simul whenev detail system model requir howev use detail model allow studi wide varieti parallel file system configur basic structur function file system compon taken vesta parallel file system highli scalabl experiment file system develop ibm behavior physic disk simul set disk model includ simpl model base seek time rotat latenc data transfer rate well highli detail model develop dartmouth detail system simul slow parallel simul potenti reduc execut time model provid greater amount memori anoth necess larg detail simul simul kernel provid support sequenti parallel execut simul parallel execut support via set conserv parallel simul protocol combin kernel builtin multithread capabl allow simul effect use howev mani host processor avail without limit size type experi may run simul also support number optim base analysi behavior parallel applic among optim made avail program behavior analysi techniqu allow simul protocol describ actual turn off elimin costli overhead global synchron in submiss benchmark system real world applic benchmark sweepd sweepd solver threedimension time independ neutron particl transport comput calcul flux particl given region space flux region depend flux neighbor cell threedimension space xyz discret threedimension cell ijk comput progress wavefront manner eight octant space octant contain six independ angl angl correspond six independ direct flux one face cubecel sweepd use domain decomposit onto array processor j direct configur sweep progress processor comput flux column cell send outgo flux inform two neighbor processor order improv perform k dimens angl divid block allow processor calcul part valu dimens angl send valu neighbor processor na benchmark na parallel benchmark npb suit parallel scientif benchmark made avail numer aerodynam simul na project nasa ame research center na suit contribut strong core experiment set repres number wellknown differ realworld nonvendorspecif code easili tailor util compass system use npb releas softwar includ varieti applic four bt lu mg sp deem stabl author bt sp lu comput solut system discret navierstok equat mg solv threedimension scalar poisson equat npb distribut provid preconfigur set problem size becaus f constraint dynam memori program oper applic problem size are increas order s a b c furthermor program run parallel specif number processor bt sp run processor lu mg run npb suit sweepd origin program fortran mpisim current support c program first translat use fc subsequ translat code automat local allow simul simul multipl simultan thread target program singl processor host machin local also convert mpi mpii call equival call defin within compass librari local fulli autom use success larg applic synthet benchmarkssampl although real world applic kernel like sweepd na use benchmark simul compass major disadvantag core algorithm difficult understand imposs modifi evalu impact altern type program structur includ comput granular commun pattern program provid mean paramet adjust larg granular chang could made serv need measur perform function specif runtim behavior thu addit use real world benchmark sought write synthet applic allow explicit tune commun comput paramet effort result sampl synthet applic messagepass librari environ c program perform precis changeabl amount calcul messagepass interprocess commun suitabl experiment analysi sampl execut messag pass via call target either compass actual mpi librari facilit valid sampl simpl loop contain two inner loop first pure comput loop whose durat vari adjust number float point divis execut second commun loop implement multipl commun pattern chang frequenc size destin messag sent and receiv process messag distribut take wide varieti pattern describ use mpi pointtopoint capabl implement number method wavefront nearest neighbor ring onetoal alltoal commun use predefin metric user easili chang commun comput ratio program io benchmark sinc implement mpii standard yet wide avail hard find real world applic stress parallel io simul capabl simul henc set synthet benchmark develop purpos benchmark use n process map uniqu comput node process gener read write request block data given size interarriv time io request sampl normal random distribut given mean block file distribut across io node disk for total md disk process issu r request given c first rc request use warm cach paramet easili modifi host target architectur sgi origin multiprocessor system silicon graphic inc origin provid cachecoher numa distribut share memori layout two mip r processor compris process node multiplex hub chip reduc memori latenc increas memori bandwidth origin testb small ten mhz r processor share mb memori due limit number processor memori could complet perform number size experi ibm sp ibm scalabl parallel sp system scalabl multiprocessor condens sever complet rs workstat one system form sharednoth collect process node connect typic bidirect ari fli multistag interconnect network achiev simultan anytoani connect packetswitch network use altern ip protocol name user space commun subsystem us css provid nearconst latenc bandwidth use us css baselin protocol experi sp new gener ibm sp showcas cluster architectur node machin way smp exampl machin new ibm sp lawrenc livermor nation laboratori current machin includ comput node four mhz e processor share mb memori attach gb disk internod commun sp give bandwidth mbsecond latenc microsecond use sp high perform switch tb current perform possibl applic run one four processor node simul behavior system run mpi applic internod commun handl way sharednoth architectur model commun processor use highperform switch howev intranod commun model use share memori inform implement mpi construct design exploit share memori yet avail ibm sp in fact current implement processor node commun use much slower ip compass model base mpi implement sgi origin certainli perform applic depend exact implement allow us demonstr capabl tool enabl studi valid perform compass valid first set experi aim valid predict compass ibm sp sgi origin figur graph execut time measur sweepd program compar execut time predict compass curv function number processor use sweepd number target processor simul compass valid thu limit number physic processor on origin compass data taken averag run time multithread combin target processor number for instanc eight target processor averag run time taken execut host processor graph seen compass inde accur correctli predict execut time benchmark within ibm sp ok even multithread oper conduct scalabl studi often case number avail host processor significantli less number target processor result sever simul thread run processor sinc multithread might affect result simul thread might affect other runtim import studi whether effect exist quantifi effect multithread abil simul correctli predict runtim applic simul sweepd use wide rang host target processor seen figur even rel high degre multithread target mpi process singl host processor variat predict runtim small below runtim sweepdon ibmsp problems mk mmi processor runtim measuredruntim figur valid compass sweepd ibm sp number processor time second measur predict figur valid compass sweepd ok hostprocessor target target target target target target figur effect compass multithread predict perform ibm sp compass also valid suit na benchmark present result sp bt benchmark origin mention earlier na program come configur run parallel predetermin number processor predetermin set problem size processor memori constraint rel small ok restrict us size benchmark figur show result valid experi bt sp both class s show good valid accuraci within respect point processor graph show predict perform sinc host processor avail machin sinc sweepd na benchmark comput intens also use communicationintens synthet benchmark sampl valid commun model measur predict execut time sampl benchmark also show excel valid compass varieti configur figur show sampl run use wavefront commun pattern computationtocommun ratio seen figur compass accur predict run time within percent result similar pattern omit breviti valid nasspandbt ok number processor btmea btpred spmea sppred figur valid na benchmark ok valid sampleon ok number processor measur predict figur valid sampl ok scalabl simul present number result demonstr rel improv perform simul obtain parallel execut figur a show perform compass simul execut sweepd problem size cell target processor ibm sp seen figur simul effect use addit processor parallel simul processor achiev speedup almost problem size compar sequenti execut time simul speedup problem size smaller becaus ultim perform simul bound perform applic anoth metric commonli use evalu perform simul slowdown simul rel target architectur defin slowdownst time simul applic use host processor time execut applic processor figur b show slowdown compass simul target problem size number host processor equal number target processor simul slowdown factor less host architectur fewer avail processor target machin slowdown get wors overal perform reason thu iratio number target processor number host processor target processor host processor slowdown factor speedup compass run target processor sweepd mk mmi ibmsp processor figur a speedup compass ibm sp sweepd slowdownof compass sweepd mk host processor avail figur slowdown compass ibm spsweepd largest configur studi target processor use host processor iratio yield slowdown consider better slowdown factor report program simul like wwt laps slowdown factor report high comput intens applic figur a show speedup attain compass origin simul processor two problem size sweepd origin compass achiev nearlinear speedup number host processor increas reach speedup host processor use slowdown graph target processor configur shown figur b show i ratio simul slowdown slowdown host processor slightli show even half desir number processor avail simul run twice slower applic target processor would speedup compassrunningsweepdwith target processor number host processor figur compass sgi origin sweepd speedup slowdown compass simul na benchmark also show improv parallel execut albeit lesser degre figur a a show speedup bt sp applic respect see speedup simul increas progress number host processor increas rate increas well final speedup attain host lower seen previou benchmark simul produc speedup high bt benchmark sp benchmark similarli slowdown curv reach low respect applic see figur b b investig indic applic scale well sweepd henc differ perform compass directli relat perform target program simul speedup slowdown experi show compass exploit parallel avail applic without ad consider overhead number host processor target figur perform compass sgi origin sweepd number host processor target target figur a speedup compass sgi origin bt number host processor target figur b slowdown compass sgi origin k bt result featur compass scalabl sweepdth perform studi first evalu scalabl sweepd function variou paramet includ size problem number processor function network latenc perform studi ibm sp use host processor figur a demonstr scalabl sweepd three problem size larg problem studi show perform scale well number processor increas almost although rel improv perform drop beyond processor largest problem size runtim applic shown time smaller run processor compar run applic processor smaller problem size element perform appear peak processor subsequ get wors observ strengthen isoeffici analysi effici defin speedup sp number processor isoeffici function determin rate problem size need increas respect number processor maintain fix effici system highli scalabl problem size need increas linearli function number processor total work w time run algorithm singl processor p time run algorithm p processor sum overhead processor give effici need grow fep maintain effici e fep defin isoeffici function speedupof compassrunningsp number host processor target target figur compass origin k sp slowdown compassrun sp number processor target figur slowdown compass origin k sp performanceof sweepd mk mmi ibmsp processor predict runtim figur a scalabl sweepd ibm sp figur b show isoeffici function sweepd variou number effici graph show problem size need maintain given effici given number processor first observ maintain even effici hard howev manag second use larg number processor given problem size effici exampl about problem size use less processor give best effici about use processor result effici sinc run problem processor might result slow runtim tradeoff time effici made processor use result effici figur b also demonstr isoeffici hard captur simpl extrapol exampl isoeffici curv flatten million problem size impli give processor applic improv effici isoeffi functionmkmmi processor problem size in figur isoeffici sweepd ibm sp impact latenc variat perform also studi effect commun latenc perform figur show perform sweepd latenc network vari problem size seen figur faster commun switch signific impact applicationth perform chang variat latenc x current switch latenc processor larger problem differ neglig howev perform appear suffer significantli latenc increas factor might case applic port network workstat latenc impact much signific small number processor processor contain larger portion comput region caus messag becom larg sensit latenc model smp cluster architectur preced experi evalu applic perform distribut memori architectur new architectur ibm sp cluster architectur use way smp node describ section exploit fast memori access share memori system scalabl distribut memori machin next set experi project improv execut time benchmark obtain migrat architectur sinc previou experi show na sweepd benchmark rel insensit commun latenc hardli surpris appear benefit notic fast intranod commun for breviti omit result howev demonstr sampl benchmark applic higher percentag commun new architectur appear offer benefit problems mk processor runtim second xsp sp xp figur a sensit sweepd latenc small problem size problems mk mmi ibmsp processor runtim second xsp sp xsp xsp figur b sensit sweepd latenc larg problem size figur a show perform sampl fix problem size per processor see simul valid well one processor per node case mea non smp compass also notic predict slightli better perform run smp node would support fast intranod commun compass smp even though current implement mpi commun smp node poor perform mea smp similarli figur b show perform sp run sampl function number comput iter here time commun total runtim number iter increas ratio comput commun constant again see predict smp perform improv averag compar singl processor per node perform see clear drawback use intranod commun support current mea current smp even though mpi sp support fast intranod commun processor smp share main memori might tempt applic develop redesign exist mpi applic use main memori processor node mpi node simul like compass help make decis invest time effort would result better perform numberofprocessor mea nonsmp mea current compass compassfor figur smp perform ibm sp sampl constant comput per comput loop runtim second mea nonsmp mea current compass compass figur smp perform ibm sp sampl increas comput per processor simul common commun pattern anoth set experi involv investig impact differ commun pattern program perform use synthet benchmark sampl scientif program produc wide varieti traffic pattern depend algorithm use sought understand differ type messag dispers affect applic perform sampl benchmark use gener number messagepass scheme studi wavefront pattern involv dimension mesh th processor resid upperlefthand corner initi commun wave toward lowerrighthand corner use mesh layout nearestneighbor dispers processor send and receiv messag four logic adjac processor ring pattern form cycl singl messag token sent around logic ring processor final onetoal pattern processor broadcast messag rout use broadcast tree other perform variou commun pattern evalu function commun latenc number processor host machin select experi origin processor figur a show perform sampl function latenc target ok architectur processor figur b show perform function number processor target architectur expect ring pattern sensit latenc processor count messag travers sequenti ring somewhat surpris result rel insensit wavefront ontoal commun howev note pattern block initi processor immedi initi commun correspond process execut next iter henc reason well overlap commun produc observ insensit slight jump predict execut time increas processor attribut chang depth broadcast tree figur b sensit commun pattern latenc latenc xorigin nearest neighbor onetoal ring wavefront figur perform commun pattern function latenc scalabilityof communicationpattern processor nearestneighbor onetoal ring wavefront figur perform commun pattern function number processor ok effect latenc parallel file system cach strategi last experi demonstr use simul evalu impact architectur featur io intens program cooper cach techniqu propos improv perform applic larg io requir suggest cach be least partial manag global rather entir local manner case comput node cnode io node ionod cach base cach simpli allow node manag cach greedi forward allow ionod cach miss check node cach requir data befor go fetch disk central coordin cach portion cnode cach collect manag ionod remain portion cnode cach manag local cnode percentag coordin manag cach vari as experi global manag cach similar coordin cach except strategi block placement cach modifi allow ionod cach hold data evict cnode cach cach techniqu depend effici access remot memori order improv cach hit rate applic perform perform depend commun latenc network figur show result set experi design measur impact chang network latenc ibm sp cooper cach techniqu support compass benchmark process separ comput node randomli read write byte block data block file distribut across io node disk for total disk process issu request first request use warm cach request read request graph plot predict execut time benchmark network latenc increas cach perform base cach no cooper greedi forward central coordin with percent coordin global manag cach shown network latenc time latenc interconnect absoluteperformanceof cachingtechniqu networklat xsp latenc base greedi global figur comparison cach techniqu ibm sp understand network latenc increas predict execut time benchmark also increas howev experi also hint extrem sensit cooper cach techniqu increas network latenc may appear cach techniqu even base cach equal affect increas network latenc found case absolut differ predict execut time diminish slightli latenc increas rel differ differ cach techniqu decreas markedli shown figur b effect network becom slower benefit use cooper cach lost perform degrad slightli better base cach result import implic use techniqu larg network workstat design hybrid strategi cach manag cooper small region network rather entir network perform cooper cachingrel tobas network latenc x sp latenc base greedi global figur perform cach techniqu rel base cach ibm sp relat work accur effici perform predict exist parallel applic target machin thousand processor challeng problem first gener simul like proteu use sequenti simul slow slowdown factor rang process target program led mani effort improv execut time program simul dpsim laps parallel proteu simo wisconsin wind tunnel tango mpisim design purpos simul typic use direct execut portion code reduc cost simul sequenti instruct typic use variat conserv parallel discreteev simul algorithm exploit parallel within simul reduc impact scale target machin size mani parallel simul use synchron approach simul simul process synchron global fix time interv order maintain program correct interv quantum taken larger commun latenc network simul guarante messag sent one quantum cannot receiv next interv also impli messag process correct order synchron simul proteu parallel architectur simul engin tango share memori architectur simul engin tunnel wwt share memori architectur simul engin simo complet system simul multipl program plu oper system term simul commun two simul engin use approach similar parallel proteu laps distinguish featur compass portabl part due implement use mpi sinc mpi readili avail parallel distribut system simul abl use data movement synchron hand laps design specif run intel paragon use paragon nativ commun primit made laps broad use limit compass also fast slowdown around proteu typic slowdown rang number simul also design simul io oper although tend use sequenti simul set collect io implement compar use starfish simul base proteu hybrid methodolog evalu perform parallel io subsystem describ pio tracedriven io simul use calcul perform io system subset problem evalu analyt model use remaind scalabl distribut memori machin examin use applic kernel investig network perform content librari also develop ppf portabl parallel file system librari design sit top multipl uf instanc provid wide varieti parallel file system capabl cach prefetch data distribut compass environ describ paper use parallel io system simul detail perhap simul combin abil integr interconnect network io file system scalabl studi also use simul data parallel program compil messagepass code addit simul highli scalabl slowdown factor singl digit larg target applic architectur conclus futur research demonstr compass use studi wide rang applic function varieti architectur characterist rang standard scalabl studi network stress test parallel io properti shown compass accur have valid multipl applic architectur within percent physic measur also fast achiev excel perform ibm sp well sgi origin achiev nearlinear speedup highli parallel applic suffer moder slowdown shown use wide rang architectur perform studi combin separ area io parallel file system perform interconnect network commun librari simul compass use detail program simul within poem project collabor poet work develop hybrid perform model combin analyt simul model techniqu also part project compass integr detail memori processor model allow us break away depend requir host processor architectur similar target processor architectur direct execut simul also provid opportun extend use parallel simul techniqu processor memori simul acknowledg work support advanc research project agenc darpacsto contract fc scalabl system softwar measur evalu darpaito contract nc endtoend perform model larg heterogen adapt paralleldistribut computercommun system thank offic academ comput ucla paul hoffman help ibm sp well lawrenc livermor nation laboratori use ibm sp mani experi execut r parallel simul parallel file system io program the na parallel benchmark a methodolog evalu parallel io perform massiv parallel processor proteu highperform parallel architectur simul distribut simul case studi design verif distribut program the vesta parallel file system avoid cachecoher problem paralleldistribut file system the rice parallel process testb parallel comput architectur hardwaresoftwar approach remot client memori improv file system perform multiprocessor simul trace use tango poem endtoend perform design larg parallel adapt comput system parallel direct execut simul messagepass parallel program fc fortran c convert ppf high perform portabl parallel file system analysi scalabl parallel algorithm architectur survey introduct parallel comput design analysi algorithm tune starfish the sgi origin ccnuma highli scalabl server asci bluepacif ibm rs tr system lawrenc livermor nation laboratori reduc synchron overhead parallel simul mpi perform studi sgi origin wisconsin wind tunnel ii fast portabl parallel architectur simul the galley parallel file system parallel simul data parallel program perform predict parallel program use parallel simul evalu mpi program the wisconsin wind tunnel virtual prototyp parallel comput improv parallel io via twophas runtim access strategi use simo machin simul studi complex comput system a simul base scalabl studi parallel system the asci sweepd benchmark code tr rice parallel process testb analysi scalabl parallel algorithm architectur introduct parallel comput wisconsin wind tunnel simulationbas scalabl studi parallel system ppf vesta parallel file system galley parallel file system reduc synchron overhead parallel simul parallel direct execut simul messagepass parallel program use simo machin simul studi complex comput system sgi origin poem mpisim parallel simul parallel file system io program parallel comput architectur avoid cachecoher problem paralleldistribut file system parallel simul data parallel program tune starfish proteu highperform parallelarchitectur simul perform predict parallel program ctr sundeep prakash ewa deelman rajiv bagrodia asynchron parallel simul parallel program ieee transact softwar engin v n p may clia l o kawabata regina h c santana marco j santana sarita m bruschi kalinka r l j castelo branco perform evalu cmb protocol proceed th confer winter simul decemb monterey california rajiv bagrodia ewa deelman thoma phan parallel simul largescal parallel applic intern journal high perform comput applic v n p februari leo t yang xiaosong frank mueller crossplatform perform predict parallel applic use partial execut proceed acmiee confer supercomput p novemb thoma phan rajiv bagrodia optimist simul parallel messagepass applic proceed fifteenth workshop parallel distribut simul p may lake arrowhead california unit state vikram s adv rajiv bagrodia ewa deelman thoma phan rizo sakellari compilersupport simul highli scalabl parallel applic proceed acmiee confer supercomput cdrom pe novemb portland oregon unit state vikram adv rizo sakellari applic represent multiparadigm perform model largescal parallel scientif code intern journal high perform comput applic v n p novemb ewa deelman rajiv bagrodia rizo sakellari vikram adv improv lookahead parallel discret event simul largescal applic use compil analysi proceed fifteenth workshop parallel distribut simul p may lake arrowhead california unit state vikram s adv rajiv bagrodia ewa deelman rizo sakellari compileroptim simul largescal applic high perform architectur journal parallel distribut comput v n p march ihsin chung jeffrey k hollingsworth use inform prior run improv autom tune system proceed acmiee confer supercomput p novemb vikram s adv rajiv bagrodia jame c brown ewa deelman aditya dube elia n housti john r rice rizo sakellari david j sundaramstukel patricia j teller mari k vernon poem endtoend perform design larg parallel adapt comput system ieee transact softwar engin v n p novemb murali k nethi jame h aylor mix level model simul larg scale hwsw system high perform scientif engin comput hardwaresoftwar support kluwer academ publish norwel ma