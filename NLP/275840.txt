t effici spars lu factor partial pivot distribut memori architectur a abstracta spars lu factor base gaussian elimin partial pivot gepp import mani scientif applic still open problem develop high perform gepp code distribut memori machin main difficulti partial pivot oper dynam chang comput nonzero fillin structur elimin process paper present approach call s parallel problem distribut memori machin s approach adopt static symbol factor avoid runtim control overhead incorpor lu supernod partit amalgam strategi improv cach perform exploit irregular task parallel embed spars lu use asynchron comput schedul paper discuss compar algorithm use data map scheme present experiment studi craytd te perform result set nonsymmetr benchmark matric encourag s achiev gflop te node best knowledg highest perform ever achiev challeng problem previou record gflop share memori machin b current comput scienc depart univers illinoi urbanachampaign pivot oper interchang row base numer valu matrix element elimin process imposs predict precis structur l u factor without actual perform numer factor adapt irregular natur spars lu data structur make effici implement algorithm hard even modern sequenti machin memori hierarchi sever approach use solv nonsymmetr system one approach unsymmetricpattern multifront method use elimin graph model irregular parallel guid parallel comput anoth approach restructur spars matrix border block upper triangular form use special pivot techniqu preserv structur maintain numer stabil accept level method implement illinoi cedar multiprocessor base aliant share memori cluster paper focus parallel issu given column order row interchang maintain numer stabil parallel spars lu partial pivot also studi share memori machin use static symbol lu factor overestim nonzero fillin avoid dynam variat lu data structur approach lead good speedup processor sequent machin work need assess perform sequenti code far know publish result parallel spars lu popular commerci distribut memori machin craytdt intel paragon ibm sp tmc cm meiko cs one difficulti parallel spars lu machin util sophist uniprocessor architectur design sequenti algorithm must take advantag cach make previous propos techniqu less effect hand parallel implement must util fast commun mechan avail ma chine easi get speedup compar parallel code sequenti code fulli exploit uniprocessor capabl easi parallel highli optim sequenti code one sequenti code superlu use supernod approach conduct sequenti spars lu partial pivot supernod partit make possibl perform numer updat use bla level dens matrixvector multipl therefor better exploit memori hierarchi superlu perform symbol factor gener supernod fli factor proce umfpack anoth competit sequenti code problem neither superlu umfpack alway better code spars matric symmetr pattern regard high qualiti deliv excel megaflop perform paper focu perform analysi comparison superlu code sinc structur code closer superlu paper present approach call consid follow key strategi togeth parallel spars lu algorithm adopt static symbol factor scheme elimin data structur variat caus dynam pivot data regular spars structur obtain symbol factor scheme effici dens oper use perform comput impact nonzero fillin overestim overal elimin time minim develop schedul techniqu exploit maximum irregular parallel reduc memori requir solv larg problem observ current commod processor memori hierarchi highli optim bla subroutin usual outperform bla subroutin implement numer oper afford introduc extra bla oper redesign lu algorithm new algorithm easi parallel sequenti perform code still competit current best sequenti code use static symbol factor techniqu first propos predict worst possibl structur l u factor without know actual numer valu develop d lu supernod partit techniqu identifi dens structur l u factor maxim use bla level subroutin dens structur also incorpor supernod amalgam techniqu increas granular comput exploit irregular parallel redesign spars lu algorithm experi two map method one use d data map use d data map one advantag use d data map correspond lu algorithm easili model direct acycl task graph dag graph schedul techniqu effici runtim support avail schedul execut dag parallel schedul execut dag parallel difficult job parallel spars problem irregular execut must asynchron import optim overlap comput commun balanc processor load elimin unnecessari commun overhead graph schedul excel job exploit irregular parallel lead extra memori space per node achiev best perform also d data map expos limit parallel due restrict also examin d data map method asynchron execut scheme exploit parallel memori constraint implement spars lu algorithm conduct experi set nonsymmetr benchmark matric craytd te experi show approach quit effect deliv good perform term high megaflop number particular d code outperform current d code processor suffici memori d code potenti solv larger problem produc higher megaflop number rest paper organ follow section give problem definit section describ structur predict d lu supernod partit spars lu factor section describ program partit data map scheme section address asynchron comput schedul execut section present experiment result section conclud paper find ja mk singular stop row k row m ik kj ik figur spars gaussian elimin partial pivot lu factor preliminari figur show nonsingular matrix factor two matric l u use gepp elimin step control loop index k element manipul step k use row index j column index convent use rest paper step elimin process row interchang may need maintain numer stabil result lu factor process express by l unit lower triangular matrix u upper triangular matrix p permut matrix contain row interchang inform solut linear system solv two triangular solver y triangular solver much less time consum gaussian elimin process cach behavior play import role achiev good perform scientif comput better exploit memori hierarchi modern architectur supernod partit import techniqu exploit regular spars matrix comput util bla routin speed comput success appli choleski factor difficulti nonsymmetr factor supernod structur depend pivot choic factor thu cannot determin advanc superlu perform symbol factor identifi supernod fli also maxim use bla level oper improv cach perform spars lu howev challeng parallel superlu distribut memori machin use precis pivot inform elimin step certainli optim data space usag reduc commun improv load balanc benefit could offset high runtim control commun overhead strategi static data structur predict valuabl avoid dynam symbol factor identifi maximum data depend pattern minim dynam control overhead use static strategi approach overestim introduc extra fillin lead substanti amount unnecessari oper numer factor observ superlu dgemv routin the bla level dens matrix vector multipl account float point oper exclud symbol factor part also fact bla routin dgemm matrixmatrix multipl usual much faster bla bla routin craytd matrix size theta dgemm achiev mflop dgemv reach mflop thu key idea approach could find way maxim use dgemm use static symbol factor even overestim nonzero extra numer oper overal code perform could still competit superlu mainli use dgemv storag predict dens structur identif storag predict purpos symbol factor obtain structur l u factor sinc pivot sequenc known numer factor way alloc enough storag space fillin gener numer factor phase overestim given spars matrix zerofre diagon simpl solut use choleski factor l c a shown structur l c use upper bound structur l u factor regardless choic pivot row step turn bound tight often substanti overestim structur l u factor refer tabl instead consid anoth method basic idea static consid possibl pivot choic step space alloc possibl nonzero would introduc pivot sequenc could occur numer factor summar symbol factor method briefli follow nonzero structur row defin set column indic nonzero fillin present given n theta n matrix a sinc nonzero pattern row chang factor proce use r k denot structur row step k factor k denot structur matrix step k k ij denot element ij k notic structur row whole matrix cover structur l u factor addit process symbol factor assum exact numer cancel occur thu ij structur nonzerog also defin set candid pivot row step k follow ik structur nonzerog assum kk alway nonzero nonsingular matrix zerofre diagon alway possibl permut row matrix permut matrix zerofre diagon though symbol factor work matrix contain zero entri diagon prefer make overestim gener symbol factor process iter n step step k row structur updat as r essenti structur candid pivot row step k replac union structur candid pivot row except column indic less k way guarante result structur n abl accommod fillin introduc possibl pivot sequenc simpl exampl figur demonstr whole process nonzero fillin figur first step symbol factor sampl theta spars matrix structur remain unchang step symbol factor appli order perform matrix reduc fillin order current use multipl minimum degre order a also permut row matrix use transvers obtain duff algorithm make zerofre diagon transvers often help reduc fillin test storag impact overestim number nonsymmetr test matric variou sourc result list tabl fourth column tabl origin number nonzero fifth column measur symmetri structur origin matrix bigger symmetri number is nonsymmetr origin matrix is unit symmetri number indic matrix symmetr matric nonsymmetr numer valu compar number nonzero obtain static approach number nonzero obtain superlu well choleski factor a matric result tabl show overestim usual lead less extra nonzero superlu scheme doe extra nonzero impli addit comput cost exampl one either check symbol nonzero actual nonzero numer factor directli perform arithmet oper could unnecessari aggreg float point oper maxim use bla subroutin sequenti code perform still competit even fifth column tabl show float oper overestim approach high time result section show actual ratio run time much less thu necessari benefici identifi dens structur spars matrix static symbol factor note case static symbol factor lead excess overestim exampl memplu matrix case static scheme produc time mani nonzero superlu doe fact case order superlu appli base instead a otherwis overestim ratio use superlu also anoth matrix wang static scheme produc time mani nonzero superlu doe code still produc gflop node te paper focus develop high perform parallel code overestim ratio high futur work studi order strategi minim overestim ratio factor entriesjaj superlu matrix er tabl test matric statist d lu supernod partit dens structur identif supernod partit commonli use techniqu improv cach perform spars code symmetr spars matrix supernod defin group consecut column nest structur l factor matrix excel perform achiev use supernod partit choleski factor howev definit directli applic spars lu nonsymmetr matric good analysi defin unsymmetr supernod l factor avail notic supernod may need broken smaller one fit cach expos parallel superlu approach l supernod partit regular dens structur u factor could make possibl use bla routin see figur a howev approach dens column or subcolumn u factor identifi static symbol factor see figur b u partit strategi explain follow l supernod partit obtain spars matrix a ie set column block possibl differ block size partit appli row matrix break supernod panel submatric offdiagon submatrix l part either dens block contain dens block furthermor follow theorem identifi dens structur pattern u factor key maxim use bla subroutin algorithm a b figur a illustr dens structur u factor superlu approach b dens structur u factor approach follow theorem show d lu partit strategi success rich set dens structur exploit follow notat use rest paper ffl l u partit divid column n column block row n row block whole matrix divid n theta n submatric submatric u factor denot u ij submatric l factor denot l ij denot diagon submatrix use ij denot submatrix necessari distinguish l u factor ffl defin si start column or row number ith column or row block conveni defin sn ffl subcolumn or subrow column or row submatrix simplic use global column or row index denot subcolumn or subrow submatrix exampl subcolumn k submatrix block u ij mean subcolumn submatrix global column index k similarli use ij indic individu nonzero element base global indic compound structur l u submatrix subcolumn subrow ffl compound structur nonzero contain least one nonzero element fillin use ij indic block ij nonzero notic algorithm need oper nonzero compound structur compound structur structur dens element nonzero fillin follow differenti nonzero fillin entri consid nonzero element theorem given spars matrix zerofre diagon static symbol factor d lu supernod partit perform a nonzero submatrix u factor contain structur dens subcolumn proof recal p k set candid pivot row symbol factor step k given supernod span column k k s definit fact step k static symbol factor affect nonzero pattern submatrix knkn zerofre diagon notic step k final structur row i p k updat symbol factor procedur r structur row k k s interest nonzero pattern u part exclud part belong l kk call partial structur ur thu ur seen kth step updat ur k know structur row k unchang step k need prove ur k ks shown below infer nonzero structur row k k subcolumn u part either structur dens zero sinc p k oe p k clear that similarli show ur ks k theorem show lu partit gener rich set structur dens subcolumn even structur dens submatric u factor also incorpor result supernod amalgam section experi indic numer updat perform bla routin dgemm show effect lu partit method figur demonstr result supernod partit theta sampl spars matrix one see submatric upper triangular part matrix contain structur dens subcolumn base theorem show structur relationship two submatric supernod column block use implement algorithm detect nonzero structur effici numer updat corollari given two nonzero submatric u ij u k u ij structur dens subcolumn k u j also structur dens nonzero figur exampl lu supernod partit proof corollari illustr figur sinc l nonzero must structur dens subrow l lead nonzero element subcolumn k u subcolumn k u ij structur dens accord theorem subcolumn k u j structur dens u figur illustr corollari corollari given two nonzero submatric u ij u structur dens u must structur dens proof straightforward use corollari supernod amalgam test spars matric averag size supernod lu partit small column result fine grain task amalgam small supernod lead great perform improv parallel sequenti spars code improv cach perform reduc interprocessor commun overhead could mani way amalgam supernod basic idea relax restrict column supernod must exactli nonzero structur diagon amalgam usual guid supernod elimin tree parent could merg children merg introduc mani extra zero entri supernod row column permut need parent consecut children howev column permut introduc amalgam method could undermin correct static symbol factor use simpler approach requir permut approach amalgam consecut supernod nonzero structur differ small number entri perform effici manner time complex on control maximum allow differ amalgam factor r experi show r rang give best perform test matric lead improv execut time sequenti code reason get bigger supernod get larger dens structur although may zero entri them take advantag bla kernel notic appli supernod amalgam dens structur identifi theorem strictli dens more call almostdens structur still use result theorem minor revis summar follow corollari result present section obtain use amalgam strategi corollari given spars matrix a supernod amalgam appli static symbol factor d lu supernod partit perform a nonzero submatrix u factor contain almoststructurallydens subcolumn program partit task depend processor map divid spars matrix submatric use lu supernod partit need partit lu code accordingli defin coars grain task manipul partit dens data structur program partit column block partit follow supernod structur typic two type task one f actork factor column kth column block includ find pivot sequenc associ column updatek j appli pivot sequenc deriv f actork jth column block modifi jth column block use kth column block instead perform row interchang right part matrix right pivot search techniqu call delayedpivot use techniqu pivot sequenc held factor kth column block complet pivot sequenc appli rest matrix ie interchang row delayedpivot import especi parallel algorithm equival aggreg multipl small messag larger one owner kth column block send column block pack togeth pivot inform processor outlin partit spars lu factor algorithm partial pivot describ figur code f actork summar figur use bla bla subroutin comput cost numer factor mainli domin task function task updatek j present figur line use dens matrix multipl perform task f actork perform task updatek j figur partit spars lu factor partial pivot find pivot row column m row row column block k scale column updat rest column column block figur descript task f actork use direct acycl task graph dag model irregular parallel aris partit spars lu program dag construct static numer factor previou work exploit task parallel spars choleski factor use elimin tree eg good way expos avail parallel pivot requir spars lu elimin tree directli reflect avail paral lelism dynam creat dag use model parallel guid runtim execut nonsymmetr multifront method given task definit figur defin structur spars lu task graph follow four properti necessari ffl n task f actork k n ffl task updatek dens matrix total nn gamma updat task ffl depend edg f actork task updatek j interchang row accord pivot sequenc lower triangular part l kk submatrix u kj dens els dens subcolumn c u u kj nonzero submatrix ij submatrix u kj dens els dens subcolumn c u u kj b correspond dens subcolumn ij figur descript task updatek j ffl depend updatek k f actork exist task updatet k add one properti necessari simplifi implement properti essenti allow exploit commut among updat task howev accord experi choleski factor perform loss due properti substanti averag graph schedul use ffl depend updatek j updatek exist task updatet j figur a show nonzero pattern partit matrix shown figur figur b correspond task depend graph d data map d data map submatric l u part column block resid processor column block map processor cyclic manner base schedul techniqu graph schedul task assign base ownercomput rule ie task modifi column block assign processor own column block one disadvantag map serial comput singl f actork word singl f actork updatek task perform b figur a nonzero pattern exampl matrix figur b depend graph deriv partit result conveni f use denot f actor u use denot updat one processor map strategi advantag pivot search subrow interchang done local without commun anoth advantag parallel model depend structur effect exploit use graph schedul techniqu data map literatur d map shown scalabl d spars choleski howev sever difficulti appli d blockori map case spars lu factor even static structur predict firstli pivot oper row interchang requir frequent wellsynchron interprocessor commun submatric column block assign differ processor effect exploit limit irregular parallel d case requir highli effici asynchron execut mechan delic messag buffer manag secondli difficult util schedul possibl irregular parallel spars lu lastli manag low space complex anoth issu sinc exploit irregular parallel maximum degre may need buffer space d algorithm use simpl standard map function scheme p avail processor view two dimension grid c nonzero submatrix block ij could l block u block assign processor p mod pr j mod pc d data map consid scalabl d data map enabl parallel singl f actork updatek j task p r processor discuss d parallel exploit use asynchron schedul execut parallel exploit schedul runtim support d method discuss d spars lu task schedul execut parallel time minim georg ng use dynam load balanc algorithm share memori machin distribut memori machin dynam adapt load balanc work well problem coars grain comput still open problem balanc benefit dynam schedul runtim control overhead sinc task data migrat cost expens spars problem mix granular use task depend graph guid schedul investig two type schedul scheme ffl computeahead schedul ca use blockcycl map task computeahead execut strategi demonstr figur idea use speed parallel dens factor execut numer factor layer layer base current submatrix index parallel exploit concurr updat order overlap comput commun f actork execut soon f actork updatek k pivot sequenc column block k next layer commun earli possibl ffl graph schedul order task execut within processor use graph schedul algorithm basic optim balanc processor load overlap comput commun hide commun latenc done util global depend structur critic path inform column block local perform task f actor broadcast column block pivot sequenc local receiv column block k pivot choic row accord pivot sequenc perform task f actork broadcast column block k pivot sequenc local column block k receiv receiv column block k pivot choic row accord pivot sequenc perform task updatek j figur d code use computeahead schedul graph schedul shown effect exploit irregular parallel applic eg graph schedul outperform ca schedul spars lu constraint order f actor task demonstr point use lu task graph figur exampl gantt chart ca schedul schedul deriv graph schedul algorithm list figur assum task comput weight edg commun weight easi see schedul approach produc better result ca schedul look ca schedul care see reason ca look ahead one step execut task f actor place updat hand graph schedul algorithm detect f actor execut updat lead better overlap commun comput pa figur a schedul deriv graph schedul algorithm b computeahead schedul conveni f use denot f actor u use denot updat howev implement ca algorithm much easier sinc effici execut spars task graph schedul requir sophist runtim system support asynchron commun protocol use rapid runtim system parallel spars lu use graph schedul key optim use remot memori accessrma commun data object two processor incur copyingbuff data transfer sinc low commun overhead critic spars code mix granular rma avail modern multiprocessor architectur craytd te meiko cs sinc rma directli write data remot address possibl content remot address still use task execut remot processor could incorrect thu gener comput permiss write remot address need obtain issu remot write howev rapid system handshak process avoid care design task commun protocol properti greatli reduc task synchron cost shown rapid spars code deliv speedup predict schedul craytd addit use rapid system greatli reduc amount implement work parallel spars lu let my rno cno d coordin processor perform scaleswapk perform updat perform updat dk j figur spmd code d asynchron code asynchron execut d code discuss previous d data map expos parallel maximum extent anoth issu timeeffici schedul may spaceeffici specif support concurr among multipl updat stage rapid ca code multipl buffer need keep pivot column block differ stage processor therefor given problem per processor space complex d code could high os space complex sequenti algorithm spars lu processor worst case may need space hold entir matrix rapid system also need extra memori space hold depend structur base observ goal d code reduc memori space requir exploit reason amount parallel solv larg problem instanc effici way section present asynchron d algorithm substanti overlap multistag updat memori requir much smaller d method figur show main control algorithm spmd code style figur show spmd code f actork execut processor column k mod p c recal algorithm use d blockcycl data map coordin processor own i mod also divid function updat in figur two part scaleswap scale delay row interchang submatrix kn kn shown figur updat d submatrix updat shown figur figur statement involv interprocessor commun mark seen comput flow d code still control pivot task f actork order execut f actork sequenti updat d task comput come from execut parallel among processor asynchron parallel come two level first singl stage task updat dk find local maximum element column m send subrow within column block k contain local maximum processor p k mod pr k mod pc processor own l kk collect local maxima find pivot row t broadcast subrow within column block k along processor column interchang subrow subrow necessari scale local entri column m updat local subcolumn column multicast pivot sequenc along processor row processor own l kk multicast l kk along processor row multicast part nonzero block l kn k own processor along processor row figur parallel execut f actork d asynchron code execut concurr processor addit differ stage updat d task updat dk also overlap idea computeahead schedul also incorpor ie f actork execut soon updat finish detail explan pivot scale swap given below line figur whole subrow commun processor report local maximum processor own l kk block let current global column number pivot conduct without synchron processor local swap subrow subrow contain select pivot element shorten wait time conduct updat littl commun volum howev line processor p k mod pr k mod pc must send origin subrow owner subrow swap select subrow processor well updat ing f actor task synchron take place line processor report local maximum p k mod pr k mod pc p k mod pr k mod pc broadcast subrow contain global maximum along processor column task scaleswap main role scale u k kn perform delay row interchang remain submatric kn kn examin degre parallel exploit algorithm determin number updat stage overlap use inform also determin extra buffer space need per processor execut algorithm correctli defin stage overlap degre receiv pivot sequenc p rno k mod pc processor part row pivot row column interchang nonzero part row row own processor cno k mod p c receiv l kk p rno k mod pc scale nonzero block u k kn own processor multicast scale result along processor column cno k mod p c receiv l kn k p rno k mod pc rno k mod p r receiv u k kn p k mod pr cno figur task scaleswapk d asynchron code updat dk use l ik u kj figur updat dk j d asynchron code updat task exist task updat dk updat dk execut concurrentlyg updat dk denot set updat dk task theorem asynchron d algorithm p processor p reachabl upper bound overlap degre p c among processor reachabl upper bound overlap degre within processor column minp r gamma proof use follow fact prove theorem ffl fact f actork execut processor column number k mod p c processor column synchron processor complet f actork processor still updat shown figur updat task belong processor must complet processor ffl fact scaleswapk execut processor row number k mod p r processor complet scaleswapk updat task belong processor must complet processor part first show updat d task overlap degre p c among processor trivial base fact p c imagin scenario processor column finish task f actork still work updat processor column could go ahead execut updat dk task processor column finish updat dk k task execut f actork finish updat dk task processor column could execut updat dk final processor column p c gamma could execut f actork moment processor column may still work updat thu overlap degre p c show contradict maximum overlap degre p c assum moment exist two updat stage execut concurr updat dk updat must complet without loss gener assum processor column execut f actork accord fact updat complet moment sinc block cyclic map use easi see processor column perform one f actorj task complet processor concurr stage updat dk k must satisfi contradict part first show overlap degre minp r gamma achiev within processor column conveni illustr consid scenario delay row interchang scaleswap take place local without commun within processor column therefor interprocessor synchron go within processor column except f actor task assum imagin moment processor column complet f actor p finish scaleswap start execut updat ds mod processor column execut updat p start scaleswap updat ds follow reason updat ds finish processor column could complet previou updat d task scaleswapsp r gamma start updat dsp r gamma p may still work updat ds thu overlap degre obvious reason stop processor column f actor case p pc gamma start updat ds pr gamma could still work updat ds gamma comput ahead schedul henc overlap degre p c need show upper bound overlap degre within processor column alreadi shown proof part overal overlap degre less p c overlap degre within processor column prove also less use similar proof part except use scaleswapk replac f actork use fact instead fact know degre overlap import determin amount memori space need accommod commun buffer processor support asynchron execu tion buffer space addit data space need distribut origin matrix four type commun need buffer pivot along processor column line figur includ commun pivot posit multicast pivot row call buffer purpos pbuffer multicast along processor row line figur commun data includ l kk local nonzero block l kn k pivot sequenc call buffer purpos cbuffer row interchang within processor column line figur call buffer ibuff multicast along processor column line figur data includ local nonzero block row panel call buffer rbuffer assum p r base experiment result set p r alway lead better perform thu overlap degre updat d task within processor row p c overlap degre within processor column p r gamma need p c separ cbuffer overlap among differ column rbuffer overlap among differ row estim size cbuffer rbuffer follow assum sparsiti ratio given matrix fillin maximum block size bsize cbuffer size maxfspac local nonzero block l knk similarli rbuffer size local nonzero block u ignor buffer size pbuffer ibuff small the size pbuffer bsize delta bsize size ibuff delta np c thu total buffer space need asynchron execut is c notic sequenti space complex practic set p c p therefor buffer space complex processor small larg matrix benchmark matric test buffer space less k word given spars matrix matrix data evenli distribut onto p processor total memori requir per processor p o consid n ae p n ae bsize lead us conclud d asynchron algorithm space scalabl experiment studi experi origin conduct craytd distribut memori machin san supercomput center node td includ dec alpha ev processor mbyte memori size intern cach kbyte per processor bla matrixmatrix multipl routin dgemm achiev mflop bla matrixvector multipl routin dgemv reach mflop number obtain assum data cach use cach readahead optim td matrix block size chosen commun network td d toru cray provid share memori access librari call shmem achiev mbytess bandwidth s commun overhead use shmem put primit use shmem put commun implement also conduct experi newli acquir crayt san diego supercomput center te node clock rate mhz kbyte intern cach kbyte second level cach mbyte main memori peak bandwidth node report mbytess peak round trip commun latenc s observ block size dgemm achiev mflop dgemv reach mflop use block size experi sinc block size larg avail parallel reduc section mainli report result te occas absolut perform concern also list result td see approach scale underlin architectur upgrad result obtain te unless explicitli state calcul mflop achiev parallel algorithm includ extra float point oper introduc overestim use follow formula achiev oper count obtain superlu parallel time algorithm td te oper count matrix report run superlu code sun workstat larg memori sinc superlu code cannot run larg matric singl td te node due memori constraint also compar sequenti code superlu make sure code use static symbol factor slow prevent parallel version deliv high megaflop impact static symbol factor sequenti perform studi introduct extra nonzero element static factor substanti affect time complex numer factor compar perform sequenti code superlu code perform tabl matric tabl time tabl includ symbol preprocess cost time superlu includ symbol factor superlu fli implement static symbol preprocess execut singl td te node also introduc two matric show well method work larger matric denser matric one two matric b truncat bcsstk current sequenti implement abl handl entir matrix due memori constraint one dens matrix approach superlu exec time ratio second mflop second mflop superlu sherman sherman jpwh goodwin dens tabl sequenti perform versu superlu impli data avail due insuffici memori though static symbol factor introduc lot extra comput shown tabl perform d lu partit consist competit highli optim superlu absolut singl node perform achiev approach td te consist rang gamma highest dgemm perform matric small medium size consid fact spars code usual suffer poor cach reus perform reason addit amount comput test matric tabl small rang million doubl precis float oper sinc characterist approach explor dens structur util bla kernel better perform expect larger denser matric verifi matrix b even larger matric vavasi cannot run one node shown later d code achiev mflop per node td processor notic megaflop perform per node spars choleksi report td node around mflop also good indic singlenod perform satisfactori present quantit analysi explain competit superlu assum speed bla kernel secondf lop speed bla kernel secondf lop total amount numer updat c f lop superlu c f lop appar simplic ignor comput scale part within column contribut littl total execut time henc have effici exampl preprocess time second singl node te largest matrix test vavasi symbol time spent dynam symbol factor superlu approach ae percentag numer updat perform dgemm let j ratio symbol factor time numer factor time superlu simplifi equat follow estim j test matric base result also measur ae approxim ae ratio number float point oper perform superlu test matric avail tabl averag valu c plug typic paramet equat have lop get te lop get estim close ratio obtain tabl discrep caus fact submatrix size supernod nonuniform lead differ cach perform submatric uniform size expect predict accur instanc dens case c exactli ratio calcul td te almost ratio list tabl analysi show use bla much possibl make competit superlu suppos machin dgemm outperform dgemv substanti ratio comput perform dgemm high enough could faster superlu matric last two entri tabl alreadi shown thi parallel perform d code subsect report set experi conduct examin overal parallel perform d code effect schedul supernod amalgam perform list mflop number d rapid code obtain variou number processor sever test matric tabl entri impli data avail due memori constraint below know megaflop dgemm te time larg td rapid code use upgrad machin speed time averag satisfactori machin perform rapid code increas number processor increas speedup compar pure sequenti code if applic reach td node te node node perform gain small except matric goodwin er b much larger problem rest reason small test matric enough amount comput parallel satur larg number processor elimin process proce toward end belief better scalabl perform obtain larger matric current avail memori node td te limit problem size solv current version rapid code matrix p p p p p p sherman sherman jpwh orsreg goodwin tabl absolut perform mflop d rapid code effect graph schedul compar perform d ca code d rapid code figur axi stand parallel time processor certain case computeahead code slightli faster rapid code number processor rapid code run faster processor involv bigger perform gap tend be reason small number processor suffici task make processor busi comput ahead schedul perform well rapid code suffer certain degre system overhead larger number processor schedul optim becom import sinc limit parallel exploit effect supernod amalgam examin effect supernod amalgam strategi use d rapid code let pt pt parallel time without supernod amalgam respect parallel time improv ratio te sever test matric list tabl similar result td appar supernod amalgam brought signific improv due increas supernod size impli increas task granular import obtain good parallel perform comparison rapid code d ca code proc sherman sherman comparison rapid code d ca code proc jpwh x goodwin figur impact differ schedul strategi d code approach matrix p p p p p p sherman sherman jpwh tabl parallel time improv obtain supernod amalgam d code perform mention befor d code exploit parallel maintain lower space complex much potenti solv larg problem show absolut perform obtain larg matric td tabl sinc matric cannot fit small number processor list result processor maximum absolut perform achiev node td gflop translat mflop per node node pernod perform mflop tabl show perform number te d code achiev gflop node node megaflop te time larg td consid dgemm megaflop te time larg td code perform use upgrad machin good notic d code cannot solv last six matric tabl matric solvabl use d rapid d code compar averag parallel time differ comput result figur d rapid code achiev matrix timesec mflop timesec mflop timesec mflop goodwin ex raefski tabl perform result d code larg matric td matrix p p p p p time mflop time mflop time mflop time mflop time mflop goodwin ex raefski inaccura af tabl perform result d asynchron algorithm te time second better perform use sophist graph schedul techniqu guid map column block order task result better overlap commun comput perform differ larger matric list left figur compar right figur partial explain reason analyz load balanc factor d rapid code d code figur load balanc factor defin work total p delta work max count work updat part major part comput d code better load balanc make impact lack effici task schedul verifi figur figur one see load balanc factor d code close rapid code eg lnsp perform rapid code much better d code load balanc factor d code significantli better rapid code eg jpwh orserg perform differ smaller synchron versu asynchron d code use global barrier d code elimin step simplifi implement cannot overlap comput among differ updat stage compar parallel time reduct asynchron code synchron code test matric tabl show asynchron design improv perform significantli especi larg number processor te demonstr import exploit parallel use asynchron execut experi comparison rapid code d code proc sherman sherman comparison rapid code d code proc jpwh x goodwin figur perform improv d rapid d code balanc comparison rapid vs d proc load balanc factor x sherman balanc comparison rapid vs d proc load balanc factor x jpwh figur comparison load balanc factor d rapid code d code data td conclud remark paper present approach parallel spars lu factor partial pivot distribut memori machin major contribut paper integr sever techniqu togeth static symbol factor schedul asynchron parallel d lu supernod partit techniqu effect identifi dens structur maxim use bla subroutin algorithm design use idea abl exploit data regular open irregular problem achiev gflop te node highest perform known challeng problem previou record gflop share memori machin matrix p p p p p p sherman sherman jpwh orsreg goodwin tabl perform improv d asynchron code d synchron code comparison result show d code better scalabl d code d map expos parallel care design buffer scheme d rapid code still outperform d code suffici memori sinc schedul execut techniqu d code simpl competit graph schedul recent conduct research develop space effici schedul algorithm retain good time effici still open problem develop advanc schedul techniqu better exploit parallel d spars lu factor partial pivot issu relat work need studi exampl altern parallel spars lu base schur complement static estim parallel exploit spars qr note static symbol factor could fail practic input matrix nearli dens row lead almost complet fillin whole matrix might possibl use differ matrix reorder avoid that fortun case matric test therefor approach applic wide rang problem use simpl order strategi interest futur studi order strategi minim overestim ratio consist deliv good perform variou class spars matric acknowledg work support nsf ria ccr nsf cda uc micro grant match sun nsf career ccr arpa dabtc rutger hpcd project would like thank kai shen effici implement static symbol factor algorithm xiaoy li jim demmel help discuss provid us test matric superlu code cleve ashcraft tim davi apostolo gerasouli esmond ng ed rothberg rob schreiber horst simon chunguang sun kathi yelick anonym refere valuabl comment r influenc relax supernod partit multifront method progress spars matrix method larg spars linear system vector supercomput user guid unsymmetricpattern multifront packag umfpack person commun unsymmetricpattern multifront method spars lu factor izat numer linear algebra parallel processor supernod approach spars partial pivot asynchron parallel supernod algorithm spars gaussian elimin extend set basic linear algebra subroutin multifront solut indefinit spars symmetr system equat algorithm obtain maximum transvers person commun structur represent schur complement spars matric comparison d d data map spars lu factor partial pivot effici runtim support irregular task comput mix granular spars lu factor partial pivot distribut memori machin space time effici execut parallel irregular comput parallel solut nonsymmetr spars linear system use h symbol factor spars gaussian elimin partial pivot parallel spars gaussian elimin partial pivot granular cluster direct acycl task graph scientif comput introduct parallel comput compil highli scalabl parallel algorithm spars matrix factor parallel unsymmetricpattern multifront method parallel algorithm spars linear system parallel spars gaussian elimin partial pivot d data map comput model task schedul parallel spars choleski factor distribut spars gaussian elimin orthogon factor exploit memori hierarchi sequenti parallel spars choleski factor improv load distribut parallel spars choleski fac toriz synchron commun te multiprocess cray te network adapt rout high perform toru decoupl synchron data transfer messag pass system parallel comput parallel spars orthogon factor distributedmemori multiprocessor pyrro static task schedul code gener messagepass multiprocessor tr ctr kai shen xiangmin jiao tao yang elimin forest guid spars lu factor proceed tenth annual acm symposium parallel algorithm architectur p june juli puerto vallarta mexico xiaoy s li jame w demmel make spars gaussian elimin scalabl static pivot proceed acmiee confer supercomput cdrom p novemb san jose ca patrick r amestoy iain s duff jeanyv lexcel xiaoy s li analysi comparison two gener spars solver distribut memori comput acm transact mathemat softwar tom v n p decemb xiaoy s li jame w demmel superlu_dist scalabl distributedmemori spars direct solver unsymmetr linear system acm transact mathemat softwar tom v n p june