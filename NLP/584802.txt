t topicori collabor crawl a major concern implement distribut web crawler choic strategi partit web among node system goal select strategi minim overlap activ individu node propos topicori approach web partit gener subject area crawler assign each examin design altern topicori distribut crawler includ creation web page classifi use context approach compar experiment hashbas partit crawler assign determin hash function comput url page content experiment evalu demonstr feasibl approach address issu commun overhead duplic content detect page qualiti assess b introduct crawler program gather resourc web web crawler wide use gather page index web search engin may also use gather inform web data mine question answer locat page specif content crawler oper maintain pend queue url crawler intend visit stage permiss make digit hard copi part work person classroom use grant without fee provid copi made distribut profit commerci advantag copi bear notic full citat first page copi otherwis republish post server redistribut list requir prior specif permiss andor fee cikm novemb mclean virginia usa crawl process url remov pend queue correspond page retriev url extract page url insert back pend queue futur process perform crawler often use asynchron io allow multipl page download simultan structur multithread program thread execut basic step crawl process concurr other execut crawler thread parallel multiprocessor distribut system improv perform implement distribut system potenti allow widescal geograph distribut crawl node minim competit local network bandwidth distribut crawler may structur group n local crawler singl local crawler run node nnode distribut sy tem local crawler may multithread pro gram maintain pend queue data structur share local execut thread local crawler may even parallel program execut cluster workstat central control global pend queue paper use term distribut crawler refer system data structur system control fulli distribut local crawler cannot oper entir independ collabor necessari avoid duplic eort sever respect local crawler must collabor reduc elimin number resourc visit one local crawler extrem case collabor local crawler might execut ident crawl visit resourc order overal distribut crawler must partit web local crawler focus subset web resourc target crawl collabor also need identifi deal duplic content often multipl url use refer site moneycnncom cnnfncom cnnfncnncom case larg set interrel page encount repeatedli crawl crawler avoid visit copi entireti exampl mani copi sun java jdk document found web result crawl use web search system user might present authorit copi resourc copi java document sun web site similarli webbas question answer depend independ assumpt duplic content in valid recogn duplic content local crawler might comput hash function content web page distribut crawler sucient inform must exchang local crawler allow duplic handl correctli collabor may necessari eectiv implement url order algorithm algorithm attempt order url pend queue desir page retriev first result order might reflect expect qualiti page need refresh local copi import page whose content known chang rapidli cho et al compar sever approach order url within pend queue introduc order base pagerank measur measur page expect qualiti compar breadthfirst order random order order base simpl backlink count report experi demonstr pagerank order like place import page earlier pend queue given page p pagerank rp may determin pagerank page t link it damp factor whose valu usual rang c outdegre total number page link given set page pagerank valu may comput assign page initi valu iter appli formula abov context crawler pagerank valu may estim url pend queue backlink inform provid page alreadi retriev depend backlink inform local crawler may need exchang inform order accur estim pagerank valu within local pend queue ideal local crawler would oper nearli indepen dentli despit need collabor amount data exchang local crawler mini mize preserv network bandwidth actual download target web resourc scalabl total amount data sent receiv local crawler small constant factor larger amount receiv data actual associ crawler partit synchron local crawler oper one local crawler may delay need commun anoth local crawler avoid avoid synchron especi import local crawler geograph distribut network node failur may disrupt commun final desir output local crawler usabl independ subcollect exampl crawl gener use distribut appli cation distribut search engin may possibl transfer data directli sourc node distribut crawler destin node distribut applic without ever central data output local crawler use independ subcollect exhibit intern cohes link graph contain dens connect compon allow pagerank qualiti metric accur estim paper present x topicori distribut crawl system x web partit content local crawler assign broad content categori target crawl page download pretrain classifi appli page determin uniqu content categori it local crawler oper independ unless encount boundari page page associ assign categori boundari page queu transfer associ local crawler remot node boundari page arriv remot crawler treat directli download local crawler next section paper provid review relat work section discuss approach collabor crawl view primari altern topicori approach altern web partit hash url page content section examin issu design x crawler section x evalu experiment includ comparison hashbas partit relat research comprehens studi web crawler design cho stanford phd thesi par ticular chapter thesi along subsequ paper repres studi we awar of attempt map explor full design space parallel distribut crawler share mani goal work address issu commun band width page qualiti divis work local crawler part work cho propos evalu hashbas approach similar discuss next section suggest occasion exchang link inform node improv accuraci local comput page qualiti estim cho thesi directli address issu duplic content detect distribut context apart cho work design parallel crawler implement form global control pend queue relat data structur maintain central actual download page take place worker node one exampl webfountain crawler base clusterofworkst architectur softwar consist three major compon ant duplic detector singl system control control manag system whole maintain global data structur monitor system perform ant respons actual retriev web resourc control assign site ant crawl retriev url site duplic detector recogn ident nearident content major featur webfountain crawler mainten up todat copi page content identifi page chang frequent reload need technic detail crawler use commerci web search servic natur regard trade secret publish detail structur implement one signific except merca tor crawler use altavista search servic replac older scooter crawler heydon najork describ detail problem associ creat commercialqu web crawler solut use address problem mercat written java mercat achiev scalabl extens larg care softwar engin research focus crawler close connect work describ present paper essenc focus crawler attempt order pend queue page concern specif topic place earlier queue goal creat special collect target topic chakrabarti et al introduc notion focus crawler experiment evalu implement concept use set fairli narrow topic cycl mutual fund hivaid order determin next url access implement use hypertext classifi determin probabl page relev topic distil identifi hub page point mani topicrel page mukherjea present wtm system gather analyz collect relat web page describ focus crawler form part system wtm crawler use vector space similar measur compar download page target vector repres desir topic url page higher similar score placer earlier pend queue mccallum et al diligenti et al recogn target page focus crawl necessarili link directli one anoth describ focus crawler learn identifi appar otop page reliabl lead ontop page menczer et al consid problem evalu compar eectiv strategi use focus crawler intellig crawler propos aggarw et al gener idea focus crawler encompass much prior research area work assum exist predic determin membership target group start arbitrari point crawler adapt learn linkag structur web lead target page consid combin featur includ page content pattern match url ratio link sibl page also satisfi predic hashbas collabor one approach implement distribut crawler partit web comput hash function url page content local crawler extract url retriev page represent first normal convert absolut url if necessari translat escap sequenc ascii valu hash function comput normal url assign one n local crawler assign local crawler locat remot node url transfer node url present correct node may ad node pend queue similarli local crawler comput hash function content page download pagecont hash function assign content one local crawler duplic detect postprocess take place hashbas scheme collabor three local crawler may involv process url encount local crawler encount node assign url hash function node content assign page content hash function although mani transfer local crawler cannot avoid local crawler maintain local tabl url page content previous seen prevent unnecessari transfer url hash function need take entir url account exampl url hash function base hostnam ensur url given server assign local crawler download allow load place server better control similarli pagecont hash function base normal content allow neardupl assign node use pagecont hash function may forc consider data transfer local crawler n uniform hash function map retriev page remot node download page map local node must export assign node nnode distribut crawler expect ratio export data total data download n n data export remot node data import node local crawler local crawler download data rate expect ratio import data total data download also n n limit number node increas amount data transfer local crawler twice amount data download web uniform hash function content page map local crawler independ locat page refer it number local crawler increas probabl referenc page assign node referenc page decreas proport properti may neg impact qualiti heurist use order pend queue and turn eectiv crawl page qualiti heurist use backlink inform may inform local avail accur estim order metric solut propos cho local crawler period exchang backlink inform cho demonstr rel small number exchang substanti improv local page qualiti estim approach increas complex commun local crawler implement approach local crawler must either select queri other backlink inform transfer backlink inform local crawler import paramet collabor crawler probabl p l link page link page assign node hashbas collabor crawler use uniform hash function p l n one goal topicori collabor reduc depend p l number node n topicori collabor previou section assum hash valu content specif page independ hash valu page refer it section outlin design topicori collabor crawler use text classifi assign page node given content web page classifi assign page one n distinct subject categori subject categori associ local crawler classifi assign page remot node local crawler transfer assign node process topicori collabor crawler may view set broadtop focus crawler partit web them breadth subject categori depend valu n n rang two subject categori might busi sport larger n subject categori narrow invest footbal hockey implement classifi use x discuss next section potenti advantag replac simpl pagecont hash function text classifi increas likelihood link page map node link page exampl link page classifi sport may like refer anoth sport page busi page mani potenti benefit topicori collabor crawl deriv assumpt topic local page tend refer page gener topic one immedi benefit topic local reduct bandwidth requir transfer page one local crawler anoth boundari page assign node retriev them transfer addit page qualiti metric depend backlink inform might accur estim page group assign categori sinc complet linkag inform may present addit advantag topicori collabora tion output local crawler meaning regard independ subcollect inform broker weight output dierent search system accord expect perform dierent queri type may abl take advantag topic focu subcollect topicori approach collabor crawl disadvantag url may independ encount download multipl crawler approach url hash alway retain node encount url encount two node node independ download page transfer common node categor properti may appear repres seriou limit topicori collabor crawl observ page encount multipl node page multipl categori refer it situat topic local assumpt tend minim benefit topicori collabor depend accuraci classifi actual intra inter categori linkag pattern found web experiment evalu issu use x crawler report section provid context evalu first describ evalu simpl classifi use x page categor text categor group text document set predefin categori topic often categor base probabl gener train set preclassifi document contain exampl topic document featur word phrase structur relationship extract preclassifi document use train classifi given unclassifi document train classifi extract featur document assign highestprob categori it text categor heavili studi subject varieti machin learn inform retriev techniqu appli problem includ rocchio feedback support vector machin expectationmaxim boost yang liu provid recent comparison five wide use method mani techniqu appli categor web page case extend exploit uniqu properti web data much work context focu crawler discuss section chakrabarti et al take advantag web link structur improv web page categor use iter relax label approach page classifi use categori assign neighbor page part featur set dumai chen take advantag larg collect hierarch organ web page provid organ yahoo looksmart develop hierarch categor techniqu base support vector machin select page categor techniqu use x sever attribut avail techniqu consid ere first categor base document content content neighbor document consid classifi page neighborhood would retriev crawler encount it retriev neighborhood page classifi like increas overlap crawler someth x seek avoid second train complet classifi must remain static cannot learn new data sinc categori assign page content everi local crawler must same third classifi ecient enough categor page rate download crawl major portion web requir minimum download rate sever mbp classifi abl match rate without requir resourc crawler itself final accuraci classifi percent page correctli cla sifi high possibl moreov minim number boundari page encount probabl link page classifi categori link page topic local also high possibl mani avail techniqu choos three studi basi simplic potenti ecient implement basic naiv bay classifi classifi base rocchio relev feedback probabilist classifi due lewi data open directori project odp use train test classifi odp selfregul organ maintain volunt expert categor url hierarch class directori similar directori provid yahoo other top level categori volunt examin content url determin categori level hierarchi contain list relev extern link list link subcategori snapshot mb data obtain odp purpos experi entir url directori tree collaps top categori two categori given special treatment region cate adult art busi comput game health recreat refer scienc world figur odp categori use topic classifi gori encompass page specif variou geograph area elimin entir sinc believ repres much separ topic altern organi zation page world categori written languag english also ignor initi classifi select phase final x classifi nonenglish page handl separ languag iden tifier toplevel categori includ world exclud region list figur ultim becam target categori use x classifi categor page preprocess first remov tag script numer inform remain text token term base whitespac punc tuation term convert lower case result term treat document featur requir classifi odp categori document randomli select train test html document contain word preprocess consid candid select perform classifi shown figur overal naiv bay classifi achiev accuraci lewi classifi rocchiotfidf classifi topicori crawl import statist topic local measur proport link page classifi categori link page test topic local web page categori randomli select train data five random link page or link page five retriev classifi result shown figur overal naiv bay classifi achiev topic local lewi classifi rocchiotfidf classifi test speed classifi web page averag length byte fed each naiv bay classifi achiev throughput pagessecond lewi classifi pagessecond rocchiotfidf classifi pagessecond although naiv bay lewi classifi perform compar term categor accuraci outperform rocchio tfidf classifi six time greater speed naiv bay classifi recommend use x crawler topic categor natur languag page written must determin languagespecif classifi may use languag identif partit page two languagespecif topic divis node natur languag languagespecif topic depend number local crawler requir mix web resourc target crawl purpos experi report paper group nonenglish page singl categori world follow odp organ number simpl statist languag identif techniqu shown good perform x identifi englishlanguag page proport common english word appear them train test identifi select random page cat egori includ world result languag identifi accuraci identifi english nonenglish page respect experiment evalu x crawler implement extens multitext web crawler origin develop part multitext project univers waterloo gather data webbas question answer crawler sinc use number extern group share design goal mercat mul titext crawler design highli modular config urabl use gener collect tb size ordinari pc crawler maintain download rate million page day includ pre postprocess page core crawler provid dataflow script languag coordin activ independ softwar compon perform actual oper crawl individu compon respons specif crawl task address resolut page download url extract url filter duplic content handl core crawler also provid transact support allow crawler action roll back restart system failur x creat ad two new compon multitext crawler one compon topic classifi compon data transfer util data transfer remot node queu local topic classifi period everi minut data transfer util poll remot node download queu data scp use perform actual transfer apart chang standard crawl script add call classifi data transfer util chang multitext crawler requir experiment evalu x pend queue maintain breadthfirst order one except breadthfirst order would place excess load singl host defin total crawler activ time period roughli one hour url associ host remov requeu anticip load drop accept level experiment evalu x use naiv bay topic classifi train odp data describ previou section preprocess retriev page first check determin unclassifi defin contain fewer term remov tag script preprocess arbitrarili assign unclassifi page categori adult unless unclassifi page languag identifi appli it page assign world categori languag identifi topic classifi appli assign page final categori experi local crawler associ categori figur local crawler map onto six node workstat cluster assign three local crawler five node singl adu art bu com gam hea hom kid new rec ref sci shp soc sptna bay lewi rocchiotfidf categori figur accuraci classifi odp data adu art bu com gam hea hom kid new rec ref sci shp soc sptna bay lewi rocchiotfidf categori topic local figur topic local classifi odp data categori world one node although multipl local crawler assign node test purpos local crawler act respect execut distinct node gener gb experiment crawl june due limit avail bandwidth gener internet crawler group limit download rate kbsecond local crawler limit kbsecond due diculti control download rate low speed actual download rate vari kbsecond kbsecond total million page download classifi figur plot distribut data across local crawler show volum download data retain local categor volum data import local crawler topic local achiev local crawler plot figur gener topic local achiev crawl slightli lower seen odp data figur rel topic local achiev topic roughli same main except local crawler adult topic local aect arbitrari decis assign unclassifi page associ categori comparison equival valu hashbas collabor p l hashbas collabor crawl url hash exchang map url uniqu local crawler prevent multipl crawler download url case topicori collabor crawl sinc page content exchang topicori collabor multipl crawler download url url referenc page one categori show log number page retriev exactli local crawler number page decreas rapidli increas page retriev local crawler gener home page major organ product wwwnytimescom wwwmicrosoftcomi order examin properti subcollect gener local crawler order page subcollect use pagerank algorithm permit direct comparison hashbas collabor redistribut page dierent subcollect use pagecont hash function comput pagerank order each final gather page singl collect comput global pagerank order figur compar pagerank order comput subcollect global pagerank order comput combin collect subcol lection figur report kendal rank correl pagerank order comput subcol port categori size figur distribut crawl data lection pagerank order page comput global collect higher correl coe cient indic accur local estim global pagerank order might expect use uniform hash function coecient hashbas subcollect nearli ident case coecient topicori subcollect greater co ecient hashbas subcollect conclus futur work paper propos concept topicori collabor crawl method implement generalpurpos distribut web crawler demonstr feasibl implement experiment evalu ation contrast url hostbas hash approach evalu cho approach allow duplic page content recogn gener subcollect page relat topic rather locat x could extend improv sever way current implement pend queue maintain breadthfirst order instead focus crawl techniqu might use order pend queue place url like refer ontop resourc closer front queue techniqu could substanti increas proport ontop page retriev decreas number transfer local crawler x transfer content boundari page node retriev node classifi assign it design decis taken consequ desir map content page uniqu local crawler order facilit duplic detect postprocess altern design would transfer url link appear boundari page content boundari page would transfer disadvantag approach content url uniqu associ local crawler everi local crawler encount url boundari page retain copi content advantag approach commun overhead local crawler reduc commun local crawler transfer url extract boundari page retent boundari page content multipl node may advantag own topic local impli page like relat topic local crawler encount high backlink count associ boundari page encount mani crawler impli high qualiti current work primari interest web page categor itself text categor method could explor use x techniqu featur select might use improv ecienc accuraci problem associ increment crawl dynam chang content consid examin futur work evalu base rel small crawl gb thorough evalu base multiterabyt crawl might reveal issu obviou current experi ment final x test abil scale larger number node correspondingli larger number categori r intellig crawl world wide web arbitrari predic author languag tree zip anatomi largescal hypertextu web search engin crawl toward etern enhanc hypertext categor use hyperlink martin van den burg crawl web discoveri mainten largescal web data parallel crawl find replic web collect exploit redund question answer autonom learn construct knowledg base world wide web topic local web focus crawl use context graph hierarch classif web content adapt model optim perform increment web crawler intellig fusion multipl method inform server select mercat scalabl perform limit java core librari probabilist analysi rocchio algorithm tfidf text classif statist learn model text classif support vector machin authorit sourc hyperlink environ scale question answer web evalu phrasal cluster represent text categor task build domainspecif search engin machin learn techniqu evalu topicdriven web crawler machin learn wtm system collect analyz topicspecif web inform text classif label unlabel document use em boost rocchio appli text filter fast categoris larg document collect tr evalu phrasal cluster represent text categor task enhanc hypertext categor use hyperlink syntact cluster web boost rocchio appli text filter method inform server select anatomi largescal hypertextu web search engin effici crawl url order perform limit java core librari reexamin text categor method focus crawl authorit sourc hyperlink environ find replic web collect hierarch classif web content topic local web myampersandldquoauthoritymyampersandrdquo mean qualiti predict expert qualiti rate web document text classif label unlabel document use em learn construct knowledg base world wide web intellig crawl world wide web arbitrari predic adapt model optim perform increment web crawler scale question answer web statist learn learn model text classif support vector machin evalu topicdriven web crawler exploit redund question answer machin learn mercat probabilist analysi rocchio algorithm tfidf text categor focus crawl use context graph crawl web ctr antonio badia tulay muezzinoglu olfa nasraoui focus crawl experi real world project proceed th intern confer world wide web may edinburgh scotland jo exposto joaquim macedo antnio pina albano alv jo rufino geograph partit distribut web crawl proceed workshop geograph inform retriev novemb bremen germani weizheng gao hyun chul lee yingbo miao geograph focus collabor crawl proceed th intern confer world wide web may edinburgh scotland