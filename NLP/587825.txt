t fulli asynchron multifront solver use distribut dynam schedul a paper analyz main featur discuss tune algorithm direct solut spars linear system distribut memori comput develop context long term european research project algorithm use multifront approach especi design cover larg class problem problem symmetr posit definit gener symmetr unsymmetr matric possibl rank defici provid user sever format algorithm achiev high perform exploit parallel come sparsiti problem avail dens matric algorithm use dynam distribut task schedul techniqu accommod numer pivot allow migrat comput task lightli load processor larg comput task divid subtask enhanc parallel asynchron commun use throughout solut process effici overlap commun computationw illustr design choic experiment result obtain sgi origin ibm sp test matric provid industri partner parasol project b introduct consid direct solut larg spars linear system distribut memori comput system form n theta n symmetr posit definit gener symmetr unsymmetr spars matrix possibl rank defici b righthand side vector x solut vector comput work present articl perform work packag within parasol project parasol esprit iv long term research project no an integr environ parallel spars matrix solver main goal project start januari finish june build test portabl librari solv larg spars system equat distribut memori system final librari public domain contain routin direct iter solut symmetr unsymmetr system context parasol produc multifront massiv parallel solver refer mump remaind paper sever aspect algorithm use mump combin give approach uniqu among spars direct solver includ ffl classic partial numer pivot numer factor requir use dynam data structur ffl abil automat adapt comput load variat numer phase ffl high perform exploit independ comput due sparsiti avail dens matric ffl capabl solv wide rang problem includ symmetr unsymmetr rankdefici system use either lu ldl factor address factor design fulli asynchron algorithm base multifront approach distribut dynam schedul task current version packag provid larg rang option includ possibl input matrix assembl format either singl processor distribut processor addit matrix input element format current one processor mump also determin rank nullspac basi rankdefici matric return schur complement matrix contain classic pre postprocess facil exampl matrix scale iter refin error analysi among work distribut memori spars direct solver awar know capabl mump solver difficulti handl dynam data structur effici distribut memori approach perform numer pivot factor phase instead base static map task data allow task migrat numer factor numer pivot clearli avoid symmetr posit definit matric unsymmetr matric duff koster design algorithm permut larg entri onto diagon shown significantli reduc numer pivot demmel li shown that one preprocess matrix use code duff koster static pivot with possibl modifi diagon valu follow iter refin normal provid reason accur solut observ preprocess combin appropri scale input matrix issu numer stabil approach rest paper organ follow first introduc main term use multifront approach section throughout paper studi perform obtain set test problem describ section discuss section main parallel featur approach section give initi perform figur show influenc order variabl perform mump section describ work accept input matric element form section briefli describ main properti algorithm use distribut assembl matric section comment memori scalabl issu section describ analys distribut dynam schedul strategi analys section show modifi assembl tree introduc parallel present summari result section result present paper obtain processor ibm sp locat gmd bonn germani node comput mhertz processor mbyte physic memori mbyte virtual memori sgi cray origin parallab univers bergen norway also use run largest test problem parallab comput consist node share gbyte physic distribut memori node two r mip risc bit processor share mbyte local memori processor run frequenc mhertz peak perform littl mflop per second experi report paper use version mump softwar written fortran requir mpi messag pass make use bla lapack blac scalapack subroutin ibm sp current use nonoptim portabl local instal scalapack ibm optim librari pessl v avail multifront method intent describ detail multifront method rather defin term use later paper refer reader earlier public detail descript exampl multifront method elimin oper take place within dens submatric call frontal matric frontal matrix partit shown figur matrix pivot chosen within block f onli schur complement matrix comput use updat later row column overal matrix call updat matrix contribut block fulli sum row partli sum row fulli sum column partli sum column figur partit frontal matrix overal factor spars matrix use multifront scheme describ assembl tree node correspond comput schur complement describ edg repres transfer contribut block son node father node tree father node assembl or sum contribut block son node entri origin matrix origin matrix given assembl format complet row column input matrix assembl onc and facilit thi input matrix order accord pivot order store collect arrowhead is permut matrix entri in exampl column row column arrowhead list associ variabl g symmetr case entri lower triangular part matrix store say store matrix arrowhead form arrowhead unassembl matric complet element matric assembl frontal matric input matrix need preprocess implement assembl tree construct symmetr pattern matrix given sparsiti order symmetr pattern mean pattern matrix aa summat symbol note allow matrix unsymmetr numer pivot possibl variabl cannot elimin frontal matrix fulli sum row column correspond variabl ad contribut block sent father node assembl fulli sum row column frontal matrix father node mean correspond elimin oper delay repeat elimin step later frontal matric introduc stabl pivot delay fulli sum part delay elimin step correspond posteriori modif origin assembl tree structur gener introduc addit numer fillin factor import aspect assembl tree oper pair node neither ancestor independ give possibl obtain parallel tree socal tree parallel exampl work commenc parallel leaf node tree fortun near root node tree tree parallel poor frontal matric usual much larger techniqu exploit parallel dens factor use for exampl block use higher level bla call node parallel discuss aspect parallel multifront method later section paper work base experi design implement multifront scheme share virtual share memori comput for exampl initi prototyp distribut memori multifront version describ design result distribut memori multifront algorithm rest paper test problem throughout paper use set test problem illustr perform algorithm describ set section tabl list unassembl assembl test problem respect except one come industri partner parasol project remain matrix bbmat forthcom rutherfordbo spars matrix collect symmetr matric show number entri lower triangular part matrix typic parasol test case follow major applic area comput fluid dynam cfd structur mechan model compound devic model ship mobil offshor platform industri process complex nonnewtonian liquid model car bodi engin compon test problem provid assembl format element format suffix rsa rse use differenti them element format origin matrix repres sum element matric nonzero entri row column correspond variabl ith element element matric may overlap number entri matrix element format usual larger matrix assembl compar matric det norsk verita norway tabl typic twice number entri unassembl element format real symmetr element rse matrix name order no element no entri origin trse det norsk verita ship rse det norsk verita ship rse det norsk verita shipsecrs det norsk verita shipsecrs det norsk verita shipsecrs det norsk verita threadrs det norsk verita xrse det norsk verita tabl unassembl symmetr test matric parasol partner in element format tabl present statist factor variou test problem use mump tabl show number entri factor number floatingpoint oper flop elimin unsymmetr problem show estim number assum pivot actual number numer pivot use statist clearli depend order use two class order consid paper first approxim minimum degre order refer amd see second class base hybrid nest dissect minimum degre techniqu refer nd hybrid order gener use onmeti combin graph partit tool scotch variant amd halo amd see matric avail assembl unassembl format use nest dissect base order provid det norsk verita denot mfr note that paper intent compar packag use obtain order discuss influenc type order perform mump in section amd order algorithm tightli integr within mump code order pass mump extern comput order tight integr observ tabl analysi time smaller use amd real unsymmetr assembl rua matrix name order no entri origin mixingtank polyflow sa bbmat rutherfordbo cfd real symmetr assembl rsa matrix name order no entri origin oilpan inpro btuer inpro crankseg macnealschwendl bmwst macnealschwendl ship rsa det norsk verita ship rsa det norsk verita shipsecrsa det norsk verita shipsecrsa det norsk verita shipsecrsa det norsk verita threadrsa det norsk verita xrsa det norsk verita tabl assembl test matric parasol partner except matrix bbmat amd order entri flop time matrix factor theta estim actual estim actual second mixingtank invextrus bbmat nd order entri flop time matrix factor theta estim actual estim actual second mixingtank bbmat tabl statist unsymmetr test problem ibm sp userdefin precomput order in paper nd mfr order addit cost comput extern order includ tabl amd order nd order entri flop time entri flop matrix factor analysi factor btuer bmwst tabl statist symmetr test problem ibm sp entri flop matrix factor ship shipsec shipsec shipsec thread tabl statist symmetr test problem avail assembl rsa unassembl rse format mfr order parallel implement issu paper assum onetoon map process processor distribut memori environ process thu implicitli refer uniqu processor and say exampl task alloc process mean task also map onto correspond processor share memori environ exploit parallel aris sparsiti tree parallel dens factor kernel node parallel avoid limit due central schedul host process charg schedul work process chosen distribut schedul strategi implement pool work task distribut among process particip numer factor host process still use perform analysi phase and identifi pool work task distribut righthand side vector collect solut implement allow host process particip comput factor solut phase allow user run code singl processor avoid one processor idl factor solut phase code solv system three main step analysi host perform approxim minimum degre order base symmetr matrix pattern carri symbol factor order also provid user host also comput map node assembl tree processor map keep commun cost factor solut minimum balanc memori comput requir process comput cost approxim number floatingpoint oper assum pivot perform storag cost number entri factor comput map host send symbol inform process use inform process estim work space requir part factor solut estim work space larg enough handl comput task assign process analysi time plu possibl task may receiv dynam factor assum excess amount unexpect fillin occur due numer pivot factor origin matrix first preprocess for exampl convert arrowhead format matrix assembl distribut process particip numer factor process alloc array contribut block factor numer factor frontal matrix perform process determin analysi phase potenti one process determin dynam factor must kept solut phase solut righthand side vector b broadcast host process comput solut vector x use distribut factor comput factor phase solut vector assembl host sourc parallel consid condens assembl tree figur leav repres subtre assembl tree subtre type type type type type figur distribut comput multifront assembl tree four processor p p p p consid tree parallel transfer contribut block node assembl tree father node requir local data movement node assign process commun requir node assign differ process reduc amount commun factor solut phase map comput analysi phase assign subtre assembl tree singl process gener map algorithm choos leaf subtre process and map subtre care onto process achiev good overal load balanc comput bottom tree describ detail howev exploit tree parallel speedup disappoint obvious depend problem typic maximum speedup illustr tabl poor perform caus fact tree parallel decreas go toward root tree moreov observ see exampl often comput perform top three level assembl tree thu necessari obtain parallel within larg node near root tree addit parallel base parallel block version algorithm use factor frontal matric node assembl tree treat one process refer node type parallel assembl tree refer type parallel parallel obtain onedimension d block partit row frontal matrix node larg contribut block see figur node refer node type correspond parallel type parallel final frontal matrix root node larg enough partit twodimension d block cyclic way parallel root node refer node type correspond parallel type parallel type parallel analysi phase node determin type number row contribut block suffici larg node type one process call master hold fulli sum row perform pivot factor block process call slave perform updat partli sum row see figur slave determin dynam factor process may select abl assembl origin matrix entri quickli frontal matrix type node duplic correspond origin matrix entri store arrowhead element matric onto process factor way master slave process type node immedi access entri need assembl local part frontal matrix duplic origin data enabl effici dynam schedul comput task requir extra storag studi detail section note type node origin matrix entri need present process handl node execut time master type node first receiv symbol inform describ structur contribut block son node tree inform sent master process handl son base inform master determin exact structur frontal matrix decid slave process particip factor node send inform process handl son enabl send entri contribut block directli appropri process involv type node assembl node subsequ perform parallel master slave process perform elimin oper frontal matrix parallel macropipelin base block factor fulli sum row use overlap commun comput effici algorithm thu depend block size use factor fulli sum row number row alloc slave process detail differ implement symmetr unsymmetr matric describ type parallel root node must factor dens matrix use standard code thi scalabl reason use block cyclic distribut root node use scalapack vendor equival implement routin pdgetrf gener matric routin pdpotrf symmetr posit definit matric actual factor current maximum one root node chosen analysi process parallel node chosen largest root provid size larger comput depend paramet otherwis factor one processor one process also call master hold indic describ structur root frontal matrix call root node determin analysi phase estim root node factor structur frontal matrix estim root node static map onto grid process map fulli determin process entri estim root node assign henc assembl origin matrix entri contribut block process hold inform easili comput exactli process must send data to factor phase origin matrix entri part contribut block son correspond estim root assembl soon avail master root node collect index inform delay variabl due numer pivot son build final structur root frontal matrix symbol inform broadcast process particip factor contribut correspond delay variabl sent son appropri process grid assembl or contribut directli assembl local destin process note that requir scalapack local copi root node requir sinc lead dimens chang delay pivot parallel triangular solut solut phase also perform parallel use asynchron commun forward elimin back substitut case forward elimin tree process leav root similar factor back substitut requir differ algorithm process tree root leav pool readytobeactiv task use chang distribut factor gener factor phase henc type parallel also use solut phase root node use scalapack routin pdgetr gener matric routin pdpotr symmetr posit definit matric basic perform influenc order earlier studi for exampl know order may serious impact uniprocessor time parallel behaviour method illustr thi report tabl perform obtain use type parallel result show use type parallel produc good speedup result also show see column speedup usual get better parallel nest dissect base order minimum degre base order thu gain use nest dissect reduct number floatingpoint oper see tabl better balanc assembl tree discuss perform obtain mump matric assembl format use refer paper perform obtain matric provid element format discuss section tabl show perform mump use nest dissect minimum degre order ibm sp sgi origin respect note speedup difficult comput ibm sp memori page often occur small number processor henc better perform nest dissect order small number processor ibm sp due part reduct memori requir processor sinc less entri factor get better idea true algorithm speedup without memori page effect give tabl uniprocessor cpu time one processor instead elaps time matrix time speedup amd nd amd nd oilpan bmwst bbmat btuer tabl influenc order time in second speedup factor phase use type parallel processor ibm sp memori larg enough run one processor estim megaflop rate use comput uniprocessor cpu time thi estim also use necessari comput speedup tabl small number processor still memori page effect may significantli increas elaps time howev speedup elaps time one processor not given consider matrix order number processor oilpan amd btuer amd crankseg amd bmwst amd nd mixingtank amd nd bbmat amd nd tabl impact order time in second factor ibm estim cpu time one processor mean enough memori tabl also show elaps time solut phase observ speedup phase quit good remaind paper use nest dissect base order unless state otherwis factor phase matrix order number processor bmwst amd nd nd solut phase matrix order number processor crankseg amd nd bmwst amd nd nd nd tabl impact order time in second factor solv phase sgi origin element input matrix format section discuss main algorithm chang handl effici problem provid element format assum origin matrix repres sum element matric nonzero entri row column correspond variabl ith element usual held dens matrix matrix symmetr lower triangular part store multifront approach element matric need assembl one frontal matrix elimin process due fact frontal matrix structur contain definit variabl adjac fulli sum variabl front consequ element matric need split assembl process note that classic fanin fanout approach properti hold sinc posit element matric assembl restrict fulli sum row column main modif make algorithm assembl matric accommod unassembl matric lie analysi distribut matrix assembl process describ detail below analysi phase exploit element format matrix detect supervari defin supervari set variabl list adjac element illustr figur matrix compos two overlap element three supervari note definit supervari differ usual definit see exampl supervari use success similar context compress graph associ assembl matric structur engin prior multipl minimum degre order assembl matric howev observ use supervari combin approxim minimum degre algorithm effici graph size matrix supervari detect trse ship rse shipsecrs shipsecrs shipsecrs threadrs xrse tabl impact supervari detect length adjac list given order phase tabl show impact use supervari size graph process order phase amd order graph size length adjac list variablessupervari given input order phase without supervari detect initi graph variabl initi matrix graph supervari sum two overlap element figur supervari detect matric element format graph size twice number offdiagon entri correspond assembl matrix work space requir analysi phase use amd order domin space requir order phase graph size plu overhead small multipl order matrix sinc order perform singl processor space requir comput order memori intens part analysi phase supervari detect complet uncompress graph need built sinc order phase oper directli compress graph tabl show that larg graph compress reduc memori requir analysi phase dramat tabl show impact use supervari time complet analysi phase includ graph compress order see reduct time due reduc time order significantli less time also need build much smaller adjac graph supervari time analysi matrix supervari detect trse ship rse shipsecrs shipsecrs shipsecrs threadrs xrse tabl impact supervari detect time in second analysi phase sgi origin time spent amd order parenthes overal time spent assembl process matric element format differ overal time spent assembl process equival assembl matrix obvious matric element format often significantli data assembl usual twice number entri matrix assembl format howev assembl process matric element format perform effici assembl process assembl matric first potenti assembl larger regular structur a full matrix second input data assembl near leaf node assembl tree two consequ assembl perform distribut way assembl origin element matric done type node henc less duplic origin matrix data necessari detail analysi duplic issu link matric element format address section experi not shown here observ that despit differ assembl process perform mump assembl unassembl problem similar provid order use reason extra amount assembl origin data unassembl problem rel small compar total number flop experiment result tabl obtain sgi origin show good scalabl code factor solut phase set unassembl matric matrix number processor ship rse shipsecrs shipsecrs shipsecrs threadrs xrse tabl time in second factor unassembl matric sgi origin mfr order use matrix number processor trse ship rse shipsecrs shipsecrs shipsecrs threadrs xrse tabl time in second solut phase unassembl matric sgi origin mfr order use distribut assembl matrix distribut input matrix avail processor main preprocess step numer factor phase step input matrix organ arrowhead format distribut accord map provid analysi phase symmetr case first arrowhead frontal matrix also sort enabl effici assembl assembl matrix initi held central host observ time distribut real entri origin matrix sometim compar time perform actual factor exampl matrix oilpan time distribut input matrix processor ibm sp averag second wherea time factor matrix second use amd order see tabl clearli larger problem arithmet requir actual factor time factor domin time redistribut distribut input matrix format expect reduc time redistribut phase parallel reformat sort task use asynchron alltoal instead onetoal commun furthermor expect solv larger problem sinc store complet matrix one processor limit size problem solv distribut memori comput thu improv memori time scalabl approach allow input matrix distribut base static map task process comput analyi phase one priori distribut input data remap requir begin factor distribut refer mump map limit commun duplic origin matrix correspond type node further studi section show influenc initi matrix distribut time redistribut compar figur three way provid input matrix central map input matrix held one process the host mump map input matrix distribut process accord static map comput analysi phase random map input matrix uniformli distribut process random manner correl map comput analysi phase figur clearli show benefit use asynchron alltoal commun requir mump random map compar use onetoal commun for central map even interest observ distribut input matrix accord mump map significantli reduc time redistribut attribut good overlap commun comput mainli data reformat sort redistribut algorithm number processor distribut time second central matrix use mump map random map figur impact initi distribut matrix oilpan time redistribut ibm sp memori scalabl issu section studi memori requir memori scalabl algorithm figur illustr mump balanc memori load processor figur show two matric maximum memori requir processor averag processor function number processor observ that vari number processor valu quit similar number processor size total space mbyte maximum averag number processorss total space mbyte maximum averag figur total memori requir per processor maximum averag factor nd order tabl show averag size per processor main compon work space use factor matrix bmw compon are ffl factor space reserv factor processor know analysi phase type node particip therefor reserv enough space abl particip type node ffl stack area space use stack contribut block factor ffl initi matrix space requir store initi matrix arrowhead format ffl commun buffer space alloc send receiv buffer ffl other size remain workspac alloc per processor ffl total total memori requir per processor line ideal tabl obtain divid memori requir one processor number processor compar actual ideal number get idea mump scale term memori compon number processor factor ideal stack area initi matrix ideal commun buffer total ideal tabl analysi memori use factor matrix bmw nd order size mbyte per processor see that even total memori sum local workspac increas averag memori requir per processor significantli decreas processor also see size factor stack area much larger ideal part differ due parallel unavoid anoth part howev due overestim space requir main reason map type node processor known analysi processor potenti particip elimin type node therefor processor alloc enough space abl particip type node work space actual use smaller and larg number processor could reduc estim factor stack area exampl success factor matrix bmw processor stack area smaller report tabl averag work space use commun buffer also significantli decreas processor mainli due type node parallel contribut block split among processor minimum granular reach therefor increas number processor decreas until reach minimum granular size contribut block sent processor note larger problem averag size per processor commun buffer continu decreas larger number processor see expect line scale sinc correspond data array size on need alloc process see space significantli affect differ total ideal especi larger number processor howev rel influenc fix size area smaller larg matric simul therefor affect asymptot scalabl algorithm imperfect scalabl initi matrix storag come duplic origin matrix data link type node assembl tree studi detail remaind section want stress howev user point view number report context relat total memori use mump packag usual domin larg problem size stack area altern duplic data relat type node would alloc origin data associ frontal matrix master process respons matrix number processor oilpan type node total entri bmwst total entri total entri shipsecrsa type node total entri shipsecrs type node total entri threadrsa type node total entri threadrs type node total entri tabl amount duplic due type node total entri sum number origin matrix entri processor theta number node also given type node assembl process master process would charg redistribut origin data slave process strategi introduc extra commun cost assembl type node thu chosen approach base duplic master process respons type node flexibl choos collabor process dynam sinc involv data migrat origin matrix howev extra cost strategi that base decis analysi node type partial duplic origin matrix must perform order keep processor busi need suffici node parallel near root assembl tree mump use heurist increas number type node number processor use influenc number processor amount duplic shown tabl repres subset test problem show total number type node sum process number origin matrix entri duplic one processor type node use data duplic figur show four matric number origin matrix entri duplic processor rel total number entri origin matrix sinc origin data unassembl matric gener assembl earlier assembl tree data matrix assembl format number duplic often rel much smaller unassembl matric assembl matric matrix threadrs in element format extrem exampl sinc even processor type node parallel requir duplic see tabl conclud section want point code scale well term memori usag virtual share memori comput total memori sum local workspac processor requir mump sometim excess therefor current investig reduc current overestim local stack area number processorspercentag bmw_ threadrsa figur percentag entri origin matrix duplic processor due type node reduc total memori requir possibl solut might limit dynam schedul type node and correspond data duplic subset processor dynam schedul strategi avoid drawback central schedul distribut memori comput implement distribut dynam schedul strategi remind reader type node static map process analysi time type task repres larg part comput parallel method involv dynam schedul strategi abl choos dynam process collabor process type node design twophas assembl process let inod node type let pmaster process inod initi map first phase master process son inod map send symbol data integ list pmaster structur frontal matrix determin pmaster decid partit frontal matrix choos slave process phase pmaster collect inform concern load processor help decis process slave process inform new task alloc them pmaster send descript distribut frontal matrix collabor process son inod send contribut block real valu piec directli correct process involv comput inod assembl process thu fulli parallel maximum size messag sent process reduc see section pool task privat process use implement dynam schedul task readi activ given process store pool task local process process execut follow algorithm algorithm node process local pool empti block receiv messag process messag elseif messag avail receiv process messag els extract work pool process endif note algorithm give prioriti messag recept main reason choic first messag receiv might sourc addit work parallel second send process might block send buffer full see actual implement use routin mpi iprob check whether messag avail implement two schedul strategi first strategi refer cyclic schedul master type node take account load processor perform simpl cyclic map task processor second strategi refer dynam flopsbas schedul master process use inform load processor alloc type task least load processor load processor defin amount work flop associ activ readytobeactiv task process charg maintain local inform associ current load simpl remot memori access procedur use exampl onesid commun routin mpi get includ mpi process access load processor necessari howev mpi avail target comput overcom thi design modul base symmetr commun tool mpi asynchron send receiv process charg updat broadcast local load control frequenc broadcast updat load broadcast significantli differ last load broadcast initi static map balanc work well expect dynam flopsbas schedul improv perform respect cyclic schedul tabl show signific perform gain obtain use dynam flopsbas schedul processor gain less signific test problem small keep processor busi thu lessen benefit good dynam schedul algorithm also expect featur improv behaviour parallel algorithm multius distribut memori comput anoth possibl use dynam schedul improv memori usag seen section size stack area overestim dynam schedul base matrix number processor schedul cyclic flopsbas cyclic flopsbas tabl comparison cyclic flopsbas schedul time in second factor ibm sp nd order matrix number processor schedul ship rse cyclic flopsbas shipsecrs cyclic flopsbas shipsecrs cyclic flopsbas tabl comparison cyclic flopsbas schedul time in second factor sgi origin mfr order memori load instead comput load could use address issu type task map least load processor in term memori use stack area memori estim size stack area base static map task split node assembl tree process parallel type node symmetr unsymmetr case factor pivot row perform singl processor processor help updat row contribut block use decomposit as present section elimin fulli sum row repres potenti bottleneck scalabl especi frontal matric larg fulli sum block near root tree type parallel limit overcom problem subdivid node larg fulli sum block illustr figur assembl tree pivot block contribut block assembl tree splittingnfrontnpiv npiv father son son figur tree subdivis frontal matrix larg pivot block figur consid initi node size nfront npiv pivot replac node son node size nfront npiv son pivot father node size son npiv father npiv gammanpiv son pivot note split node increas number oper factor add assembl oper nevertheless expect benefit split increas parallel experi simpl algorithm postprocess tree symbol factor algorithm consid node near root tree split larg node far root suffici tree parallel alreadi exploit would lead addit assembl commun cost node consid split distanc root is number edg root node let inod node tree dinod distanc inod root node inod dinod dmax appli follow algorithm algorithm split node larg enough comput number flop perform master inod comput number flop perform slave assum nproc gamma slave particip w master w slave split inod node son father npiv son appli algorithm recurs node son father endif endif algorithm appli node nfront npiv larg enough want make sure son split node type the size contribut block son nfront npiv son node split amount work master w master larg rel amount work slave w slave reduc amount split away root add step algorithm rel factor w slave factor depend machin depend paramet increas distanc node root paramet p allow us control gener amount split final algorithm recurs may divid initi node two new node effect split illustr tabl symmetr matrix crankseg unsymmetr matrix invextrus ncut correspond number type node cut valu use flag indic split flopsbas dynam schedul use run section best time obtain given number processor indic bold font see signific perform improv of reduct time obtain use node split best time gener obtain rel larg valu p split occur smaller valu p correspond time chang much p number processor time time time ncut time ncut p number processor time time time time tabl time in second factor number node cut differ valu paramet p ibm sp nest dissect order flopsbas dynam schedul use summari tabl show result obtain mump use dynam schedul node split default valu paramet control effici packag use therefor time alway correspond fastest possibl execut time comparison result present tabl summar well benefit come work present section matrix number processor oilpan btuer bmwst mixingtank bbmat tabl time in second factor use mump default option ibm sp nd order use estim cpu time mean swap enough memori matrix number processor bmwst ship rse shipsecrs shipsecrs shipsecrs threadrs xrse tabl time in second factor use mump default option sgi origin nd mfr order use largest problem solv date symmetr matrix order million entri number entri factor theta number oper factor theta one processor sgi origin factor phase requir hour two nonded processor hour requir total amount memori estim reserv mump could solv processor issu address improv scalabl global address memori comput analysi perform pure distribut memori comput larger number processor possibl solut mention paper limit dynam schedul andor memori base dynam schedul develop futur acknowledg grate jennif scott john reid comment earli version paper r approxim minimum degre order algorithm linear algebra calcul virtual share memori comput vector multiprocessor multifront code memori manag issu spars multifront method multiprocessor multifront parallel distribut symmetr unsymmetr solver fanboth famili columnbas distribut choleski factoris algorithm compress graph minimum degre algorithm scalapack user guid parallel solut method larg spars system equat supernod approach spars partial pivot make spars gaussian elimin scalabl static pivot work note user guid blac v algorithm algorithm rutherfordbo spars matrix collect direct method spars matric design use algorithm permut larg entri diagon spars matric algorithm permut larg entri diagon spars matrix multifront solut indefinit spars symmetr linear system developp dune approch multifrontal pour machin a memoir distribue et reseau heterogen de station de travail spars choleski factor local memori multiprocessor highli scalabl parallel algorithm spars matrix factor parallel algorithm spars linear system improv runtim qualiti nest dissect order scotch user guid hybrid nest dissect halo approxim minimum degre effici spars matrix order tr ctr kai shen parallel spars lu factor differ messag pass platform journal parallel distribut comput v n p novemb omer meshar dror ironi sivan toledo outofcor spars symmetricindefinit factor method acm transact mathemat softwar tom v n p septemb patrick r amestoy iain s duff jeanyv lexcel xiaoy s li impact implement mpi pointtopoint commun perform two gener spars solver parallel comput v n p juli kai shen parallel spars lu factor secondclass messag pass platform proceed th annual intern confer supercomput june cambridg massachusett hong zhang barri smith michael sternberg peter zapol sip shiftandinvert parallel spectral transform acm transact mathemat softwar tom v n pe june mark baertschi xiaoy li solut threebodi problem quantum mechan use spars linear algebra parallel comput proceed acmiee confer supercomput cdrom p novemb denver colorado iain s duff jennif a scott parallel direct solver larg spars highli unsymmetr linear system acm transact mathemat softwar tom v n p june adapt grid refin model two confin interact atom appli numer mathemat v n p februari abdou guermouch jeanyv lexcel gil utard impact reorder memori multifront solver parallel comput v n p septemb vladimir rotkin sivan toledo design implement new outofcor spars choleski factor method acm transact mathemat softwar tom v n p march dror ironi gil shklarski sivan toledo parallel fulli recurs multifront spars choleski futur gener comput system v n p april abdou guermouch jeanyv lexcel construct memoryminim schedul multifront method acm transact mathemat softwar tom v n p march olaf schenk klau grtner twolevel dynam schedul pardiso improv scalabl share memori multiprocess system parallel comput v n p februari patrick r amestoy abdou guermouch jeanyv lexcel stphane pralet hybrid schedul parallel solut linear system parallel comput v n p februari patrick r amestoy iain s duff jeanyv lexcel xiaoy s li analysi comparison two gener spars solver distribut memori comput acm transact mathemat softwar tom v n p decemb xiaoy s li jame w demmel superlu_dist scalabl distributedmemori spars direct solver unsymmetr linear system acm transact mathemat softwar tom v n p june olaf schenk klau grtner solv unsymmetr spars system linear equat pardiso futur gener comput system v n p april michel benzi precondit techniqu larg linear system survey journal comput physic v n p novemb patrick r amestoy iain s duff stphane pralet christof vmel adapt parallel spars direct solver architectur cluster smp parallel comput v n p novemberdecemb anshul gupta recent advanc direct method solv unsymmetr spars system linear equat acm transact mathemat softwar tom v n p septemb timothi a davi column preorder strategi unsymmetricpattern multifront method acm transact mathemat softwar tom v n p june nichola i m gould jennif a scott yifan hu numer evalu spars direct solver solut larg spars symmetr linear system equat acm transact mathemat softwar tom v n pe june a n f klimowicz m d mihajlovi m heil deploy parallel direct spars linear solver within parallel finit element code proceed th iast intern confer parallel distribut comput network p februari innsbruck austria a bendali y boubendir m fare fetilik domain decomposit method coupl finit element boundari element larges problem acoust scatter comput structur v n p may