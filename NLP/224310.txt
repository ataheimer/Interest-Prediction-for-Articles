t distribut chemic process optim applic gigabit network a evalu impact gigabit network implement distribut chemic process optim applic optim problem formul stochast linear assign problem solv use think machin cm simd cray c vector comput psc intel iwarp mimd system cmu connect gigabit nectar testb report experi distribut applic across heterogen set system present measur show commun requir applic depend structur applic use detail trace build applic perform model use estim elaps time applic differ comput system network combin result show applic benefit highspe network need high network throughput increas comput system get faster also observ support high burst rate critic although structur applic commun overlap comput relax bandwidth requir b introduct highperform network made attract distribut computeintens applic across comput system connect localarea widearea network obviou benefit applic combin resourc sever system reduc execut time heterogen comput special case distribut comput ad benefit applic compon map onto appropri architectur thu optim effici comput result heterogen comput result superlinear speed up ie use n system applic run n time faster individu system eg import question critic network perform success distribut comput mani coarsegrain applic distribut success across rel slow network highspe network need want appli distribut comput wide class applic includ grand challeng nation challeng applic first indic mani applic comput intens also data intens larg data set exchang distribut task second observ comput system get faster network keep up paper evalu impact network bandwidth perform chemic process optim applic applic econom import chemic process industri repres larg class optim problem field use twostep evalu strategi first distribut applic across three system connect gigabit nectar testb intel iwarp system mimd cmu think machin cray c vector comput psc implement show feasibl distribut comput class problem allow us identifi problem distribut larg applic across heterogen system evalu howev limit singl set system singl network second step evalu use detail trace inform collect execut build perform model applic model captur commun comput requir applic data depend differ task model use estim execut time differ comput system differ network remaind paper organ follow section give overview applic section discuss map nectar testb section describ experiment set comput result present applic perform model section use model studi impact network node perform applic execut time section summar section applic overview model optim applic primari tool decis make throughout chemic process industri typic engin exampl process design materi alloc optim control realtim optim process schedul product capac plan applic uncertainti inher decis model process ie model paramet estim real valu although failur account uncertainti key paramet decis problem lead nonoptim solut determinist optim method still predomin chemic engin today includ explicit treatment uncertainti model increas comput requir beyond capabl today comput optim resourc alloc problem express linear assign problem lap state follow given n resourc eg raw materi n demand eg process unit set cost c ij assign resourc demand j find set one toon resourcedemand assign minim total cost z primal lap defin follow lap subject x i i term denot sourc termin node set correspond resourc demand respect term denot bipartit graph match edg set i connect sourc termin node exclus manner despit combinatori larg solut space varieti polynomi time algorithm develop achiev effici exploit special structur linear assign problem see detail serial parallel algorithm lap problem stochast cost c ij defin probabilist ie repres probabl distribut instead singl valu stochast lap problem solv convert determinist problem use certainti equival transform howev transform increas size lap problem exponenti furthermor transform alter structur problem solut procedur special determinist lap longer appli algorithm appropri stochast lap cost independ normal distribut variabl present distribut stochast lap solut stochast lap problem combin divers task best suit differ type comput system eg mpp fast scalar processor consequ applic good candid heterogen comput section describ map stochast lap onto set heterogen system connect nectar gigabit testb discuss solut method well mpp comput implement result report applic overview map figur show structur map distribut chemic process optim applic real world model ie product cost data simul iwarp sampl normal distribut mean rang standard deviat randomli set cost element product data sent c analyz use comput lap cost matrix mean statist model analysi statist reduct c send cost matrix cm lap solver initi entir cost matrix receiv cm comput reduc cost matrix send indic potenti optim assign c maximum cardin match comput c base initi zeroel indic reduc cost matrix unweight bipartit match cm complet weight match portion lap solut complet implement applic would includ bayesian infer step gener probabl manifold product data would also use mean varianc lap solver simplif fundament alter structur map applic product data process network market influenc process disturb raw materi select stochast linear assign problem optim initi match model analysi bayesian infer real world materi process j pq y n q n ts raw materi product pq y l pyq pq dq figur map stochast linear assign problem nectar testb map applic shown figur driven perform suitabl task system iwarp mimd system well suit independ complex function evalu requir simul integr problem c use highperform vector machin well suit statist reduct comput serial initi match procedur cm simd machin well suit row column scan matrix arithmet oper characterist weight match portion lap solver inde task could theoret run system none task well suit machin sinc singlemachin version applic cannot measur speedup expect effici processor util result superlinear speedup implement applic develop ad iwarp sampl gener phase exist highli optim lap solver run c cm illustr import advantag heterogen comput larg applic built exist separatelydevelop applic compon allow fast applic develop without port effort expect opportun one main motiv heterogen comput highspe network implement program control carri use pvm cm act master node read key paramet cm lap start data gener job iwarp gen analysi job c ana simul plant data gener iwarp sent use stream packag hippi network read c use socket interfac commun c cm dhsc librari dhsc support distribut comput across cm c use hippi network interconnect stream packag dhsc use raw hippi actual commun fact varieti tool use ad consider complex distribut applic moreov sinc tool support messag pass hide detail system individu expertis system help program develop program tool hide system detail parallel compil clearli need make distribut comput access user figur show differ comput task applic interact figur scale horizont bar repres comput task execut differ system arrow repres commun applic delin two main phase i data gener analysi ii lap solut first phase consist data gener iwarp data analysi c lap initi cm sinc time requir lap initi cm insignific phase end last packet cost matrix data reach cm c gener analysi pipelin minim execut time commun view separ stage pipelin pipelin comput stage execut concurr execut time determin slowest stage pipelin lap solut phase applic mainli perform cm remot procedur call rpc c perform unweight initi match phase oper includ commun perform serial execut time sum execut time compon data gener statist analysi lap solver initi convert reduc cost matrix unweight bipartit match weight bipartit match time data gener analysi lap solut figur applic flowchart stochast optim problem commun requir applic commun requir summar tabl tabl give sourc destin data format data size four data stream applic compon item tabl refer applic execut imag resid comput node program gen ana lap initi compris data gener analysi phase applic program lap fem denot lap solver initi match lap solut phase applic data sourc data type destin size plant data gen iwarp float ana c n n ts stochast model ana c int lap cm n cost data lap cm int fem c n initi match fem c int lap cm n tabl summari applic commun requir commun requir applic depend size n lap number sampl n ts use simul plant data number grid point n use map probabl manifold number stochast paramet n q stochast lap applic execut n n ts sinc probabl manifold map ie bayesian infer problem omit n q sinc cost assum uncertain use mean varianc lap solut instead mean would doubl commun requir stochast model data stream implement reduct factor invers proport n discuss detail section seen tabl data reduct analysi step ana c first phase applic significantli larger data requir second data format chang sever time comput result present layer overhead gener sampl repres bit ieee float point number iwarp transmit c convert bit cray float point format statist analysi c sampl mean convert bit integ transmit cm data exchang cm c initi match procedur use integ represent howev result differ data represent c cm transfer still requir expens transform structur applic figur four data stream differ characterist first stream sent continu stream kbyte packet sinc iwarp send data gener second stream sent sequenc burst number burst degre pipelin ie number block use comput data transfer c exampl degre pipelin problem size k stream would consist burst mbyte each final last two data stream sent singl burst experiment result distribut optim applic describ execut nectar gigabit testb may section describ execut environ present analyz measur nectar testb nectar testb one five nation gigabit testb fund arpa nsf cnri nectar testb joint effort nectar group cmu bellcor pittsburgh supercomput center psc bell atlant goal testb build gigabit metropolitan area network man demonstr valu applic testb consist twentyf dec alpha workstat iwarp parallel array paragon cmu campu cray c cray td cm alpha cluster pittsburgh supercomput center psc alpha workstat iwarp use network interfac provid architectur support copi avoid optim throughput paragon alpha frame buffer alpha file server cmu campu pittsburgh supercomput center km atmsonet file server alpha alpha alpha alpha file server parallel data lab vision lab offic figur nectar testb highperform distribut comput system network figur connect system consist two hippi base lan link gb atmsonet link repres metropolitan area network man execut run report paper use hippi link run parallel atmsonet link peak throughput link mbsec howev latenc set hippi connect across km link packet maximum achiev throughput mbsec packet size kbyte experiment set solv lap problem instanc size number sampl n ts fix anoth paramet experi degre pipelin gener analysi phase block size use c analysi model data similarli size run random number seed use iwarp gener simul plant data ident lead gener ident pseudorandom cost matric done allow repeat ident run help isol element system perform experi cm run dedic mode ie k processor attach user front end result repeat run lap compon cm c applic compon run singl node interact mode variat elaps time occur repeat instanc problem iwarp array run dedic mode variat iwarp elaps time occur repeat instanc problem due nonconst load front end measur analysi experiment data problem size k plot figur problem size k k give similar result figur show total execut time applic broken gener gen lap initi match c fem reduct cost matrix plu lap weight match solut lap iwarp time domin pipelin execut gener analysi phase time gen repres total time phase time c cm shown time shown function degre pipelin elaps time sec pipelin c npassesnnrow fem gen figur elaps time versu degre pipelin gener analysi phase size k k problem gener compon account total execut time ie iwarp bottleneck observ first phase take slightli longer highest lowest degre pipelin effect small surpris sinc block size rang mbyte mbyte iwarpc commun factor smaller ccm commun larg enough allow effici commun number block larg enough pipelin fill drain time signific figur show cpu time model analysi data transfer convers comput c k case data read anaread_data iwarp convert cray float point format anacfc statist comput apost perform reduc data set cost mean sent anadhsc_writ cm shown figur read write cpu time rel small vector format convers c quit fast nsec per float point number nearli costli statist comput indic improv convers routin elimin agre singl represent highli desir sinc cpu time not elaps time record program ana c data pipelin effect second pipelin c npassesnnrow anadhsc_writ anaread_data anacfc apost figur c cpu time versu degre pipelin analysi step idl initialmatch sec solut sec figur initi match cm idl versu lap solut time size k x axi figur show total execut time second phase applic lap solver run cm lap c lap initi match elaps time c left black full curv cm idl right red dash curv given versu total lap solut time differ run solver sinc cm ran fulli dedic mode cm time ident run problem size chang lap solut time result load chang c graph confirm thi close correspond increas initi match time wait idl time cm increas overal lap solut time implic distribut comput system load imbal one machin affect util comput resourc system run distribut applic effici requir care alloc system resourc reason predict respons time system use applic applic perform model project behavior applic result chang distribut comput system ie number comput node network bandwidth develop model applic compon next section use model examin sensit applic execut time network comput system perform comput model comput model repres execut time task applic function problem size and case iwarp cm size system paramet summar tabl model deriv use data larg seri parameter run model present section spars approach discuss tradeoff spars dens approach section function paramet valu problem size n kk number data sampl n tsrang cost matrix valu r number iwarp node p iwarp measur number cm node p cm kk measur bandwidth iwarp c bw hippi bandwidth c cm bw hippi tabl model paramet use experi data gener model gen run iwarp time test indic node iwarp array capabl gener per second rate sampl gener correspond directli number iwarp node doubl number node doubl rate result follow elapsedtim perform model gener sampl iwarp ts data analysi model ana run c use singl node c elaps time close correl data size n model analysi c obtain regress experiment data is sampl mean comput lap solver model lap run cm use data set parametr test statist model fit data give follow perform model lap solver constant are c natur scale equat number cost element number cm node also time solv weight match problem weakli correl degre precis character rang r lap initi match model fem run c use data set parametr test statist model fit data routin fem run c give r constant are c equat first second order term ie n n respect character lap size ratio nr repres natur indic rel degre precis traffic model traffic model shown tabl simpler comput model linear depend data time size data stream tabl transfer time inverselyproport sustain network bandwidth note sustain network bandwidth practic limit send receiv host abil put data in remov data network transfer model equat gen output iwarp c dt ts ana output c cm dt lap output cm c dt fem output c cm dt fem n tabl applic traffic model equat requir explan program lap run cm comput reduc cost matrix part initi procedur zero element matrix potenti optim assign lap program fem run c find optim initi match given zero element indic reduc cost matrix simplest way transmit element cm transmit entir reduc cost matrix c select zero element call dens approach howev use effici way call spars approach cm first locat indic zero element send vector indic c initi match advantag spars approach lower data transfer requir ie number indic sent c gener less n howev tradeoff cm less effici c construct spars incid matrix paramet equat reduct factor indic densiti incid matrix set give traffic model dens approach scalar applic depend lie gener n depend rel precis problem defin ratio n r use experi invers proport problem size perform analysi equat repres perform model distribut applic section use model studi perform two phase comput individu combin degre freedom model are lap problem size number node use iwarp cm machin sustain network bandwidth also use model compar dens spars transfer option cm c look differ structur two applic phase influenc depend execut time network bandwidth data gener analysi perform model result pipelin gener analysi phase throughput system determin slowest compon note iwarp c overlap commun comput commun phase independ stage pipelin elaps time gener analysi phase maximum genx dt ana realist paramet iwarp comput iwarp c commun limit perform phase figur show estim gener analysi time function two limit paramet iwarp node bandwidth time phase figur estim execut time sec gener analysi phase size k function network bandwidth mbsec iwarp system size number node experi use node iwarp system model predict execut time second close match measur time gen figur note point flat part graph respect network bandwidth indic gener phase limit comput iwarp network bandwidth scale iwarp system node equival node paragon system applic node node paragon coprocessor use comput execut time drop almost linearli demonstr sustain applic throughput mbsec iwarp c network becom bottleneck iwarp grow node note paragon system much larger node built mbsec network bandwidth mean excess data analysi c becom bottleneck system size p iwarp node iwarp c bandwidth lap solut perform model comput commun occur sequenti lap solut phase elaps time sum lapx dt fem figur show execut time lap solver function network bandwidth number cm node shape graph differ graph summar gener analysi phase specif flat region impact network bandwidth absolut execut time independ number cm node cm node bandwidth time phase figur estim execut time sec lap solver phase size k function network bandwidth mbsec size cm system number node spars incid matrix transfer cm c experi use cm k node bandwidth c cm mbsec model predict execut time second close measur time lap plu fem figur note ccm bandwidth would need present layer format convers cm higher throughput execut time would reduc applic perform model combin equat give overal applic perform model account iwarp data gener transfer overlap discuss abov figur show sensit network bandwidth number iwarp node given k cm node bw result compar figur sinc data gener analysi phase applic domin execut time again strongest sensit network bandwidth limit data flow iwarp c adequ number iwarp node avoid gener comput bottleneck rel iwarp node gener comput becom restrict network shown graph observ experi iwarp node bandwidth time figur estim execut time sec combin lap applic size k function iwarpccm network bandwidth mbsec iwarp system size number node spars approach figur show overal applic perform sensit network bandwidth number cm node use iwarp node assum bw data gener comput iwarp limit high bandwidth improv occur increas bandwidth data gener rate gain increas number node cm base line time remain second due data gener bottleneck figur b show compar sensit use iwarp node increas number iwarp node data gener bottleneck remov overal perform continu improv network bandwidth increas cm node bandwidth time a iwarp node cm node bandwidth time b iwarp node figur estim execut time sec lap applic size k function iwarpccm network bandwidth mbsec size cm number node two iwarp system size tradeoff dens spars reduc matrix transfer two altern exist transfer initi reduc cost matrix zero element indic cm c initi match comput see section spars approach cm locat zeroel indic initi reduc cost matrix pack incid vector send vector c initi match subproblem dens approach cm send entir initi reduc cost matrix c form spars incid matrix advantag spars approach lower data transfer requir sinc densiti term equat bound howev tradeoff cm less effici c construct spars incid matrix cm node bandwidth time phase figur estim execut time sec lap solver phase size k function network bandwidth mbsec size cm system number node dens incid matrix transfer cm c figur time lap solut phase plot function network bandwidth number cm node dens matrix transfer method problem size k sensit network bandwidth minim spars method figur while expect dens transfer method show higher sensit network bandwidth figur sinc time pack incid matric cm rel small compar overal lap solut spars method superior perform dens method almost casesth dens method sometim faster small data set howev c time dens method could acceler use singl c node multipl c node use expect dens method would faster spars method network bandwidth suffici high impact network throughput execut time impact network speed overal applic perform view term percent time applic commun bound vs comput bound interconnect comput speed number node network perform requir lap applic shown figur applic phase figur commun comput oper overlap henc long network perform adequ applic run complet comput bound increas network bandwidth speed applic decreas network bandwidth immedi slow applic down complet stop limit phase figur commun comput oper serial case clearli key accept perform keep commun time significantli comput time increas decreas network perform direct effect elaps time phase applic commun time bind not practic insignific due smaller data set transfer from statist reduct cost data percent elaps time bound commun phase iwarp node figur estim percent elaps time applic bound commun versu effect network bandwidth variou iwarp size lap size k percent elaps time bound commun phase cm node figur phase estim percent elaps time applic bound commun versu effect network bandwidth variou cm size lap size k spars transfer conclus paper describ distribut chemic process optim applic across heterogen system consist intel iwarp cray c think machin cm comput connect nectar gigabit testb implement demonstr sever benefit heterogen comput effici execut abil build applic connect exist separatelydevelop applic compon across network without port code note benefit eg effici system util may realiz differ system simultan dedic applic suffici network bandwidth avail creat organiz problem supercomput center implement applic also demonstr difficulti distribut applic across heterogen comput sinc expertis number differ comput system requir better program tool requir time hide detail system integr system better measur perform model applic show sensit execut time network bandwidth depend strongli structur distribut applic comput commun serial rpcbase solut phase applic network delay includ increas latenc due physic distanc increas execut time abl burst high rate critic averag bandwidth requir applic indic commun requir contrast commun comput overlap pipelin gener analysi phase applic network bandwidth requir relax speed light latenc hidden specif averag bandwidth requir applic repres bandwidth sustain network acknowledg would like thank michael hemi jamshid mahdavi todd mummert help distribut optim applic gregori j mcrae input conceptu applic also grate acknowledg use comput resourc pittsburgh supercomput center especi psc applic support group also grate use comput resourc sandia nation lab final prepar manuscript research support nation scienc foundat defens advanc research project agenc cooper agreement ncr corpor nation research initi advanc research project agenc dod monitor space naval warfar system command spawar contract nc r cray td system architectur overview model model valu stochast program integr solut highspe parallel comput gigabitsec wide area comput network potenti applic technolog challeng schedul presenc uncertainti probabilist solut assign problem schedul presenc uncertainti linear assign problem solut largescal model optim problem use heterogen supercomput system plan uncertainti use parallel comput pvm system supercomput level concurr comput heterogen network workstat recent develop evalu optim flexibl chemic process gigabit io distributedmemori system architectur applic paragon xp product overview gb sonet datalink stsc input hippi interfac gigabit comput network shortest augment path algorithm dens spars linear assign problem softwar support outboard buffer checksum experi gigabit neurosci applic cm combinatori optim network matroid deploy hippibas distribut supercomput environ pittsburgh supercomput center deploy hippibas distribut supercomput environ pittsburgh supercomput center linear assign problem implement test primaldu algorithm assign problem run climat model heterogen gigabit network testb architectur evalu highspe network subsystem distributedmemori system host interfac architectur highspe network program task data parallel multicomput network supercomput experi cray cm hippi connect tr shortest augment path algorithm dens spars linear assign problem warp integr solut highspe parallel comput gigabit network testb exploit task data parallel multicomput experi gigabit neurosci applic cm architectur evalu highspe network subsystem distributedmemori system softwar support outboard buffer checksum gigabit io distributedmemori machin host interfac architectur highspe network ctr michael hemi peter steenkist gigabit io distributedmemori machin architectur applic proceed acmiee confer supercomput cdrom pe decemb san diego california unit state peter steenkist highspe network interfac distributedmemori system architectur applic acm transact comput system toc v n p feb peter steenkist networkbas multicomput practic supercomput architectur ieee transact parallel distribut system v n p august