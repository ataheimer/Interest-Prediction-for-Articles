t prune algorithm rule learn a preprun postprun two standard techniqu handl nois decis tree learn preprun deal nois learn postprun address problem overfit theori learn first review sever adapt pre postprun techniqu separateandconqu rule learn algorithm discuss fundament problem primari goal paper show solv problem two new algorithm combin integr pre postprun b introduct separateandconqu rulelearn system gain popular recent success induct logic program algorithm foil quinlan analyz differ prune method type induct rule learn algorithm discuss problem main contribut paper two new algorithm topdown prune tdp approach combin pre postprun increment reduc prune irep effici integr preand postprun preprun decis combin pre postprun integr pre postprun postprun preprun liter postprun decis figur prune method separateandconqu rule learn algorithm prune common framework avoid problem overfit noisi data basic idea incorpor bia toward gener simpler theori order avoid overli specif theori tri find explan noisi exampl preprun method deal nois learn instead tri find theori complet consist given train data heurist socal stop criteria use relax constraint stop learn process although posit exampl may yet explain neg exampl may still cover current theori final theori learn one pass see figur separateandconqu rule learner like cn clark niblett foil quinlan fossil furnkranz use form nois handl anoth famili algorithm deal nois learn postprun algorithm typic first induc theori complet consist train data theori examin rule condit discard seem explain characterist particular train set thu reflect true regular domain figur show schemat depict process qualiti found rule condit commonli evalu separ set train exampl seen learn postprun algorithm includ reduc error prune rep brunk pazzani grow cohen shown effect noisehandl howev also ineffici wast time learn overfit concept descript subsequ prune signific portion rule condit one remedi problem combin pre postprun purpos preprun heurist use reduc not entir prevent amount overfit learn prune effici sketch third part figur particular implement approach topdown prune tdp furnkranz use simpl algorithm gener set theori prune differ degre top down generaltospecif order accuraci theori evalu separ set data specif theori accuraci compar accuraci best theori far submit subsequ postprun phase experi show initi topdown search better start theori effici overfit phase classic postprun algorithm search typic return theori closer final theori postprun phase also sped up less prune oper need get final theori motiv success method develop rigor approach tightli integr pre postprun instead learn entir theori prune thereaft increment reduc error prune irep furnkranz widmer prune singl claus right learn new algorithm entir avoid learn overfit theori use postprun method preprun stop criterion shown figur method signific speedup achiev noisi domain avoid problem approach incorpor postprun irep also learn accur theori learn algorithm mani rule learn algorithm tri construct rule socal separateandconqu strategi method root earli day machin learn cover algorithm famou aq famili michalski michalski mozet hong lavrac cn clark niblett clark boswel combin aq cover strategi greedi informationbas test select id quinlan yield power rule learn algorithm term separateandconqu coin pagallo haussler context learn decis list final separateandconqu learn basic control structur foil algorithm effici induc logic program quinlan pioneer signific research field relat learn induct logic program figur show basic separateandconqu rule learn algorithm input algorithm set posit neg exampl target concept output set rule abl prove given posit exampl none neg exampl repres rule form prolog claus gener separateandconqu learn algorithm foil procedur separateandconquerexampl negativecov returntheori figur separateandconqu rule learn algorithm concept liter proposit learn as cn condit test valu certain attribut concept relat learn as foil one also specifi relat attribut head condit rule gener prolog liter consid set rule prolog program ie rule check order one fire exampl fulfil condit rule consequ classifi instanc learn concept rule fire instanc consid member concept separateandconqu construct rule success ad condit righthand side current rule process repeat enough condit found rule neg exampl posit exampl cover rule separ train set next rule learn remain exampl henc name separateandconqu rule learn way posit exampl left method guarante posit exampl cover least one rule complet rule cover neg exampl consist simpl separateandconqu algorithm figur sever drawback realworld data may noisi noisi data problem mani learn algorithm hard distinguish rare except erron exampl fundament algorithm figur form complet consist theori i e tri explain posit exampl none neg exampl presenc nois therefor attempt find explan neg exampl erron classifi posit tri exclud posit exampl neg classif train set explan noisi exampl typic complic exhibit low predict accuraci classifi unseen exampl problem known overfit nois one remedi problem tri increas predict accuraci consid complet consist theori also simpl theori may overgener train exampl final theori allow deliber cover neg train exampl leav posit train exampl uncov order learn simpler predict theori usual achiev via prune heurist procedur prepruningexampl negativecov stoppingcriteriontheorynewclausecov exit exit returntheori figur rule learn algorithm use preprun preprun figur show adapt simpl separateandconqu algorithm order address noisi data preprun heurist algorithm ident one figur except inner loop contain call subroutin stoppingcrit rion stop criterion heurist determin stop ad condit rule stop ad rule concept descript current rule new condit ad fulfil stop criterion inner loop termin incomplet claus ad concept descript claus contain liter assum claus found explain remain posit exampl theori without claus return remain posit exampl thu consid noisi classifi neg return theori separateandconqu algorithm employ stop criteria nois handl commonli use among encod length restrict heurist use induct logic program algorithm foil quinlan base minimum descript length principl rissanen tri avoid learn complic rule cover exampl make sure number bit need encod current claus less number bit need encod instanc cover it ffl signific test first use proposit cn induct algorithm clark niblett later relat learner mfoil dzeroski bratko test signific differ distribut posit neg number bit need encod train instanc log nlog number train instanc p posit instanc cover current claus liter encod specifi relat log relat bit variabl log variabil bit whether negat bit sum term liter reduc log n sinc order liter within claus gener irrelev exampl cover rule overal distribut posit neg exampl compar likelihood ratio statist distribut degre freedom desir signific level insignific rule reject cutoff complex complex cutoff accuraci accuraci figur accuraci complex vs cutoff ffl cutoff stop criterion use separateandconqu learn system fossil fossil use search heurist base statist corr lation enabl judg relev liter uniform scale thu user requir condit consid claus construct certain minimum correl valu cutoff paramet properti use simpl robust criterion filter nois expect tupl origin nois data low correl predic background knowledg differ set valu caus differ amount preprun set result learn theori complet consist current train set everi liter correl hand gener empti theori learn trivial learn problem background liter correl figur show typic plot accuraci rule complex vs differ valu cutoff paramet commonli use krk endgam classif task nois ad accur rule found cutoff valu approxim higher cutoff valu result overgener theori lower set cutoff obvious result overfit data thu fossil cutoff paramet may view mean directli control overfit avoid bia schaffer log pn pn pn pn short descript krk domain along experiment setup found begin section procedur postpruningexampl splitratio exampl growingset pruningset loop exit loop returntheori figur postprun algorithm wolpert set cutoff good gener heurist seem independ nois level data furnkranz postprun preprun approach tri avoid overfit rule gener postprun approach first ignor problem overfit nois learn complet consist concept descript result theori subsequ analyz if necessari simplifi gener order increas predict accuraci unseen data postprun approach commonli use decis tree learn algorithm cart breiman friedman olshen stone id quinlan assist niblett bratko overview comparison variou approach found minger esposito malerba semeraro reduc error prune common among method reduc error prune rep simpl algorithm adapt decis tree learn quinlan separateand conquer rule learn framework pagallo haussler brunk pazzani begin train data split two subset grow set usual prune set first phase attent paid nois data concept descript explain posit none neg exampl learn grow set result theori simplifi greedili delet condit rule theori delet would result decreas predict accuraci measur prune set pseudocod version algorithm shown figur subroutin prunetheori simplifi current theori delet condit rule usual one time result set theori select one highest accuraci prune set continu prune theori repeat accuraci best prune theori predecessor rep shown learn accur theori preprun algorithm foil krk domain sever level nois brunk pazzani problem reduc error prune although rep quit effect rais predict accuraci noisi domain brunk pazzani sever shortcom discuss section particular suggest postprun incompat separateandconqu learn strategi effici cohen shown worstcas time complex rep bad random data n number exampl grow initi concept hand n therefor long run cost prune far outweigh cost gener initi concept descript alreadi higher cost use preprun algorithm entir avoid overfit hillclimb rep employ greedi hillclimb strategi liter claus delet concept definit predict accuraci prune set greedili maxim possibl oper lead decreas predict accuraci search process stop local maximum howev noisi domain theori gener grow phase much specif see figur rep prune signific portion theori ampl opportun err way therefor also expect rep specifictogener search slow also inaccur noisi data separateandconqu strategi postprun algorithm origin research decis tree learn usual wellknown divideandconqu learn strategi use node current train set divid disjoint set accord outcom chosen test thi algorithm recurs appli set independ although separateandconqu approach share mani similar divid andconqu strategi one import differ prune branch decis tree never affect neighbour branch wherea prune liter rule affect subsequ rule figur a illustr postprun decis tree learn work right half initi grown tree cover set c train instanc prune algorithm decid prune two leav ancestor node becom leaf cover exampl cd left branch decis tree influenc oper hand prune liter claus mean claus gener ie cover posit instanc along neg instanc consequ addit posit neg instanc remov train set cannot influenc learn subsequ claus exampl figur b first three rule simplifi cover exampl origin version cover also exampl third rule cover sever exampl second rule cover third rule could easili remov postprun algorithm necessarili case exampl guarante prune train exampl a prune train exampl train exampl c b figur postprun a divideandconqu b separateandconqu learn algorithm second rule one prune version good explan remain set exampl b b subset origin set b prune oper gener concept ie increas set cover exampl might well good explan b need total differ set liter explan superset b thu learner may lead garden path unprun claus begin theori may chang evalu candid liter subsequ claus wrong choic liter cannot undon prune grow algorithm solv problem section particular effici topdown postprun algorithm base techniqu use pagallo haussler propos cohen like rep grow algorithm first find theori overfit data instead prune intermedi theori delet result decreas accuraci prune set first step intermedi theori augment gener claus second step claus expand theori iter select form final concept descript claus improv predict accuraci prune set found gener claus intermedi theori form repeatedli delet final sequenc condit claus error grow set goe least thu grow improv upon rep replac bottomup hillclimb search rep topdown approach instead remov useless claus liter specif theori add promis gener rule initi empti theori result signific gain effici along slight gain accuraci experi cohen show howev asymptot time complex grow postprun method still complex initi rule grow phase recent shown cameronjon explan speedup gain topdown strategi start empti theori mani noisi domain much closer final theori overfit theori also seen figur look complex specif theori complex optim theori cutoff thu surpris grow shown outperform rep varieti dataset cohen howev still suffer ineffici caus need gener overli specif theori first pass combin pre postprun section seen intermedi theori result initi overfit phase much complex final theori postprun ineffici case work perform learn phase undon prune phase natur solut problem would start prune phase simpler theori idea first investig cohen effici postprun algorithm grow see section combin weak preprun heurist speed learn phase goal preprun context entir prevent overfit reduc amount thu subsequ postprun phase less work less like go wrong howev alway danger predefin stop criterion overgener theori section therefor discuss altern approach search appropri start point postprun phase topdown prune one advantag fossil simpl effici cutoff stop criterion furnkranz close search heurist fossil need mere comparison heurist valu best candid liter cutoff valu order decid whether add candid liter claus hand not properti use gener theori could learn fossil set cutoff paramet see figur basic idea behind algorithm follow assum fossil tri learn theori cutoff unless one liter background knowledg perfectli discrimin posit neg exampl which case trivial exampl parentab childba find liter correl thu learn empti theori procedur alltheoriesexampl cutoff theori theori returntheori figur algorithm gener theori learnabl fossil howev rememb liter maximum correl use inform follow way make anoth call fossil cutoff set exactli maximum correl valu least one liter the one produc maximum correl ad theori typic follow sever liter correl valu higher new cutoff result new theori usual littl specif predecessor maximum correl liter cut rememb obvious valu old cutoff new maximum theori would learn thu choos valu cutoff next run also expect new theori specif previou one process repeat certain set cutoff liter prune maximumprunedcorrel thu specif theori reach figur show complet seri theori gener fossil noisefre exampl domain distinguish legal illeg posit kingrook chess endgam set cutoff paramet would yield one six theori on train set seen theori gener less gener specif order topdown simpler theori expect accur noisi domain best theori learn iter therefor may possibl stop gener theori soon reason good theori found order avoid expens learn mani overlyspecif theori may save lot work figur indic besid also possibl reus part previou theori point highest cutoff occur total cost gener complet seri concept descript may much higher cost gener mere specif theori at least case cutoff occur near end learn theori frequent case base idea conceiv algorithm shown figur use basic algorithm figur find best theori order avoid overgener tri find specif among reason good theori learn fossil use theori start point reduc error prune precis gener theori generaltospecif order evalu design test set data usual stop measur accuraci one theori fall measur accuraci best theori far minu one standard error classif last theori within se margin hope littl specif base idea cart breiman friedman olshen stone gener correct posit neg e e a correct posit neg correct posit neg correct posit neg correct posit neg correct posit neg figur gener seri theori krk domain procedur tdpexampl splitratio exampl growingset pruningset repeat loop exit loop returntheori figur combin pre postprun topdown prune gener subsequ gener use reduc error prune initi generaltospecif search good theori name method topdown prune tdp algorithm succe find start theori close final theori expect algorithm faster basic rep initi search good start theori ffl speed grow phase expens theori gener ffl speed prune phase prune start simpler theori thu number possibl prune oper smaller preliminari experi turn sometim cutoff happen point small fraction avail posit exampl cover clearli theori useless therefor ad constraint theori cover posit exampl grow set evalu prune set theori fulfil criterion improv ad claus achiev lower cutoff valu would need start new claus prune decis tree within one se best select standard classif error comput n p probabl misclassif estim prune set n number exampl prune set argument cours appli noisi domain nonnoisi domain specif theori gener precis thu algorithm slower gener theori cutoff note method may yield theori learnabl origin fossil valu cutoff paramet chang gener theori experiment result compar topdown prune tdp reduc error prune rep term accuraci runtim krk endgam domain artifici nois ad setup experi describ detail begin section algorithm split suppli data set grow ca prune set ca algorithm use reduc error prune describ brunk pazzani postprun phase order exclud possibl influenc underli learn algorithm ran rep use fossil basic learn modul averag accuraci run rep prune prune tdp prune prune tabl accuraci krk domain nois tabl show tdp wors rep term predict accuraci rep better train set size tdp heavili overprun one case tdp start theori correct unfortun one liter support prune set consequ prune thu yield theori mere happen rep got caught correct theori even get theori increas train set size tdp seem slightli superior rep although differ probabl small statist signific compar accuraci intermedi theori show tdp start significantli better theori rep see first line tabl obvious topdown search better start theori success particular higher train set size rep sometim get stuck local optimum return bad theori howev seen rep may profit rare case tdp less like get stuck local optimum prune start initi theori alreadi quit close final theori problem local optima greedi hillclimb also like appear tdp topdown search start theori at least domain intermedi theori usual appear iter tdp toplevel loop compar runtim rep tdp tabl confirm tdp significantli faster rep fact even faster rep initi phase overfit alon tdp find fairli gener theori rep gener huge theori fit noisi exampl expectedli increas train set size cost rep domin prune process tdp hand even manag decreas prune time grow train set size signific runtim increas exampl mainli due one set much specif theori learn cpu sec grow cpu sec prune time remain set averag runtim cpu sec grow cpu sec prune version rep use fossil better version use foil section show result obtain use implement foil gener initi theori rep averag runtim run rep grow prune total tdp grow prune total tabl runtim krk domain nois result confirm tdp exhibit fast converg toward good theori faster rep both learn prune start theori learn fossil becom increasingli accur train set grow mean learn faster also less less prune done integr pre postprun algorithm present section motiv observ postprun incompat separateandconqu learn strategi discuss section problem attempt solv postprun approach take account prune claus gener eventu cover exampl train set may influenc evalu candid liter subsequ claus increment reduc error prune basic idea increment reduc error prune irep instead first grow complet concept descript prune thereaft individu claus prune right gener ensur algorithm remov train exampl cover prune claus subsequ claus learn thu avoid exampl influenc learn follow claus figur show pseudocod version algorithm usual current set train exampl split grow usual prune set usual howev entir theori one claus learn grow set liter delet claus greedi fashion delet would decreas accuraci claus prune set result rule ad concept descript cover posit neg exampl remov train grow prune set remain train instanc redistribut new grow new prune set ensur two set contain predefin percentag remain exampl set next claus learn predict accuraci prune claus predict accuraci empti claus ie claus bodi fail claus ad concept descript irep return learn claus thu accuraci prune claus prune set also serv stop criterion postprun method use preprun heurist procedur irep exampl splitratio exampl growingset pruningset negativecov loop exit loop accuracyclausepruningset accuracyfailpruningset exit returntheori figur integr pre post prune increment reduc error prune algorithm prune entir set claus prune one success name increment reduc error prune irep expect irep improv upon postprun algorithm aim solv problem discuss section effici irep asymptot complex size train set significantli lower complex grow overfit theori shown omegagamma n log n assumpt cohen rep grow one claus pure random data cost n log n each approxim logn liter test n exampl irep consid everi liter claus prune ie log n liter evalu n exampl final claus found ie log n time thu cost prune one claus n log n assum size final theori constant overal cost hillclimb similarli grow irep use topdown approach instead rep bottomup final program found remov unnecessari claus liter overli specif theori repeatedli ad claus initi empti theori howev grow still gener intermedi overli specif concept descript irep directli construct final theori separateandconqu strategi irep learn claus order use prolog interpret subsequ rule learn claus complet learn prune cover exampl remov reason problem incompat learn strategi prune strategi cannot appear irep experiment result tabl show comparison runtim postprun algorithm irep krk domain artifici nois ad algorithm use foil inform gain criterion search heurist column initi rule growth refer initi grow phase rep grow common column rep grow give result prune phase onli total runtim rep grow runtim initi rule growth plu runtim rep grow irep phase tightli integr total valu runtim given domain initi rule growth rep grow irep krk tabl averag runtim obviou irep significantli faster postprun algorithm fact alway faster rep grow initi grow phase alon irep avoid learn intermedi overfit theori also seen grow prune algorithm much faster rep confirm result cohen order get idea asymptot complex variou algorithm perform loglog analysi cameronjon estim asymptot complex divid differ logarithm two runtim differ logarithm correspond train set size thu estim slope loglogplot tabul slope adjac train set size tabl domain initi rule growth rep grow irep tabl loglog analysi runtim noisi krk data fact tabl suggest irep subquadrat time complex consist conjectur irep time complex omegagamma n log n gener result get consist analysi perform cameronjon random data surpris view noiselevel degre random data particular evid support result rep complex omegagamma n initi rule grow phase on log n shown cohen also confirm main result cameronjon name asymptot complex grow asymptot complex initi rule grow phase origin suggest cohen howev experi absolut valu runtim grow prune phase neglig compar initi overfit phase rep often get caught local maxima abl gener right level interestingli observ that despit topdown search strategi grow also occasion overfit nois data phenomenon also predict cameronjon irep hand stop gener claus whenev found claus support prune set therefor irep expect fast runtim pure random data where rep grow expens high chanc first claus fit exampl prune set stop algorithm immedi without accept singl claus thu effect avoid overfit domain initi rule growth rep grow irep krk krk tabl averag accuraci term accuraci tabl irep also superior postprun algorithm although seem sensit small train set size reason bad distribut grow prune exampl may caus irep stop criterion prematur stop learn redistribut exampl new grow prune set learn new claus cannot help here littl redund data small sampl size howev larger exampl set size irep outperform algorithm experiment evalu test algorithm present paper varieti domain algorithm implement sicstu prolog major part implement common particular share interfac data use procedur split train set mode type symmetri inform background relat use restrict search space wherev applic inform gain use search heurist rep grow irep fossil correl heurist use fossil tdp runtim measur cpu second sun sparcstat elc summari experi krk domain first summar experi domain recogn illeg chess posit krk endgam muggleton bain hayesmichi michi domain becom standard benchmark problem relat learn system cannot solv trivial way proposit learn algorithm background knowledg contain relat like sign train instanc deliber revers gener artifici nois data learn concept evalu test set noisefre ex ampl use stateoftheart relat learner foilquinlan benchmark foil implement c use default set except v option set avoid introduct new variabl necessari task algorithm argument mode declar input effect prevent recurs algorithm train ident set size exampl report result averag run except train set size run perform complex task algorithm figur show curv accuraci runtim differ train set size irep bad start accuraci exampl achiev highest accuraci predict accuraci foil poorli stop criterion encod length depend train set size thu weak effect prevent overfit nois exampl foil learn concept rule incomprehens furnkranz irep hand consist produc correct understand rule approxim correct concept descript theori correctli identifi illeg posit except one white king black king white rook thu block check would make posit illeg white move postprun approach rep grow equal tdp lose accuraci compar them three howev rare find th rule specifi white king white rook must squar also seen preprun approach taken fossil need mani exampl order make heurist prune decis reliabl fossil hand fastest algorithm foil although implement c slower increas train set size learn claus fossil see also rep prove prune method ineffici grow effici prune algorithm still suffer expens overfit phase tdp faster rep grow abl start postprun much better theori rep grow irep howev learn much better theori faster grow prune phase tdp fact irep postprun integr preprun criterion littl slower fossil much accur thu said truli combin merit postprun accuraci preprun effici becom also appar figur accuraci with standard deviat observ differ run plot logarithm runtim current version foil avail anonym ftp ftpcssuozau file name pubfoilnsh integ n experi perform version train train irep grow fossil foil run time cpu sec train runtim vs train irep grow fossil foil figur krk domain nois differ train set size runtim cpu sec irep grow fossil foil figur krk domain nois exampl mesh domain also test algorithm finit element mesh design problem first studi describ detail dolsak muggleton problem mesh design break complex object number finit element order abl comput pressur deform forc appli object basic problem manual mesh design select optim number finit element edg structur sever author tri ilp method problem dolsak muggleton dzeroski bratko quinlan avail background knowledg consist attributebas descript edg topolog relat edg setup experi quinlan ie learn rule four five object data set test learn concept fifth object learn theori test quinlan littl differ setup use dzeroski bratko instead actual predict valu number finit element edg mere check possibl valu whether valu could deriv learn rule not basic differ test ground instanc wherea dzeroski bratko test target predic unbound valu number finit element posit exampl set also test learn theori neg exampl make sure overgener tabl two number given five set first number accuraci posit exampl onli second number show accuraci test neg exampl well given runtim total runtim learn prune irep clearli faster postprun algorithm without lose predict accuraci tdp find accur start theori rep shorter time span consequ prune time much shorter rep learn theori littl accur howev tdp faster grow although start prune phase algorithm accuraci fossil initi theori rep grow grow initi theori tdp irep tabl experi mesh domain simpler theori reason implement tdp use rep prune theori result initi search good start theori might worthwhil improv tdp use grow algorithm postprun phase also indic domain tdp initi topdown search effect krk domain work left postprun phase algorithm faster accur irep fossil cutoff howev fossil discov signific regular data thu consist learn empti theori all liter background knowledg correl nevertheless still best algorithm term accuraci show poorli algorithm domain hope abl improv result domain tri faster algorithm new data set dolsak bratko jezernik contain total object and thu hope provid redund howev compar studi new data set big interest phenomenon although prune liter gener claus posit exampl cover prune theori whole cover fewer posit exampl obvious mani learn rule gener improv accuraci much remov entir rule did therefor overal accuraci theori primarili optim delet mani rule cover posit exampl also equal greater number neg exampl also taken evid regular detect basic separateandconqu induct modul reliabl proposit data set also experi data set uci repositori machin learn databas previous use compar proposit learn algorithm appendix holt give summari result achiev variou algorithm commonli use data set uci repositori short descript set select experi remain set use either descript data set unclear two class could handl implement learn algorithm lymphographi data set remov exampl class normal find fibrosi order get class problem data use describ holt data set task learn definit minor class dataset background knowledg consist relat one variabl breast cancer accuraci stnd dev rang time fossil grow tdp irep hepat accuraci stnd dev rang time fossil rep grow tdp irep sick euthyroid accuraci stnd dev rang time fossil rep grow irep tabl result breast cancer hepat sick euthyroid domain one constant argument wherev appropri comparison two differ variabl data type allow well experi valu fossil cutoff paramet set runtim dataset measur cpu second sun sparcstat elc except mushroom krkpa dataset quit big thu run consider faster sparcstat s experi follow setup use holt ie algorithm train data test remain howev run perform algorithm data set result found tabl line show averag accuraci set standard deviat rang differ maximum minimum accuraci encount runtim algorithm result c decis tree learn system extens noisehandl capabl quinlan taken experi perform holt meant indic perform stateoftheart decis tree learn algorithm data set short look show result vari term accuraci quit consist glass g accuraci stnd dev rang time fossil rep grow irep vote accuraci stnd dev rang time fossil rep grow tdp irep vote vi accuraci stnd dev rang time fossil rep grow irep tabl result glass vote domain runtim irep fastest algorithm test problem secondbest remain tabl also confirm grow usual faster rep tdp result consist faster rep grow case indic initi topdown search good start theori overfit data much initi rule grow phase rep grow doe fossil runtim unstabl fastest algorithm dataset far slowest data set differ accuraci statist signific signific differ found krkpa chess endgam domain tdp fossil perform significantli wors algorithm fossil significantli better tdp vote vi domain outperform sometim algorithm use rang test use quickli determin signific differ medium valu small n sampl size mitteneck valu signific level signific level medium valu r rang found tabl krkpa accuraci stnd dev rang time fossil rep grow irep lymphographi class accuraci stnd dev rang time c on class fossil grow mushroom accuraci stnd dev rang time fossil grow tabl result chess krkpa lymphographi mushroom domain lymphographi domain gener c seem littl superior algorithm one cannot count result lymphographi rule learn algorithm presum easier class task howev rule learn algorithm seem competit allow structur analysi group domain subclass tabl contain domain overfit seem harm ie rep postprun phase significantli at least improv upon concept learn initi overfit phase tabl contain domain prune make signific differ final tabl contain domain prune cannot recommend exemplifi mushroom data overfit phase learn correct concept descript significantli better learn prune might justifi argu use separ run prune data comparison main purpos howev compar differ prune approach evalu merit prune itself result initi overfit phase rep grow tdp may nevertheless indic latter and come addit cost algorithm mushroom krkpa domain known free nois medic domain tabl noisi therefor assum group domain correspond amount nois contain data conclus paper discuss differ prune techniqu separateandconqu rule learn algorithm convent preprun method effici alway accur postprun method latter howev tend expens learn overspeci theori first addit ineffici point fundament incompat postprun method separateand conquer rule learn system solut investig two method combin integr pre postprun algorithm tdp perform initi topdown search hypothesi space find theori overfit train data still fairli gener theori use start theori subsequ postprun phase tri gener theori appropri level systemat algorithm vari cutoff paramet preprun algorithm fossil provid effici way gener theori generaltospecif order good start theori often found consider less time would need gener specif theori fit train exampl cours prune phase simpler theori also shorter prune phase specif theori irep integr pre postprun one algorithm instead postprun entir theori rule prune right learn experi show approach effect combin effici preprun accuraci postprun domain high redund realworld databas typic larg noisi thu requir learn algorithm effici noisetoler irep seem appropri choic purpos irep tdp deliber design close resembl basic postprun algorithm rep instanc alreadi point tdp improv use grow instead rep tdp postprun phase case irep chosen accuraci prune set basic prune stop criterion order get fair comparison rep concentr methodolog differ postprun irep effici integr pre postprun import advantag postprun method way evalu theori or rule irep case entir independ basic learn algorithm prune stop criteria improv perform elimin weak instanc point cohen accuraci estim lowcoverag rule high varianc therefor irep like stop prematur overgener domain suscept small disjunct problem holt acker porter cohen also point defici accuracybas prune criterion show stop criterion base descript length better prune criterion significantli improv irep accuraci without loss effici anoth way improv irep tri furnkranz irep tri improv upon rep prune rule level instead theori level investig way take tri improv upon irep algorithm prune liter level result algorithm rep seem littl stabl low train set size signific differ runtim could observ appear littl slower irep although asymptot algorithm clearli subquadrat current investig merit avoid loss inform caus need split train set separ grow prune set particular techniqu base wellknown minim descript length principl could provid valuabl altern acknowledg research sponsor austrian fond zur forderung der wissenschaftlichen forschung grant number pmat financi support austrian research institut artifici intellig provid austrian feder ministri scienc research would like thank gerhard widmer patient read improv numer version paper r classif regress tree investig noisetoler relat concept learn algorithm rule induct cn recent improv cn induct algorithm effici prune method separateandconqu rule learn system fast effect rule induct decis tree prune search state space fossil robust relat learner tight integr prune learn tight integr prune learn extend abstract increment reduc error prune concept learn problem small disjunct simpl classif rule perform well commonli use dataset pattern recognit ruleguid infer empir comparison prune method decis tree induc tion planung und statistisch auswertung von experimenten th experiment comparison human machin learn formal learn decis rule noisi domain boolean featur discoveri empir learn learn effici classif procedur applic chess end game learn logic definit relat minimum descript length principl categor the ori model shortest data descript overfit avoid bia overfit avoid bia tr learn decis rule noisi domain simplifi decis tree boolean featur discoveri empir learn experiment comparison human machin learn formal rule induct cn learn nonrecurs definit relat linu c program machin learn overfit avoid bia simpl classif rule perform well commonli use dataset fossil learn logic definit relat empir comparison prune method decis tree induct cn induct algorithm learn noisi exampl decis tree prune search state space tight integr prune learn extend abstract complex batch approach reduc error rule set induct ctr jo ranilla oscar luac antonio bahamond heurist learn decis tree prune classif rule ai commun v n p jo ranilla oscar luac antonio bahamond heurist learn decis tree prune classif rule ai commun v n p april johann frnkranz peter a flach roc n rule learn toward better understand cover algorithm machin learn v n p januari johann frnkranz round robin classif journal machin learn research p marco muselli diego liberati binari rule gener via ham cluster ieee transact knowledg data engin v n p novemb johann frnkranz separateandconqu rule learn artifici intellig review v n p jan s b kotsianti i d zaharaki p e pintela machin learn review classif combin techniqu artifici intellig review v n p novemb