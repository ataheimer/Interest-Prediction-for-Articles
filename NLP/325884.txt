t use model tree classif a model tree type decis tree linear regress function leav form basi recent success techniqu predict continu numer valu appli classif problem employ standard method transform classif problem problem function approxim surprisingli use simpl transform model tree inducerm base quinlan m gener accur classifi stateoftheart decis tree learner c particularli attribut numer b introduct mani applic machin learn practic involv predict class take continu numer valu techniqu model tree induct prove success address problem quinlan wang witten structur model tree take form decis tree linear regress function instead termin class valu leav numer valu attribut play natur role regress function discret attribut also handledthough less natur way convers classic decisiontre situat classif discret attribut play natur role prompt symmetri situat wonder whether model tree could use classif discov turn classifi surprisingli accur order appli continuouspredict techniqu model tree discret classif problem consid condit class probabl function seek modeltre approxim it classif class whose model tree gener greatest approxim probabl valu chosen predict class result present paper show model tree induc use gener classifi significantli accur decis tree produc c next section explain method use review featur respons good perform experiment result thirtythre standard dataset report section section briefli review relat work section summar result appli model tree classif model tree binari decis tree linear regress function leaf node thu repres piecewis linear approxim unknown function model tree gener two stage first build ordinari decis tree use split criterion maxim intrasubset variat target valu second prune tree back replac subtre linear regress function wherev seem appropri whenev model use predict smooth process invok compens sharp discontinu inevit occur adjac linear model leav prune tree although origin formul model tree linear model intern node use smooth process incorpor leaf model manner describ below section first describ salient aspect model tree algorithm describ procedur new paper model tree use classif justif procedur given next subsect follow give exampl infer class probabl artifici situat true probabl known modeltre algorithm construct use model tree clearli describ quinlan account scheme implement call describ wang witten along implement detail freeli avail version use paper differ describ wang witten improv handl miss valu describ appendix chang tune paramet necessari elabor briefli two key aspect model tree surfac discuss experiment result section first central idea model tree linear regress step perform leav prune tree variabl involv regress attribut particip decis node subtre prune away step omit target taken averag target valu train exampl reach leaf tree call regress tree instead second aspect smooth procedur that origin formul occur whenev model use predict idea first use leaf model comput predict valu filter valu along path back root smooth node combin valu predict linear model node quinlan calcul use model tree classif predict pass next higher node p predict pass node below q valu predict model node n number train instanc reach node below k constant quinlan default valu use experi below implement achiev exactli effect use slightli differ represent final stage model format creat new linear model leaf combin linear model along path back root leaf model automat creat smooth predict without need adjust predict made exampl suppos model leaf involv two attribut x y linear coeffici b model parent node involv two attribut z combin two model singl one use formula z continu way root give us singl smooth linear model instal leaf use predict thereaft smooth substanti enhanc perform model tree turn appli equal applic classif procedur figur show diagrammat form model tree builder use classifi cation data taken wellknown iri dataset upper part depict train process lower part test process train start deriv sever new data set origin dataset one possibl valu class case three deriv dataset setosa virginica versicolor varieti iri deriv dataset contain number instanc origin class valu set depend whether instanc appropri class not next step model tree induc employ gener model tree new dataset specif instanc output one model tree constitut approxim probabl instanc belong associ class sinc output valu model tree approxim necessarili sum one test process instanc unknown class process model tree result approxim probabl belong class class whose model tree give highest valu chosen predict class deriv dataset origin dataset predict class attribut target attribut target attribut class versicolor virginica instanc model tree attribut target attribut target a setosa b virginica c versicolor c versicolor f figur use classif use model tree classif x a x y c x y figur exampl use model tree classif a class probabl data gener b train dataset c infer class probabl justif learn procedur effect divid instanc space region use decis tree strive minim expect mean squar error model tree output target valu zero one train instanc within particular region train instanc lie particular region view sampl underli probabl distribut assign class valu zero one instanc within region standard procedur statist estim probabl distribut minim mean squar error sampl taken devroy gyoerfi lugosi breiman friedman olshen stone exampl consid twoclass problem true class probabl linear function two attribut x y pclassjx y depict figur a sum point dataset instanc gener randomli accord probabl thi uniformli distribut x y valu chosen probabl x y valu use determin whether instanc assign first second class data gener depict frank et al figur b class repres fill hollow circl appar densiti fill circl greatest lower left corner decreas toward upper right corner convers true hollow circl data figur b submit gener two model tree case structur tree gener trivialthey consist singl node root figur c show linear function fclassjx y repres tree discuss intim excel approxim origin class probabl data gener class boundari point intersect two plane figur c exampl illustr classifi base model tree abl repres obliqu class boundari one reason model tree produc outperform univari decis tree produc c anoth smooth regress function adjac leav model tree experiment result experi design explor applic model tree classif compar result decis tree induct linear regress determin compon essenti good perform specif address follow question classifi base model tree compar stateoftheart decis tree classifi base simpl linear regress import a linear regress process leav b smooth process answer first question compar accuraci classifi base smooth model tree gener prune decis tree gener c see often perform better howev perform improv might conceiv due aspect procedur convert nomin attribut n attribut valu binari attribut use procedur employ cart breiman et al gener one model tree class test ran c use exactli encod transform nomin attribut binari one use procedur employ gener one dataset class build decis tree dataset use class probabl provid c arbitr class refer result algorithm c also report result linear regress lr use inputoutput encod investig second question first compar accuraci classifi base model tree gener one base smooth regress tree srt note abov regress tree model tree constant function leaf node thu cannot repres obliqu class bound ari appli smooth oper routin appli model tree compar accuraci classifi base smooth model tree base unsmooth model tree umt use model tree classif tabl dataset use experi dataset instanc miss numer binari nomin class valu attribut attribut attribut balancescal breastw glass g glass heartstatlog hepat ionospher iri letter pimaindian segment sonar vehicl vote waveformnois anneal audiolog australian auto breastcanc horsecol hypothyroid german labor lymphographi primarytumor sick soybean vowel smooth regress tree special case smooth model tree unsmooth tree special case smooth tree minor modif code need gener srt umt model experi thirtythre standard dataset uci collect merz murphi use experi summar tabl first sixteen involv numer binari attribut last seventeen involv nonbinari nomin attribut well sinc linear regress function design numericallyvalu domain binari attribut special case numer attribut expect classifi base smooth model tree particularli appropri first group tabl summar accuraci method investig result give percentag correct classif averag ten tenfold nonstratifi crossvalid run standard deviat ten also shown fold use scheme result c star show signific improv correspond result vice versa throughout speak result significantli differ differ statist signific level accord pair twosid ttest pair data point consist estim obtain one tenfold crossvalid run two learn scheme compar tabl show differ method compar other entri indic number dataset method associ column significantli accur method associ row discuss result answer first question abov observ tabl outperform c fifteen dataset wherea c outperform five these number also appear boldfac tabl sixteen dataset numer binari attribut significantli accur nine significantli less accur none remain dataset significantli accur six significantli less accur five result show classifi base smooth model tree gener significantli accur prune decis tree gener c major dataset particularli numer attribut tabl show c significantli less accur c twelv dataset first column last row significantli accur five first row last column significantli less accur seventeen dataset significantli accur three result show superior perform due chang inputoutput encod complet discuss first question compar simpl linear regress lr c tabl show lr perform significantli wors seventeen dataset significantli wors c eighteen lr outperform eleven dataset c fourteen result linear regress surprisingli good howev dataset applic linear regress lead disastr result one cannot recommend gener techniqu answer second two question begin compar accuraci classifi base one base smooth regress tree srt assess import linear regress process leav which former incorpor latter not tabl show produc significantli accur classifi twentythre dataset significantli less accur one two compar cs prune decis tree classifi base smooth regress tree significantli less accur fifteen dataset use model tree classif tabl experiment result percentag correct classif standard deviat balancescal sigma sigma sigma sigma sigma sigma breastw sigma sigma sigma sigma sigma sigma glass g sigma sigma sigma sigma sigma sigma glass sigma sigma sigma sigma sigma sigma heartstatlog sigma sigma sigma sigma sigma sigma hepat sigma sigma sigma sigma sigma sigma ionospher sigma sigma sigma sigma sigma sigma iri sigma sigma sigma sigma sigma sigma letter sigma sigma sigma sigma sigma sigma pimaindian sigma sigma sigma sigma sigma sigma segment sigma sigma sigma sigma sigma sigma sonar sigma sigma sigma sigma sigma sigma vehicl sigma sigma sigma sigma sigma sigma vote sigma sigma sigma sigma sigma sigma waveformnois sigma sigma sigma sigma sigma sigma zoo sigma sigma sigma sigma sigma sigma anneal sigma sigma sigma sigma sigma sigma audiolog sigma sigma sigma sigma sigma sigma australian sigma sigma sigma sigma sigma sigma auto sigma sigma sigma sigma sigma sigma breastcanc sigma sigma sigma sigma sigma sigma heartc sigma sigma sigma sigma sigma sigma hearth sigma sigma sigma sigma sigma sigma horsecol sigma sigma sigma sigma sigma sigma hypothyroid sigma sigma sigma sigma sigma sigma german sigma sigma sigma sigma sigma sigma krvskp sigma sigma sigma sigma sigma sigma labor sigma sigma sigma sigma sigma sigma lymphographi sigma sigma sigma sigma sigma sigma primarytumor sigma sigma sigma sigma sigma sigma sick sigma sigma sigma sigma sigma sigma soybean sigma sigma sigma sigma sigma sigma vowel sigma sigma sigma sigma sigma sigma significantli accur five result show linear regress function leaf node essenti classifi base smooth model tree outperform ordinari decis tree final complet second question compar accuraci classifi base classifi base unsmooth model tree umt tabl show produc significantli accur classifi twentyf dataset significantli less accur classifi one comparison cs prune decis tree also lead conclus smooth process necessari ensur high accuraci modeltre base classifi tabl result pair ttest p number indic often method column significantli outperform method row lr relat work neural network obviou altern model tree classif task appli neural network classif standard procedur approxim condit class probabl function output node neural network approxim probabl function one class contrast neural network probabl function class approxim singl network model tree necessari build separ tree class model tree offer advantag neural network user make guess structur size obtain accur result built fulli automat much effici neural network moreov offer opportun structur analysi approxim class probabl function wherea neural network complet opaqu idea treat multiclass problem sever twoway classif prob lem one possibl valu class appli standard decis tree dietterich bakiri use c quinlan predecessor c gener twoway classif tree class howev found accuraci obtain significantli inferior direct applic c origin multiclass problemalthough abl obtain better result use errorcorrect output code instead simpl oneperclass code smyth gray fayyad retrofit decis tree classifi kernel densiti estim leav order obtain better estim class probabl function although improv accuraci class probabl estim three artifici dataset classif accuraci significantli better moreov result structur opaqu includ kernel function everi train instanc torgo also investig fit tree kernel estim leav time regress tree rather classif tree could appli classif problem manner model tree advantag abl repres nonlinear class boundari rather linear obliqu class boundari model tree howev suffer incomprehens model employ kernel estim import differ smyth et al torgo model tree algorithm smooth model use model tree classif adjac leav model tree substanti improv perform model tree classif problem saw also close relat method linear regress method find linear discrimin compar experiment result obtain ordinari linear regress find although mani dataset linear regress perform well sever case give disastr result linear model simpli appropri conclus work shown classif problem transform problem function approxim standard way success solv construct model tree produc approxim condit class probabl function individu class classifi deriv outperform stateoftheart decis tree learner problem numer binari attribut and often not problem multivalu nomin attribut too although result classifi less comprehens decis tree opaqu produc statist kernel densiti approxim expect time taken build model tree loglinear number instanc cubic number attribut thu model tree class built effici dataset modest number attribut acknowledg waikato machin learn group support new zealand foundat research scienc technolog provid stimul environ research thank anonym refere help construct comment m zwitter m soklic donat lymphographi primari tumor dataset appendix treatment miss valu explain instanc miss valu treat version use result paper test whenev decis tree call test attribut whose valu unknown instanc propag path result combin linearli standard way as quinlan problem deal miss valu train tackl problem breiman et al describ surrog split method which whenev split valu v attribut consid particular instanc miss valu differ attribut use surrog split instead appropri chosen valu v that is test v replac v attribut valu v select maxim probabl latter test effect former work describ paper made two alter pro cedur first simplif breiman origin procedur follow let set train instanc node whose valu split attribut known let l subset split v assign left branch r correspond subset right branch defin l r way surrog split v number instanc correctli assign left subnod surrog split correspond number right subnod probabl v predict v correctli estim chosen surrog split v maxim estim wherea breiman choos attribut valu v maxim estim simplif alway choos surrog attribut class but continu select optim valu v describ stratagem report wang witten second differ blur sharp distinct made breiman pro cedur accord origin procedur train instanc whose valu attribut miss assign left right subnod accord whether not produc sharp stepfunct discontinu inappropri case v poor predictor v modif employ version use present paper soften decis make stochast accord probabl curv illustr figur a steep transit determin likelihood test v assign instanc incorrect subnod assess consid train instanc valu attribut known first estim probabl p r v assign instanc miss valu rightmost subnod probabl assign left node probabl instanc incorrectli assign left subnod v estim p il likewis probabl correctli assign right subnod p cr l mean class valu instanc l r correspond valu r estim p r model form x class valu a b chosen make curv pass point m l p il cr shown figur a desir effect approxim sharp step function v good predictor p il p cr decis unimport l r howev predict unreliablethat is p il significantli greater p cr significantli less the decis soften particularli importantthat is l r differ appreci train instanc stochast assign right subnod test surrog split cannot use class use model tree classif v class r cr figur a soft step function model fit train data valu is cours unavail instead instanc propag left right subnod result outcom combin linearli use weight scheme describ quinlan left outcom weight proport train instanc assign left subnod right outcom proport assign right subnod note c successor c quinlan although commerci product test version avail httpwwwrulequestcom see httpwwwcswaikatoacnzml realist evalu standard dataset imper miss valu accom modat remov instanc miss valu half dataset lower part tabl would instanc usabl follow holt g variant glass dataset class combin class delet horsecol dataset attribut delet attribut use class also delet identifi attribut dataset r classif regress tree probabilist theori pattern recognit solv multiclass learn problem via errorcorrect output code veri simpl classif rule perform well commonli use dataset uci repositori machin learn databas httpwww learn continu class retrofit decis tree classifi use kernel densiti estim ca morgan kaufmann kernel regress tree induct model tree predict continu class tr c program machin learn simpl classif rule perform well commonli use dataset ctr niel landwehr mark hall eib frank logist model tree machin learn v n p may donato malerba floriana esposito michelangelo ceci annalisa appic topdown induct model tree regress split node ieee transact pattern analysi machin intellig v n p may rudi setiono feedforward neural network construct use cross valid neural comput v n p decemb v zorkadi d a karra m panayot effici inform theoret strategi classifi combin featur extract perform evalu improv fals posit fals neg spam email filter neural network v n p june ronni kohavi j ross quinlan data mine task method classif decisiontre discoveri handbook data mine knowledg discoveri oxford univers press inc new york ny duncan pott increment learn linear model tree proceed twentyfirst intern confer machin learn p juli banff alberta canada duncan pott claud sammut increment learn linear model tree machin learn v n p novemb saso deroski bernard enko combin classifi stack better select best one machin learn v n p march eib frank leonard trigg geoffrey holm ian h witten technic note naiv bay regress machin learn v n p oct joo gama function tree machin learn v n p june d p solomatin m b siek modular learn model forecast natur phenomena neural network v n p march m t musavi h ressom s srirangam p natarajan r w virnstein l j morri w tweedal neural networkbas light attenu model monitor seagrass popul indian river lagoon journal intellig inform system v n p august