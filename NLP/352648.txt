t comparison predict accuraci complex train time thirtythre old new classif algorithm a twentytwo decis tree nine statist two neural network algorithm compar thirtytwo dataset term classif accuraci train time in case tree number leav classif accuraci measur mean error rate mean rank error rate criteria place statist splinebas algorithm call polyclsss top although statist significantli differ twenti algorithm anoth statist algorithm logist regress second respect two accuraci criteria accur decis tree algorithm quest linear split rank fourth fifth respect although splinebas statist algorithm tend good accuraci also requir rel long train time polyclass exampl third last term median train time often requir hour train compar second algorithm quest logist regress algorithm substanti faster among decis tree algorithm univari split c indcart quest best combin error rate speed c tend produc tree twice mani leav indcart quest b introduct much current research machin learn statist commun algorithm decis tree classifi often emphasi accuraci algorithm one studi call statlog project michi spiegelhalt taylor compar accuraci sever decis tree algorithm nondecis tree algorithm larg number dataset studi smaller scale includ brodley utgoff brown corrubl pittard curram minger shavlik mooney towel recent comprehens tree structur receiv attent comprehens typic decreas increas tree size complex two tree employ kind test predict accuraci one fewer leav usual prefer breslow aha survey method tree simplif improv comprehens third criterion larg ignor rel train time algorithm statlog project find algorithm uniformli accur dataset studi instead mani algorithm possess compar accuraci algorithm excess train time may undesir hand purpos paper extend result statlog project follow way addit classif accuraci size tree compar train time algorithm although train time depend somewhat impl mentat turn larg differ time second versu day differ cannot attribut implement alon includ decis tree algorithm includ statlog project name splu tree clark pregibon holt maass oc murthi kasif salzberg lmdt brodley utgoff quest loh shih also includ sever newest splinebas statist algorithm classif accuraci may use benchmark comparison algorithm futur studi effect ad independ nois attribut classif accuraci where appropri tree size algorithm turn except possibl three algorithm other adapt nois quit well examin scalabl promis algorithm sampl size increas experi compar twentytwo decis tree algorithm nine classic modern statist algorithm two neural network algorithm mani dataset taken univers california irvin uci repositori machin learn databas merz murphi fourteen dataset reallif domain two artifici construct five dataset use statlog project increas probabl find statist signific differ algorithm number dataset doubl addit nois attribut result total number dataset thirtytwo section briefli describ algorithm section give background dataset section explain experiment setup use studi section analyz result issu scalabl studi section conclus recommend given section algorithm short descript algorithm given detail may found cite refer algorithm requir class prior probabl made proport train sampl size tree rule cart use version cart breiman friedman olshen stone implement cart style ind packag buntin caru ana gini index divers split criterion tree base se se prune rule denot ic ic respec tive softwar obtain httpicwwwarcnasagovicprojectsbayesgroupindindprogramhtml splu tree variant cart algorithm written languag becker chamber wilk describ clark pregibon employ devianc split criterion best tree chosen tenfold crossvalid prune perform ptree function treefix librari venabl ripley statlib archiv httplibstatcmuedu se se tree denot st st respect c use releas quinlan quinlan default set includ prune httpwwwcseunsweduauquinlan tree construct c rule induct program use produc set rule tree denot ct rule cr fact fast classif tree algorithm describ loh vanichs takul employ statist test select attribut split node use discrimin analysi find split point size tree determin set stop rule tree base univari split split singl attribut denot ftu base linear combin split split linear function attribut denot ftl fortran program obtain httpwwwstatwisceduloh quest new classif tree algorithm describ loh shih quest use univari linear combin split uniqu featur attribut select method neglig bia attribut uninform respect class attribut approxim chanc select split node tenfold crossvalid use prune tree univari se se tree denot qu qu respect correspond tree linear combin split denot ql ql respect result paper base version program softwar obtain httpwwwstatwiscedulohquesthtml ind due buntin use version default set ind come sever standard predefin style compar four bayesian style paper bay bay opt mml mml opt denot ib ibo im imo respect opt method extend nonopt method grow sever differ tree store compact graph struc ture although time memori intens opt style increas classif accuraci oc algorithm describ murthi et al use version httpwwwcsjhuedusalzbergannounceochtml compar three style first one denot ocm default use mixtur univari linear combin split second one option a denot ocu use univari split third one option o denot ocl use linear combin split option kept default valu lmdt algorithm describ brodley utgoff construct decis tree base multivari test linear combin attribut tree denot lmt use default valu softwar httpyakeecnpurdueedubrodleysoftwarelmdthtml cal fraunhof societi institut inform data process germani muller wysotzki muller wysotzki use version cal design specif numericalvalu attribut howev procedur handl categor attribut mix attribut numer categor includ studi optim two paramet control tree construct predefin threshold signific level ff randomli split train set two part stratifi class twothird use construct tree onethird use valid set choos optim paramet configura tion employ cshell program come cal packag choos best paramet vari ff step best combin valu minim error rate valid set chosen tree construct record train set use chosen paramet valu denot cal t onelevel decis tree classifi exampl basi one split singl attribut holt split categor attribut b categori produc b reserv miss attribut valu hand split continu attribut yield j leav j number class one leaf reserv miss valu softwar obtain httpwwwcsiuottawacaholtelearningothersiteshtml statist algorithm lda linear discrimin analysi classic statist method model instanc within class normal distribut common covari matrix yield linear discrimin function qda quadrat discrimin analysi also model class distribut normal estim covari matrix correspond sampl covari matrix result discrimin function quadrat detail lda qda found mani statist textbook eg johnson use sa proc discrim sa institut inc implement lda qda default set nn sa proc discrim implement nearest neighbor method pool covari matrix use comput mahalanobi distanc log logist discrimin analysi result obtain poli tomou logist regress see eg agresti fortran routin written first author httpwwwstatwiscedulimtlogdiscr fda flexibl discrimin analysi hasti tibshirani buja gener linear discrimin analysi cast classif problem one involv regress mar friedman nonparametr regress procedur studi here use splu function fda mda librari statlib archiv two model use addit model degre denot fm model contain firstord interact degre penalti denot fm pda form penal lda hasti buja tibshirani design situat mani highli correl attribut classif problem cast penal regress framework via optim score pda implement splu use function fda methodgenridg mda stand mixtur discrimin analysi hasti tibshirani fit gaussian mixtur densiti function class produc classifi mda implement splu use librari mda pol polyclass algorithm kooperberg bose stone fit polytom logist regress model use linear spline tensor product provid estim condit class probabl use predict class label pol implement splu use function polyfit polyclass librari statlib archiv model select done tenfold crossvalid neural network lvq use learn vector quantiz algorithm splu class librari venabl ripley statlib archiv detail algorithm may found kohonen ten percent train set use initi algorithm use function lvqinit train carri optim learn rate function olvq fast robust lvq algorithm addit finetun learn perform function lvq number iter ten time size train set olvq lvq use default valu ff learn rate paramet olvq lvq respect rbf radial basi function network implement sa tnnsa macro sarl feedforward neural network httpwwwsascom network architectur specifi archrbf argument studi construct network one hidden layer number hidden unit chosen total number input output unit hidden unit dna dna dataset hidden unit tae tae dataset memori storag limit although macro perform model select choos optim number hidden unit util capabl would taken long dataset see tabl below therefor result report algorithm regard lower bound perform hidden layer fulli connect input output layer direct connect input output layer output layer class repres one unit take valu particular categori otherwis except last one refer categori avoid local optima ten preliminari train conduct best estim use subsequ train detail radial basi function network found bishop ripley dataset briefli describ sixteen dataset use studi well modif made experi fourteen real domain two artifici creat thirteen uci breast cancer bcw one breast cancer databas uci collect univers wisconsin w h wolberg problem predict whether tissu sampl taken patient breast malign benign two class nine numer attribut observ sixteen instanc contain singl miss attribut valu remov analysi result therefor base record error rate estim use tenfold crossvalid decis tree analysi subset data use fact algorithm report wolberg tanner loh wolberg tanner loh wolberg tanner loh dataset also analyz linear program method mangasarian wolberg contracept method choic cmc data taken nation indonesia contracept preval survey sampl marri women either pregnant know pregnant time interview problem predict current contracept method choic no use longterm method shortterm method woman base demograph socioeconom characterist lerman molyneaux pangemanan iswarati three class two numer attribut seven categor attribut record error rate estim use tenfold crossvalid data obtain httpwwwstatwiscedupstatftppublohtreeprogsdataset statlog dna dna uci dataset molecular biolog use project splice junction point dna sequenc su perfluou dna remov process protein creation higher organ problem recogn given sequenc dna boundari exon the part dna sequenc retain splice intron the part dna sequenc splice out three class sixti categor attribut four categori sixti categor attribut repres window sixti nucleotid one four categori middl point window classifi one exonintron boundari intronexon boundari neither these exampl databas divid randomli train set size test set size error rate estim test set statlog heart diseas hea uci dataset cleveland clinic foundat courtesi r detrano problem concern predict presenc absenc heart diseas given result variou medic test carri patient two class seven numer attribut six categor attribut record statlog project employ unequ misclassif cost use equal cost algorithm allow unequ cost error rate estim use tenfold crossvalid boston hous bo uci dataset give hous valu boston suburb harrison rubinfeld three class twelv numer attribut one binari attribut record follow loh vanichs takul class creat attribut median valu owneroccupi home follow class otherwis error rate estim use tenfold crossvalid led display led artifici domain describ breiman et al contain seven boolean attribut repres seven lightemit diod ten class set decim digit attribut valu either zero one accord whether correspond light digit attribut valu ten percent probabl valu invert class attribut integ zero nine inclus c program uci use gener record train set record test set error rate estim test set bupa liver disord bld uci dataset contribut r s forsyth problem predict whether male patient liver disord base blood test alcohol consumpt two class six numer attribut record error rate estim use tenfold crossvalid pima indian diabet pid uci dataset contribut v sigillito patient dataset femal least twentyon year old pima indian heritag live near phoenix arizona usa problem predict whether patient would test posit diabet given number physiolog measur medic test result two class seven numer attribut record origin dataset consist record eight numer attribut howev mani attribut notabl serum in sulin contain zero valu physic imposs remov serum insulin record imposs valu attribut error rate estim use tenfold cross valid statlog satellit imag sat uci dataset give multispectr valu pixel within theta neighborhood satellit imag classif associ central pixel neighborhood aim predict classif given multispectr valu six class thirtysix numer attribut train set consist record test set consist record error rate estim test set imag segment seg uci dataset use statlog project sampl databas seven outdoor imag imag handseg creat classif everi pixel one brickfac sky foliag cement window path grass seven class nineteen numer attribut record dataset error rate estim use tenfold crossvalid algorithm could handl dataset without modif program requir larg amount memori therefor but algorithm discret attribut except attribut one hundr categori attitud toward smoke restrict smo survey dataset bull obtain httplibstatcmuedudatasetscsb problem predict attitud toward restrict smoke workplac prohibit restrict unrestrict base bylawrel smokingrel sociodemograph covari three class three numer attribut five categor attribut divid origin dataset train set size test set size error rate estim test set thyroid diseas thi uci anntraindatacontribut r werner problem determin whether patient hyperthyroid three class normal hyperfunct subnorm function six numer attribut fifteen binari attribut train set consist record test set record error rate estim test set statlog vehicl silhouett veh uci dataset origin ture institut glasgow scotland problem classifi given silhouett one four type vehicl use set featur extract silhouett vehicl view mani angl four model vehicl doubl decker bu chevrolet van saab opel manta four class eighteen numer attribut record error rate estim use tenfold crossvalid congression vote record vot uci dataset give vote member u s hous repres th congress sixteen issu problem classifi congressman democrat republican base sixteen vote two class sixteen categor attribut three categori yea nay neither record rate estim tenfold crossvalid waveform wav artifici threeclass problem base three wave form class consist random convex combin two waveform sampl integ nois ad descript gener data given breiman et al c program avail uci twentyon numer attribut record train set error rate estim independ test set record evalu tae data consist evalu teach perform three regular semest two summer semest teach assist ta assign statist depart univers wisconsinmadison score group three roughli equals categori low medium high form class attribut predictor attribut i whether nativ english speaker bi nari ii cours instructor categori iii cours categori iv summer regular semest binari v class size numer dataset first report loh shih differ dataset two categor attribut larg number categori result decis tree algorithm cart employ exhaust search usual take much longer train algorithm cart evalu split categor attribut c valu error rate estim use tenfold crossvalid data obtain httpwwwstatwiscedupstatftppublohtreeprogsdataset summari attribut featur dataset given tabl tabl characterist dataset last three column give number type ad nois attribut dataset notat n denot standard normal distribut uimn denot uniform distribut integ n inclus u denot uniform distribut unit interv train no origin attribut no type nois attribut data sampl no num categor total numer categor total set size class dna led bld pid smo thi veh vot tae experiment setup algorithm design categor attribut case categor attribut convert vector attribut is categor attribut x take k valu fc g replac dimension vector d otherwis vector consist zero affect algorithm statist neural network algorithm well tree algorithm ftl ocu ocl ocm lmt order increas number dataset studi effect nois attribut algorithm creat sixteen new dataset ad independ nois attribut number type nois attribut ad given right panel tabl name new dataset origin dataset except addit symbol exampl bcw dataset nois ad denot bcw dataset use one two differ way estim error rate algorithm larg dataset size much larger test set size least use test set estim error rate classifi construct use record train set test test set twelv thirtytwo dataset analyz way remain twenti dataset use follow tenfold crossvalid procedur estim error rate dataset randomli divid ten disjoint subset contain approxim number record sampl stratifi class label ensur subset class proport roughli whole dataset subset classifi construct use record it classifi test withheld subset obtain crossvalid estim error rate ten crossvalid estim averag provid estim classifi construct data algorithm implement differ program languag languag avail platform three type unix workstat use studi workstat type implement languag algorithm given tabl rel perform workstat accord spec mark given tabl float point spec mark show task take one second dec would take second sparcstat ss sparcstat ss respect therefor enabl comparison train time report term equival secondsth train time record ss ss divid respect result error rate train time algorithm given separ tabl dataset appendix tabl also report error rate naiv plural rule ignor inform covari classifi everi record major class train sampl exploratori analysi error rate present formal statist analysi result help studi summari tabl mean error rate algorithm dataset given second row minimum maximum error rate tabl hardwar softwar platform algorithm workstat dec alpha model dec sun sparcstat model ss sun sparcstat ss algorithm platform algorithm platform tree rule st splu tree se dec qu quest univari se decf lmt lmdt linear decc qu quest univari se decf cal cal ssc ql quest linear se decf singl split decc ql quest linear se decf ftu fact univari decf statist linear decf lda linear discrimin anal decsa ct c tree decc qda quadrat discrimin anal decsa cr c rule decc nn nearestneighbor decsa ib ind bay style ssc log linear logist regress decf ibo ind bay opt style ssc fm fda degre sss im ind mml style ssc fm fda degre sss imo ind mml opt style ssc pda penal lda sss ic ind cart se ssc mda mixtur discrimin anal sss ic ind cart se ssc pol polyclass sss ocu oc univari ssc ocl oc linear ssc neural network ocm oc mix ssc lvq learn vector quantiz sss st splu tree se dec rbf radial basi function network decsa tabl spec benchmark summari workstat specfp specint sourc dec dec model spec newslett mhz vol issu june ss sun sparcstat spec newslett model mhz vol issu june ss sun sparcstat spec newslett mhz vol issu june plural rule given dataset last three column let p denot smallest observ error rate row ie dataset algorithm error rate within one standard error p consid close best indic p tabl standard error estim follow p independ test set let n denot size test set otherwis p crossvalid estim let n denot size train set standard error p estim formula pn algorithm largest error rate within row indic x total number p xmark algorithm given third fourth row tabl follow conclus may drawn tabl tabl minimum maximum naiv plural rule error rate dataset p mark indic algorithm error rate within one standard error minimum dataset xmark indic algorithm worst error rate dataset mean error rate algorithm given second row decis tree rule statist algorithm net error rate naiv mean bcw cmc pp pp dna dna hea bo led bld pid seg smo thi ppp ppp pp ppp pp vot tae x tae pp algorithm pol lowest mean error rate order algorithm term mean error rate given upper half tabl algorithm also rank term total number p xmark criterion accur algorithm pol fifteen p tabl order algorithm mean error rate mean rank error rate mean pol log mda ql lda ql pda ic fm ibo imo error rate cr im lmt ct qu qu ocu ic ib ocm st mean pol fm log fm ql lda qu cr imo mda pda rank ct ql ibo im ic ftl qu ocu ic st st error rate lmt ocm ib rbf ftu qda lvq ocl cal nn mark xmark eleven algorithm one xmark rank increas order number xmark in parenthes are ftl ocm st fm mda fm ocl qda nn lvq t exclud these remain algorithm rank order decreas number p mark in parenthes as pol log ql lda pda ql ocu qu qu cr ibo rbf ct imo im ic st ftu ic cal ib lmt top four algorithm also rank among top five upper half tabl last three column tabl show algorithm sometim less accur plural rule nn at cmc cmc smo bld qda smo thi thi ftl tae st tae easiest dataset classifi bcw bcw vot vot error rate lie difficult classifi cmc cmc tae minimum error rate greater two difficult dataset smo smo case smo margin lower error rate plural rule algorithm lower error rate plural rule smo dataset largest rang error rate thi thi rate rang howev maximum due qda qda ignor maximum error rate drop six dataset one p mark each bld pol sat lvq sat fm seg ibo veh veh qda time overal addit nois attribut appear increas significantli error rate algorithm statist signific error rate analysi varianc statist procedur call mix effect analysi varianc use test simultan statist signific differ mean error rate algorithm control differ dataset neter wasserman kutner p although make assumpt effect dataset act like random sampl normal distribut quit robust violat assumpt data procedur give signific probabl less gamma henc hypothesi mean error rate equal strongli reject simultan confid interv differ mean error rate obtain use tukey method miller p accord procedur differ mean error rate two algorithm statist signific level differ visual result figur a plot mean error rate algorithm versu median train time second solid vertic line plot unit right mean error rate pol therefor algorithm lie left line mean error rate statist significantli differ pol algorithm seen form four cluster respect train time cluster roughli delin three horizont dot line correspond train time one minut ten minut one hour figur b show magnifi plot eighteen algorithm median train time less ten minut mean error rate statist significantli differ pol analysi rank avoid normal assumpt instead analyz rank algorithm within dataset is dataset algorithm lowest error rate assign rank one second lowest rank two etc averag rank assign case tie lower half tabl give order algorithm term mean rank error rate pol first last note howev mean rank pol show far uniformli accur across dataset compar two method order tabl seen pol log ql lda algorithm consist good perform three algorithm perform well one criterion mda fm fm case mda low mean error rate due excel perform four dataset veh veh wav wav mani algorithm poorli domain concern shape identif dataset contain numer mean error rate median sec ftu cr ib ibo im imo ocu ocl ocm st st lda qda pda mda pol rbf hr min min a thirtythre method mean error rate median sec ftu cr ib im ocu lda pda mda min min b less min accuraci sig differ pol figur plot median train time versu mean error rate vertic axi logscal solid vertic line plot a divid algorithm two group mean error rate algorithm left group differ significantli at simultan signific level pol minimum mean error rate plot b show algorithm statist significantli differ pol term mean error rate median train time less ten minut attribut mda gener unspectacular rest dataset reason tenth place rank term mean rank situat fm fm quit differ low mean rank indic fm usual good perform howev fail miser seg seg dataset report error rate fifti percent algorithm error rate less ten percent thu fm seem less robust algorithm fm also appear lack robust although lesser extent worst perform bo dataset error rate fortytwo percent compar less thirtyf percent algorithm number xmark algorithm tabl good predictor errat poor perform mda fm fm least one xmark friedman test standard procedur test statist signific differ mean rank experi give signific probabl less gamma therefor null hypothesi algorithm equal accur averag reject further differ mean rank greater statist signific level holland wolf p thu pol statist significantli differ twenti algorithm mean rank less equal figur a show plot median train time versu mean rank algorithm algorithm lie left vertic line statist significantli differ pol magnifi plot subset algorithm significantli differ pol median train time less ten minut given figur b algorithm differ statist significantli pol term mean error rate form subset differ pol term mean rank thu rank test appear power analysi varianc test experi fifteen algorithm figur b may recommend use applic good accuraci short train time desir train time tabl give median dec equival train time algorithm rel train time within dataset owe larg rang train time order rel fastest algorithm dataset report fastest algorithm indic algorithm xgamma time slow indic valu x exampl case dna dataset fastest algorithm ct t requir two second slowest algorithm fm take three million second almost forti day henc time slow last two column tabl give fastest slowest time dataset tabl give order algorithm fastest slowest accord median train time overal fastest algorithm ct follow close ftu ftl lda two reason superior speed ct compar decis tree algorithm first split categor attribut mean rank median sec ftu cr ib ibo im imo ocu ocl ocm lda qda pda mda pol rbf hr min min a thirtythre method mean rank median sec cr im ocu lda pda mda min min b less min accuraci sig differ pol figur plot median train time versu mean rank error rate vertic axi logscal solid vertic line plot a divid algorithm two group mean rank algorithm left group differ significantli at simultan signific level pol plot b show algorithm statist significantli differ pol term mean rank median train time less ten minut tabl dec equival train time rel time algorithm second third row give median train time rank algorithm entri x subsequ row indic algorithm time slower fastest algorithm dataset fastest algorithm denot entri minimum maximum train time given last two column s m h d denot second minut hour day respect decis tree rule statist algorithm net cpu time median cpu m m m m m m m m m m m h m h h m h rank tabl order algorithm median train time m m m m m m ocm m m m m m m m h h h h mani subnod number categori therefor wast time form subset categori second prune method requir crossvalid increas train time sever fold classic statist algorithm qda nn also quit fast expect decis tree algorithm employ univari split faster use linear combin split slowest algorithm pol fm rbf two splinebas one neural network although ic ic st st claim implement cart algorithm ind version faster splu version one reason ic ic written c wherea st st written languag anoth reason ind version use heurist buntin person commun instead greedi search number categori categor attribut larg appar tae dataset categor attribut twentysix categori case ic ic take around forti second versu two half hour st st result tabl indic ind classif accuraci advers affect heurist see aroni provost anoth possibl heurist sinc onelevel tree may appear surpris faster algorithm ct produc multilevel tree reason split continu attribut j number class hand ct alway split continu attribut two interv onli therefor j spend lot time search interv size tree tabl give number leav tree algorithm dataset nois attribut ad case error rate obtain tenfold cross valid entri mean number leav ten crossvalid tree tabl show much number leav chang addit nois attribut mean median number leav classifi given last column two tabl ibo imo clearli yield largest tree far apart t necessarili short design algorithm shortest tree averag ql follow close ftl ocl rank algorithm univari split in increas median number leav is t ic st qu ftu ic st ocu qu ct algorithm ct tend produc tree mani leav algorithm one reason may due underprun although error rate quit good anoth that unlik binarytre algorithm ct split categor attribut mani node number categori addit nois attribut typic decreas size tree except ct cal tend grow larger tree imo seem fluctuat rather wildli result complement oat jensen look effect sampl size number leav decis tree algorithm found signific relationship tree size train sampl size ct observ tree algorithm employ costcomplex prune better abl control tree growth scalabl algorithm although differ mean error rate pol mani algorithm statist signific clear error rate sole criterion pol would method choic unfortun pol one computeintens algorithm see train time increas sampl size small scalabl studi carri algorithm qu ql ftl ct cr ic log fm pol train time measur algorithm train set size four dataset use gener samplessat smo tae new larg uci dataset call adult two class six continu seven categor attribut sinc first three dataset larg enough experi bootstrap resampl employ gener train set is n sampl randomli drawn replac dataset avoid get mani replic record valu class attribut sampl case randomli chang anoth valu probabl the new valu select pool altern equal probabl bootstrap sampl carri adult dataset record instead nest train set obtain random sampl without replac time requir train algorithm plot in loglog scale figur except pol fm log logarithm train time seem increas linearli logn nonmonoton behavior pol fm puzzl might due random use crossvalid model select errat behavior log adult dataset caus converg problem model fit mani line figur roughli parallel suggest rel comput speed algorithm fairli constant rang sampl size consid ql cr two except cohen observ cr scale well conclus result show mean error rate mani algorithm suffici similar differ statist insignific differ also probabl insignific practic term exampl mean error rate top rank algorithm pol log ql differ less small differ import real applic user may wish select algorithm base criteria train time interpret classifi unlik error rate huge differ train time algorithm pol algorithm lowest mean error rate take fifti time long train next accur algorithm ratio time roughli equival hour versu minut figur show maintain wide rang sampl size larg applic time factor may advantag use one quicker algorithm interest old statist algorithm lda mean error rate close best surpris i design binaryvalu attribut all categor attribut transform vector prior applic lda ii expect effect class densiti cpu time cpu time tae cpu time adult cpu time figur plot train time versu sampl size loglog scale select algorithm multimod fast easi implement readili avail statist packag provid conveni benchmark comparison futur algorithm low error rate log lda probabl account much perform better algorithm exampl pol basic modern version log enhanc flexibl log employ splinebas function automat model select although strategi comput costli produc slight reduct mean error rateenough bring top pack good perform ql may similarli attribut lda quest linearsplit algorithm design overcom difficulti encount lda multimod situat appli modifi form lda partit data partit repres leaf decis tree strategi alon howev enough higher mean error rate ftl show latter base fact algorithm precursor quest one major differ quest fact algorithm former employ costcomplex prune method cart wherea latter not result suggest form bottomup prune may essenti low error rate purpos construct algorithm data interpret perhap decis rule tree univari split suffic except cal t differ mean error rate decis rule tree algorithm statist signific pol ic lowest mean error rate qu best term mean rank cr ct far behind four algorithm provid good classif accuraci ct fastest far although tend yield tree twice mani leav ic qu cr next fastest figur show scale well ic slightli faster tree slightli fewer leav qu howev loh shih show cartbas algorithm ic prone produc spuriou split situat acknowledg indebt p auer c e brodley w buntin t hasti r c holt c kooperberg s k murthi j r quinlan w sarl b schulmeist w taylor help advic instal comput program also grate j w molyneaux provid nation indonesia contracept preval survey data final thank w cohen f provost review mani help comment suggest r categor data analysi increas effici data mine algorithm breadthfirst marker propag new languag neural network pattern recognit classif regress tree simplifi decis tree survey multivari versu univari decis tree multivari decis tree comparison decis tree classifi backpropag neural network multimod classif problem analysi attitud toward workplac smoke restrict learn classif tree introduct ind version recurs partit fast effect rule induct neural network multivari adapt regress spline with discuss use rank avoid assumpt normal implicit analysi varianc construct assess classif rule hedon price demand clean air discrimin analysi gaussian mixtur penal discrimin analysi flexibl discrimin analysi optim score nonparametr statist method simpl classif rule perform well commonli use dataset appli multivari statist analysi polychotom regress cancer diagnosi via linear program uci repositori machin learn databas machin learn automat construct decis tree classif decisiontre algorithm cal base statist approach split algorithm system induct obliqu decis tree appli linear statist model effect train set size decis tree complex improv use continu attribut c pattern recognit neural network neural network statist model sa institut symbol neural learn algorithm empir comparison modern appli statist splu diagnost scheme fine needl aspir breast mass fine needl aspir breast mass diagnosi statist approach fine needl aspir diagnosi breast mass tr appli multivari statist analysi symbol neural learn algorithm c program machin learn simpl classif rule perform well commonli use dataset multivari decis tree selforgan map saset user guid version neural network pattern recognit effect train set size decis tree complex multivari versu univari decis tree simplifi decis tree survey ctr ganesan velayathan seiji yamada behaviorbas web page evalu proceed th intern confer world wide web may edinburgh scotland ganesan velayathan seiji yamada behaviorbas web page evalu proceed ieeewicacm intern confer web intellig intellig agent technolog p decemb samuel e buttrey ciril karo use knearestneighbor classif leav tree comput statist data analysi v n p juli kwekumuata oseibryson evalu decis tree multicriteria approach comput oper research v n p septemb richi nayak lauri buy jan loviekitchin data mine conceptualis activ age proceed fifth australasian confer data mine analyst p novemb sydney australia xiangyang li nong ye supervis cluster algorithm comput intrus detect knowledg inform system v n p novemb laura elena raileanu kilian stoffel theoret comparison gini index inform gain criteria annal mathemat artifici intellig v n p may nong ye xiangyang li scalabl increment learn algorithm classif problem comput industri engin v n p septemb jonathan eckstein peter l hammer ying liu mikhail nediak bruno simeon maximum box problem applic data analysi comput optim applic v n p decemb sattar hashemi mohammad r kangavari parallel learn use decis tree novel approach proceed th wsea intern confer appli mathemat comput scienc p april rio de janeiro brazil khale m s badran peter i rockett role divers preserv mutat prevent popul collaps multiobject genet program proceed th annual confer genet evolutionari comput juli london england nigel william sebastian zander grenvil armitag preliminari perform comparison five machin learn algorithm practic ip traffic flow classif acm sigcomm comput commun review v n octob sorin alex peter l hammer acceler algorithm pattern detect logic analysi data discret appli mathemat v n p may s ruggieri effici c ieee transact knowledg data engin v n p march md zahidul islam ljiljana brankov framework privaci preserv classif data mine proceed second workshop australasian inform secur data mine web intellig softwar internationalis p januari dunedin new zealand efstathio stamatato gerhard widmer automat identif music perform learn ensembl artifici intellig v n p june niel landwehr mark hall eib frank logist model tree machin learn v n p may karann toh quoclong tran dipti srinivasan benchmark reduc multivari polynomi pattern classifi ieee transact pattern analysi machin intellig v n p june abraham bernstein foster provost shawndra hill toward intellig assist data mine process ontologybas approach costsensit classif ieee transact knowledg data engin v n p april rich caruana alexandru niculescumizil empir comparison supervis learn algorithm proceed rd intern confer machin learn p june pittsburgh pennsylvania nicola baskioti michl sebag c compet map phase transitioninspir approach proceed twentyfirst intern confer machin learn p juli banff alberta canada gabriela alex peter l hammer span pattern logic analysi data discret appli mathemat v n p may ingolf geist framework data mine kdd proceed acm symposium appli comput march madrid spain anthoni j t lee yaot wang effici data mine call path pattern gsm network inform system v n p decemb kwekumuata oseibryson postprun decis tree induct use multipl perform measur comput oper research v n p novemb friedhelm schwenker han a kestler gther palm unsupervis supervis learn radialbasisfunct network selforgan neural network recent advanc applic springerverlag new york inc new york ny chenfu chien wenchih wang jenchieh cheng data mine yield enhanc semiconductor manufactur empir studi expert system applic intern journal v n p juli irma becerrafernandez stelio h zanaki steven walczak knowledg discoveri techniqu predict countri invest risk comput industri engin v n p septemb zhiwei fu bruce l golden shreevardhan lele s raghavan edward wasil diversif better classif tree comput oper research v n p novemb rueyshiang guh hybrid learningbas model onlin detect analysi control chart pattern comput industri engin v n p august krzysztof krawiec genet programmingbas construct featur machin learn knowledg discoveri task genet program evolv machin v n p decemb huimin zhao sudha ram combin schema instanc inform integr heterogen data sourc data knowledg engin v n p may karann toh train reciprocalsigmoid classifi featur scalingspac machin learn v n p octob o asparoukhov w j krzanowski nonparametr smooth locat model mix variabl discrimin statist comput v n p octob r chandrasekaran young u ryu varghes s jacob sungchul hong isoton separ inform journal comput v n p octob chingpao chang chihp chu defect prevent softwar process actionbas approach journal system softwar v n p april toni van gestel johan a k suyken bart baesen stijn viaen jan vanthienen guido deden bart de moor joo vandewal benchmark least squar support vector machin classifi machin learn v n p januari elena barali silvia chiusano essenti classif rule set acm transact databas system tod v n p decemb siddharth pal david j miller extens iter scale decis data aggreg ensembl classif journal vlsi signal process system v n p august man cheang kwong sak leung kin hong lee genet parallel program design implement evolutionari comput v n p june foster provost pedro domingo tree induct probabilitybas rank machin learn v n p septemb johann gehrk wieyin loh raghu ramakrishnan classif regress money can grow tree tutori note fifth acm sigkdd intern confer knowledg discoveri data mine p august san diego california unit state vasant dhar dashin chou foster provost discov interest pattern invest decis make glower xcirca genet learner overlaid entropi reduct data mine knowledg discoveri v n p octob perlich foster provost jeffrey s simonoff tree induct vs logist regress learningcurv analysi journal machin learn research p foster provost venkateswarlu kolluri data mine task method scalabl handbook data mine knowledg discoveri oxford univers press inc new york ny gabriela alex sorin alex tibriu o bonat alexand kogan logic analysi data vision peter l hammer annal mathemat artifici intellig v n p april krzysztof j cio lukasz a kurgan clip hybrid induct machin learn algorithm gener inequ rule inform scienc intern journal v n p june foster provost venkateswarlu kolluri survey method scale induct algorithm data mine knowledg discoveri v n p june s b kotsianti i d zaharaki p e pintela machin learn review classif combin techniqu artifici intellig review v n p novemb