t logist regress adaboost bregman distanc a give unifi account boost logist regress learn problem cast term optim bregman distanc strike similar two problem framework allow us design analyz algorithm simultan easili adapt algorithm design one problem other problem give new algorithm explain potenti advantag exist method algorithm iter divid two type base whether paramet updat sequenti one time parallel all onc also describ parameter famili algorithm includ sequenti parallelupd algorithm special case thu show sequenti parallel approach unifi algorithm give converg proof use gener formal auxiliaryfunct proof techniqu one sequentialupd algorithm equival adaboost provid first gener proof converg adaboost show algorithm gener easili multiclass case contrast new algorithm iter scale algorithm conclud experiment result synthet data highlight behavior old newli propos algorithm differ set b introduct give unifi account boost logist regress show learn problem cast term optim bregman distanc frame work two problem becom extrem similar real differ choic bregman distanc unnorm rel entropi boost binari rel entropi logist regress fact two problem similar framework allow us design analyz algorithm si multan abl borrow method maximumentropi literatur logist regress appli exponenti loss use adaboost especi convergenceproof techniqu convers easili adapt boost method problem minim logist loss use logist regress result famili new algorithm problem togeth converg proof new algorithm well adaboost adaboost logist regress attempt choos paramet weight associ given famili function call featur weak hypothes adaboost work sequenti updat paramet one one wherea method logist regressionmost notabl iter scale iter updat paramet parallel iter first new algorithm method optim exponenti loss use parallel updat seem plausibl parallelupd method often converg faster sequentialupd method provid number featur larg make parallel updat infeas preliminari experi suggest case second algorithm parallelupd method logist loss although parallelupd algorithm well known function updat deriv new preliminari experi indic new updat may also much faster unifi treatment give exponenti logist loss function abl present prove converg algorithm two loss simultan true algorithm present paper well next describ analyz sequentialupd algorithm two loss function exponenti loss algorithm equival adaboost algorithm freund schapir view algorithm frame work abl prove adaboost correctli converg minimum exponenti loss function new result although kivinen warmuth mason et al given converg proof adaboost proof depend assumpt given minim problem may hold case proof hold gener without assumpt unifi view lead instantli sequentialupd algorithm logist regress minor modif adaboost similar one propos duffi helmbold like adaboost algorithm use conjunct classif algorithm usual call weak learn algorithm accept distribut exampl return weak hypothesi low error rate respect distribut how ever new algorithm provabl minim logist loss rather arguabl less natur exponenti loss use adaboost anoth potenti import advantag new algorithm weight place exampl bound suggest may possibl use new algorithm set boost algorithm select exampl present weak learn algorithm filter stream exampl such larg dataset point watanab domingo watanab possibl adaboost sinc weight may becom extrem larg provid modif adaboost purpos weight truncat new algorithm may viabl cleaner altern next describ parameter famili iter algorithm includ parallel sequentialupd algorithm also interpol smoothli two extrem converg proof give hold entir famili algorithm although paper consid binari case two possibl label associ exampl turn multiclass case requir addit work is algorithm converg proof give binari case turn directli applic multiclass case without modif comparisonw also describ gener iter scale algorithm darroch ratcliff rederiv procedur set abl relax one main assumpt usual requir algorithm paper organ follow section describ boost logist regress model usual formul section give background optim use bregman distanc section describ boost logist regress cast within framework section give parallelupd algorithm proof converg section give sequentialupd algorithm converg proof parameter famili iter algorithm describ section extens multiclass problem given section section contrast method iter scale section give preliminari experi previou work variant sequentialupd algorithm fit gener famili arc algorithm present breiman well mason et al anyboost famili algorithm informationgeometr view take also show algorithm studi includ adaboost fit famili algorithm describ bregman satisfi set constraint work base directli gener set laf ferti della pietra della pietra one attempt solv optim problem base gener bregman distanc gave method deriv analyz parallelupd algorithm set use auxilliari function algorithm converg proof base method work build sever previou paper compar boost approach logist regress fri man hasti tibshirani first note similar boost logist regress loss function deriv sequentialupd algorithm logitboost logist loss howev unlik algorithm requir weak learner solv leastsquar problem rather classif problem anoth sequentialupd algorithm differ relat problem propos cesabianchi krogh warmuth duffi helmbold gave condit loss function give boost algorithm show minim logist loss lead boost algorithm pac sens suggest algorithm problem close their may turn also pac boost properti lafferti went studi relationship logist regress exponenti loss use famili bregman distanc howev set describ paper appar cannot extend precis includ exponenti loss use bregman distanc describ import differ lead natur treatment exponenti loss new view logist regress work build heavili kivinen warmuth who along lafferti first make connect adaboost inform geometri show updat use adaboost form entropi project howev bregman distanc use differ slightli one chosen normal rel entropi rather unnorm rel entropi adaboost fit model quit complet particular converg proof depend assumpt hold gener kivinen warmuth also describ updat gener bregman distanc includ one exampl bregman distanc use captur logist regress boost logist model loss function set train exampl instanc x belong domain instanc space x label assum also given set realvalu function follow convent maxent literatur call function featur boost literatur would call weak base hypothes studi problem approxim s use linear combin featur is interest problem find vector paramet r n f good approxim measur good approxim vari task mind classif problem natur tri match sign f is attempt minim true otherwis although minim number classif error may worthwhil goal gener form problem intract see instanc therefor often advantag instead minim nonneg loss function instanc boost algorithm adaboost base exponenti loss exp gammay verifi eq upper bound eq howev latter loss much easier work demonstr adaboost briefli seri round adaboost use oracl subroutin call weak learn algorithm pick one featur weak hypothesi h j associ paramet j updat note breiman variou later author step done way approxim caus greatest decreas exponenti loss paper show first time adaboost fact provabl effect method find paramet minim exponenti loss assum weak learner alway choos best h j also give entir new algorithm minim exponenti loss which round paramet updat parallel rather one time hope parallelupd algorithm faster sequentialupd algorithm see section preliminari experi regard instead use f classif rule might instead postul s gener stochast function x s attempt use f x estim probabl associ label y natur wellstudi way pass f logist function is use estim likelihood label occur sampl gammay maxim likelihood equival minim log loss model gammay gener improv iter scale popular parallelupd method minim loss paper give altern parallelupd algorithm compar iter scale techniqu preliminari experi section section give background optim use bregman distanc form unifi basi studi boost logist regress particular setup follow taken primarili lafferti della pietra della pietra r continu differenti strictli convex function defin close convex set bregman distanc associ f defin instanc bf unnorm rel entropi du shown that gener everi bregman distanc nonneg equal zero two argument equal natur optim problem associ bregman distanc name find vector closest given vector q subject set linear constraint constraint specifi theta n matrix vector d vector p satisfi constraint problem find arg min convex dual problem give altern formul here problem find vector particular form closest given vector p form vector defin via legendr transform written simpli v ffi q f clear context use calculu seen equival instanc bf unnorm rel entropi verifi use calculu v eq use note given theta n matrix vector q d consid vector obtain take legendr transform linear combin column vector q is vector set dual optim problem state problem find arg min q closur q remark fact two optim problem solut same and moreov solut turn uniqu point intersect p q take statement theorem lafferti della pietra della pietra result appear due csisz ar topso proof case normal rel entropi given della pietra della pietra lafferti see also csisz ar survey articl theorem let q abov assum bf exist uniqu bf delta q moreov one four properti determin q uniqu theorem extrem use prove converg algorithm describ below show next section boost logist regress view optim problem type given part theorem then prove optim need show algorithm converg point p q regress revisit return boost logist regress problem outlin section show cast form optim problem outlin abov recal boost goal find exp gammay minim or precis minimum attain finit seek procedur find sequenc caus function converg infimum shorthand call exploss problem view problem form given section s s vector follow m final take f eq bf unnorm rel entropi note earlier case v ffi q given eq particular mean furthermor trivial see du du delta equal eq thu minim du equival minim eq theorem equival find satisfi constraint logist regress reduc optim problem form nearli way recal goal find or sequenc s minim shorthand call logloss problem defin exactli exponenti loss vector q still constant defin space restrict minor differ howev import differ choic function f name result bregman distanc trivial choic f verifi use calculu v delta equal eq minim db equival minim eq befor find q q satisfi constraint eq section describ new algorithm exploss logloss problem use iter method weight j updat iter algorithm shown fig algorithm use function f satisfi certain condit describ below particular see use choic f given section thu realli singl algorithm use lossminim problem set paramet appropri note that without loss gener assum section instanc i algorithm simpl iter vector ffi comput shown ad paramet vector assum algorithm input infinitevalu updat never occur algorithm new minim problem optim method exploss notabl adaboost paramet assumpt bf lim ffl ffl updat paramet figur parallelupd optim algorithm gener involv updat one featur time parallel updat method logloss well known see exam ple howev updat take differ form usual updat deriv logist model use point distribut q t simpl function previou distribut q eq give exploss logloss respect prove next algorithm given fig converg optim either loss prove abstractli matrix vector q function f satisfi follow assumpt assumpt v r q d assumpt c set bound show later choic f given section satisfi assumpt allow us prove converg exploss logloss prove convergencew use auxiliaryfunct techniqu della pietra della pietra lafferti roughli idea proof deriv nonneg lower bound call auxiliari function much loss decreas iter sinc loss never increas lower bound zero auxiliari function must converg zero final step show auxiliari function zero constraint defin set p must satisfi therefor theorem must converg optim formal defin auxiliari function sequenc continu function satisfi two condit prove converg specif algorithm prove follow lemma show roughli sequenc auxiliari function sequenc converg optimum point q thu prove converg specif algorithm reduc simpli find auxiliari function auxiliari function matrix m assum q s lie compact subspac q q eq particular case assumpt hold bf lim proof condit bf delta nonincreas sequenc bound zero therefor sequenc differ bf must converg zero condit mean aq must also converg zero assum q s lie compact space sequenc q s must subsequ converg point d continu a a p eq hand q limit sequenc point q theorem argument uniqu q show singl limit point q suppos entir sequenc converg q could find open set b contain q fq contain infinit mani point therefor limit point must close set must differ q thi alreadi argu imposs therefor entir sequenc converg q appli lemma prove converg algorithm fig theorem let f satisfi assumpt assum bf let sequenc gener algorithm fig lim q eq is lim proof let w claim function auxiliari function clearli continu nonposit upper bound chang delta round aq follow eq follow eq assumpt respect eq use fact that x j s jensen inequ appli convex function e x eq use definit w tj w gamma tj eq use choic ffi inde ffi chosen specif minim eq j q is thu auxiliari function theorem follow immedi lemma appli theorem exploss logloss prob lem need verifi assumpt satisfi exploss assumpt hold equal logloss first second equal use eq respec tive final inequ use assumpt hold trivial logloss sinc bound exploss du clearli defin bound subset r sequenti algorithm section describ anoth algorithm minim problem describ section howev unlik algorithm section one present updat weight one featur time parallelupd algorithm may give faster converg mani featur sequentialupd algorithm use larg number featur use oracl select featur updat next instanc adaboost essenti equival sequentialupd algorithm exploss use assum weak learn algorithm select weak hypothesi ie one featur sequenti algorithm present logloss use exactli way algorithm shown fig theorem given assumpt theorem algorithm fig converg optim sens theorem proof theorem use auxiliari function paramet same fig output same fig ae ff els ffl updat paramet figur sequentialupd optim algorithm function clearli continu nonposit eq use convex e gammaff x eq use choic ff as befor chose ff minim bound eq thu auxiliari function theorem follow immedi lemma mention abov algorithm essenti equival adaboost specif version adaboost first present freund schapir adaboost iter distribut train exampl comput weak learner seek weak hypothesi low error respect distribut algorithm present section assum space weak hypothes consist featur h learner alway succe select featur lowest error or accur error farthest translat notat weight i assign exampl adaboost exactli equal q ti z weight error tth weak hypothesi equal theorem first proof adaboost alway converg minimum exponenti loss assum ideal weak learner form abov note theorem also tell us exact form lim howev know limit behavior q know limit behavior paramet whether q also present section new algorithm logist regress fact algorithm one given duffi helmbold except choic ff practic term littl work would requir alter exist learn system base adaboost use logist loss rather exponenti lossth differ manner q comput even system base confidencer boost ff j chosen togeth round minim eq rather approxim express use algorithm fig note proof theorem easili modifi prove converg algorithm use auxiliari parameter famili iter algorithm previou section describ separ parallel sequentialupd algorithm section describ parameter famili algorithm includ parallel updat algorithm section well sequentialupd algorithm differ one section famili algorithm also includ algorithm may appropri either certain situat explain below algorithm shown fig similar parallelupd algorithm fig round quantiti tj w gamma tj comput befor vector comput ffi comput fig now howev vector ad directli instead anoth vector select provid scale featur vector chosen maxim measur progress restrict belong set allow form scale vector given set a paramet algorithm restrict vector satisfi constraint i parallelupd algorithm fig obtain choos assum i equival make assumpt choos paramet same fig r n mthetan satisfi condit output same fig ffl tj ffl j ffl updat paramet figur parameter famili iter optim algorithm obtain sequentialupd algorithm choos set unit vector ie one compon equal other equal assum ij j updat becom ae tj els anoth interest case assum i natur choos ensur a maxim solv analyt give updat thi idea gener easili case dual norm p q final case restrict scale vector all ie choos r case maxim problem must solv choos linear program problem n variabl constraint prove converg entir famili algorithm theorem given assumpt theorem algorithm fig converg optim sens theorem proof use auxiliari function j theorem function continu nonposit bound chang use techniqu given theorem tj tj tj final j sinc everi j exist j impli appli lemma complet theorem section show result extend multiclass case gener preced result see new algorithm need devis new converg proof need prove case rather preced algorithm proof directli appli multiclass case multiclass case label set cardin k featur form h logist regress use model e f xy y e f xgammaf xy y loss train set e f transform framework follow let fy gg vector p q etc work r is mdimension index pair b let convex function f use case defin space result bregman distanc y clearli shown v assumpt verifi note ib let k plug definit give delta equal eq thu algorithm section use solv minim problem correspond converg proof also directli applic sever multiclass version adaboost ada boostm a special case adaboostmr base loss function ib exp loss use similar set except choic f instead use ib fact actual f use binari adaboost mere chang index set b thu befor ib v choos multiclass logist regress bf delta equal loss eq thu use preced algorithm solv multiclass problem well particular sequentialupd algorithm give adaboostm adaboostmh anoth multiclass version ada boost adaboostmh replac b index set exampl label defin ae loss function adaboostmh exp let use f binari adaboost q obtain multiclass version adaboost comparison iter section describ gener iter scale gi procedur darroch ratcliff comparison algorithm larg follow descript gi given berger della pietra della pietra multiclass case make comparison stark possibl present gi notat prove converg use method develop previou section so also abl relax one key assumpt tradit use studi gi adopt notat setup use multiclass logist regress section to knowledg analog gi exponenti loss consid case logist loss also extend notat defin q iy q i defin moreov verifi q defin eq gi follow assumpt regard featur usual made section prove gi converg second condit replac milder one name sinc multiclass case constant ad featur h j without chang model loss function sinc featur scale constant two assumpt consid clearli made hold without loss gener improv iter scale algorithm della pietra della pietra lafferti also requir milder assumpt much complic implement requir numer search such newton raphson featur iter gi work much like parallelupd algorithm section f q defin multiclass logist regress section differ comput vector updat ffi gi requir direct access featur h j specif gi ffi defin clearli updat quit differ updat describ paper use notat section reformul framework follow theta h j ib prove converg updat use usual auxiliari function method theorem let f q abov modifi gi algorithm describ converg optim sens theorem proof show auxilliari function vector q comput gi clearli continu usual nonneg properti unnorm rel entropi impli eq h w thu impli constraint q proof theorem remain shown introduc notat rewrit gain follow use eq plug definit first term eq written next deriv upper boundon second term eq train loss is seq seq par train loss is seq seq par figur train logist loss data gener noisi hyperplan mani left right relev featur eq follow log bound ln x x gamma eq use eq assumpt form h j s eq follow definit updat ffi final combin eq give eq complet proof clear differ gi updat given paper stem eq deriv ith term sum choic c effect mean log bound taken differ point ln gener case bound exact vari c vari bound taken therebi vari updat section briefli describ experi use synthet data experi preliminari intend suggest possibl algo rithm practic valu systemat experi clearli need use realworld synthet data compar new algorithm commonli use procedur first test effect method minim logist loss train data first ex periment gener data use noisi hyperplan specif first gener random hyperplan dimension space repres vector w r chosen uniformli random unit sphere chose point x r point normal distribut x n i next assign label point depend whether fell chosen hyperplan ie label chosen perturb point x ad random amount n i effect caus label point near separ hyperplan noisi point farther it featur identifi coordin x ran parallel sequentialupd algorithm section denot par seq figur data also ran sequentialupd algorithm special case parameter famili describ section denot seq final ran iter scale algorithm describ section is result experi shown left fig show plot logist loss train set four method function number iter the loss normal method well comparison iter scale parallelupd method clearli best follow close second sequentialupd algorithm parallelupd method much time faster in term number iter iter scale right fig shown result similar experi four compon w forc zero word four relev variabl featur experi sequentialupd algorithm perform kind featur select initi signific advantag test error log seq exp seq log par exp par figur test misclassif error data gener noisi hyperplan boolean featur parallelupd algorithm eventu overtaken last experi test effect new competitor adaboost minim test misclassif error experi chose separ hyperplan w first experi now howev chose point x uniformli random boolean hypercub fgamma g label comput befor label chosen flip coordin point x independ probabl nois model effect caus exampl near decis surfac noisier far it experi use parallel sequenti updat algorithm section denot par seq case use variant base exponenti loss exp logist loss log in case sequentialupd algorithm section identi cal fig show plot classif error separ test set exampl larg differ perform exponenti logist variant algorithm howev parallelupd variant start much better although eventu method converg roughli perform level acknowledg mani thank manfr warmuth first teach us bregman distanc mani comment earlier draft thank also nigel duffi david helmbold raj iyer help discuss suggest research done yoram singer att lab r della pietra relax method find common point convex set applic solut problem convex program arc edg predict game arc classifi gener iter scale loglinear model induc featur random field scale boost base learner via adapt sampl potenti booster decisiontheoret gener onlin learn applic boost addit logist regress statist view boost boost entropi project addit model statist learn algorithm base bregman di tanc function gradient techniqu combin hypoth se improv boost algorithm use confidencer predict inform theoret optim techniqu comput learn theori discoveri scienc tr ctr stefan riezler new develop pars technolog comput linguist v n p septemb nir kraus yoram singer leverag margin care proceed twentyfirst intern confer machin learn p juli banff alberta canada hoiem alexei a efro martial hebert automat photo popup acm transact graphic tog v n juli zhihua zhang jame t kwok dityan yeung surrog maximizationminim algorithm adaboost logist regress model proceed twentyfirst intern confer machin learn p juli banff alberta canada cynthia rudin ingrid daubechi robert e schapir dynam adaboost cyclic behavior converg margin journal machin learn research p steven j phillip miroslav dudk robert e schapir maximum entropi approach speci distribut model proceed twentyfirst intern confer machin learn p juli banff alberta canada amir globerson terri y koo xavier carrera michael collin exponenti gradient algorithm loglinear structur predict proceed th intern confer machin learn p june corvali oregon zhihua zhang jame t kwok dityan yeung surrog maximizationminim algorithm extens machin learn v n p octob tane mielikinen evimaria terzi panayioti tsapara aggreg time partit proceed th acm sigkdd intern confer knowledg discoveri data mine august philadelphia pa usa joshua goodman sequenti condit gener iter scale proceed th annual meet associ comput linguist juli philadelphia pennsylvania stefano merler bruno capril cesar furlanello parallel adaboost weight dynam comput statist data analysi v n p februari gokhan tur extend boost larg scale spoken languag understand machin learn v n p octob hoiem alexei a efro martial hebert recov surfac layout imag intern journal comput vision v n p octob w john wilbur lana yeganova kim synergi pav adaboost machin learn v n p novemb heinz h bauschk dualiti bregman project onto translat cone affin subspac journal approxim theori v n p march joseph turian i dan melam advanc discrimin pars proceed st intern confer comput linguist th annual meet acl p juli sydney australia michael collin paramet estim statist pars model theori practic distributionfre method new develop pars technolog kluwer academ publish norwel ma michael collin terri koo discrimin rerank natur languag pars comput linguist v n p march ron meir gunnar rtsch introduct boost leverag advanc lectur machin learn springerverlag new york inc new york ny