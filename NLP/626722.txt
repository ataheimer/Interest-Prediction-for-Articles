t finit precis error analysi neural network hardwar implement a parallel process low precis fix point hardwar use build high speed neural network comput engin low precis result drastic reduct system cost reduc silicon area requir implement singl process unit taken advantag implement multipl process unit singl piec silicon oper parallel import question aris much precis requir implement neural network algorithm low precis hardwar theoret analysi error due finit precis comput undertaken determin necessari precis success forward retriev backpropag learn multilay perceptron analysi easili extend provid gener finit precis analysi techniqu neural network algorithm set hardwar constraint may evalu b introduct high speed desir implement mani neural network algorithm backpropag learn multilay perceptron mlp may attain use finit precis hardwar finit precis hardwar howev prone error method theoret deriv statist evalu error present could use guid detail hardwar design algorithm implement paper devot deriv techniqu involv well detail backpropag exampl intent provid gener framework neural network algorithm set hardwar constraint may evalu section demonstr sourc error due finit precis comput statist properti gener error model also deriv equat error output gener compound oper may written exampl error equat deriv section oper requir forward retriev error backpropag step mlp statist analysi simul result result distribut error individu step mlp also includ section error equat integr section predict influenc finit precis comput sever stage earli middl final stage backpropag learn final conclud remark given section sourc error finit precis comput finit precis comput nonlinear oper multipl variabl sever sourc error exist exampl comput oewx two input variabl w x input error ffl w ffl x respect whose sourc prior finit precis data manipul error gener due finit precis comput involv oper specif finit precis multipl two variabl gener one error ffl similarli finit precis nonlinear oper oe gener error ffl oe therefor result finit precis result equal assum error product ffl w ffl x neglig first order taylor seri approxim use input error propag oper exampl multipl two variabl finit precis error propag error ie wffl x propag error along gener finit precis multipl error ffl propag nonlinear oper result total finit precis error total finit precis error ffl impos becom input finit precis error variabl futur oper error gener propag success oper compound oper produc success oper oe shown figur error input ffl x error gener oper ffl oe propag remain oper output approxim output error ffl term ffl x ffl oe oe figur figur success oper gener propag error defin intermedi result first success oper carri similar expans intermedi valu rewrit k defin product shown chain rule deriv y approxim deriv without error y note approxim equival approxim alreadi made first order taylor seri therefor y x y error gener propag gener compound oper effect finit precis error output gener system compound oper multipl input variabl calcul extens previou analysi success oper singl variabl follow step employ break comput calcul graph see exampl given figur gener calcul graph made n oper foe g system input g number oper oe intermedi gener input fy k g oper oe lower indic oper output extend eq multipl input total finit precis error ffl given y y use calcul graph partial deriv evalu substitut eq give equat ffl statist method discuss use evalu error eq method includ comput mean varianc variou function random variabl well approxim use central limit theorem common techniqu finit precis comput three common techniqu use finit precis comput truncat jam round truncat oper simpli chop q lowest order bit number leav new lowest order bit r th place unchang jam oper chop q lowest order bit number forc new lowest order bit r th place q remov bit otherwis new lowest order bit retain valu oper equival replac r th bit logic r th bit q bit chop off jam oper advantag gener error zero mean gener error higher varianc compar truncat one round oper also chop q lowest order bit number new lowest order bit r th place q bit valu chop greater equal r result valu increment r otherwis remain unchang error gener truncat jam round techniqu may consid discret random variabl distribut rang determin specif techniqu employ statist view error desir know mean varianc error gener three techniqu discret random variabl x mean given by varianc is usual assum uniformli distribut truncat truncat gener error uniformli distribut rang r q possibl error valu equal probabl therefor mean varianc may comput jam error gener jam uniformli distribut probabl error zero twice probabl error hold possibl valu rang error x result q possibl error valu mean varianc jam error round round gener error uniformli distribut rang r q possibl error valu equal probabl therefor mean varianc comput nonlinear function discret random variabl nonlinear function discret random variabl x mean varianc given by statist properti independ random variabl two independ random variabl x y mean x varianc oe constant a follow properti mean varianc shown multipl constant sum two independ random variabl product two independ random variabl statist properti sum independ random variabl expect squar error expect squar error written term mean varianc error consid set error independ random variabl fffl g mean varianc oe then expect valu averag sum squar fffl g written n note oe expect valu averag sum squar error equal central limit theorem central limit theorem state fx g independ random variabl densiti sum properli normal tend normal curv n discret random variabl probabl tend sampl normal curv normal achiev coupl differ way discret random variabl mean varianc oe sum x result invok central limit theorem probabl sum random variabl x equal discret valu x k approach sampl normal curv n larg oe x sum product independ random variabl central limit theorem extend cover case random variabl sum product random variabl say independ random variabl fx g fy g probabl densiti random variabl xy approach normal curv larg n mean varianc equal xy applic neural network retriev learn shown oper retriev learn phase neural network model formul linear affin transform interleav simpl linear nonlinear scalar oper term hardwar implement formul call mac multipli accumul processor hardwar without loss gener specif discuss multilay perceptron neural network model backpropag learn forward retriev back propag mlp given train fixedweight mlp retriev phase receiv input test pattern fx i g propag forward network comput activ valu output layer fx lj g use indic classif regress purpos hand commonli use learn phase mlp input train pattern fx i g first propag forward network activ valu fx lj g comput accord forward oper use retriev phase output activ valu fx lj g compar target valu g valu output delta fffi lj g neuron output layer deriv error signal propag backward allow recurs comput hidden delta fffi lj g well updat valu weight fw lij g layer mani method propos acceler learn estim local curvatur train error surfac use second order deriv inform discuss method beyond scope paper refer oper forward retriev llayer perceptron formul forward affin transform interleav nonlinear scalar activ function x li denot activ valu th neuron l th layer w lij denot synapt weight interconnect th neuron l th layer j th neuron th layer nonlinear activ function f usual taken sigmoid learn mlp follow iter gradient descent approach follow updat present train data pair comput backpropag error ffi lj formul backward affin transform interleav postmultipl deriv nonlinear activ function initi outputlay propag error finit precis analysi forward retriev explicitli follow procedur discuss section calcul graph forward retriev oper simplifi notat see eq mlp shown figur r r x n w nj f figur calcul graph forward retriev mlp denot truncat jam round oper carri analyt formula given eq forward retriev mlp sever partial deriv need comput eq y w ij y y y y i y i y substitut valu partial deriv eq gener propag error variabl oper eq ffl finit precis analysi output eq calcul graph comput backpropag error output neuron simplifi notat shown figur r r j figur calcul graph output neuron again carri analyt formula given eq output delta comput mlp partial deriv eq evalu y y y y j substitut partial deriv individu error term overal finit precis error output delta comput is finit precis analysi hidden eq calcul graph comput backpropag error hidden layer neuron simplifi notat shown figur follow similar partial deriv evalu use eq comput finit precis error hidden delta see eq ffl k ffl k r figur calcul graph hidden neuron finit precis analysi weight updat eq calcul graph comput weight updat without momentum term simplifi notat shown figur r j figur calcul graph follow similar partial deriv evalu use eq comput finit precis error weight updat see eq statist evalu finit precis error given analyt express finit precis error associ forward retriev backpropag mlp statist evalu error undertaken evalu base mean varianc analysi use truncat jam round techniqu also base statist properti independ random variabl sum independ random variabl sum product independ random variabl discuss section first step choos precis compon employ step problem practic limit precis implement mlp algorithm might use precis follow neuron bit bit right decim input output target rang weight bias use bit one sign bit bit left bit right decim rang output ffi bit one sign bit bit right decim rang hidden ffi use bit one sign bit bit left bit right decim rang finit precis error forward retriev expect forward retriev error calcul singl layer neuron multipl layer neuron propag upward finit precis error lower layer first simplif may made equat multipli accumul step comput without gener error enough bit eg bit use intermedi step i case ffl practic sinc expens accumul precis small oper reduc final bit sum bit one sign bit bit left bit right decim valu use input sigmoid lookup tabl equat may rewritten invok central limit theorem sum necessari know distribut random variabl w ij ffl w ij tabl show statist evalu valu contribut compon eq base assumpt bit size given abov evalu start bit weight uniformli distribut across entir rang weight error come truncat bit weight bit input neuron x bit valu uniformli distribut truncat bit valu therefor ffl x truncat error distribut f j approxim function normal distribut random variabl ffl error gener accumul bit valu jam becom bit valu ffl f j error gener lookup tabl approxim round bit place rv type q r oe round truncat jam round tabl mean varianc variabl forward retriev calcul base precis assumpt given tabl variou size bitalloc weight figur show statist evalu averag sum squar ffl defin eq due finit precis comput singl step forward retriev evalu weight bit number say kbit weight alway contain one sign bit leftofdecim bit k rightofdecim bit lower solid curv show statist evalu finit precis error introduc neuron first hidden layer upper solid curv show second hidden layer or output layer layer perceptron note statist evalu error show divein around bit weight fact suggest implement finit precis hardwar forward retriev purpos train network use high precis comput under constraint sign plu bit left decim download well train finit precis portion bit total bit right decim weight hardwar perform degrad due finit precis convers almost neglig weight bit averag squar figur statist evalu simul evalu valu effl introduc neuron first second hidden layer simul also conduct verifi statist evalu layer perceptron input hidden neuron output neuron simul set randomli gener dimension input data test averag weight network also randomli gener averag sum squar differ finit precis comput full precis bit contribut compon comput thu obtain lower dash curv show simul evalu finit precis error introduc neuron first hidden layer upper dash curv show output layer curv match quit consist produc statist evalu finit precis analysi iter learn discuss section backpropag learn involv four consecut step comput forward retriev output delta comput hidden delta comput weight updat therefor weight updat iter present train pattern finit precis error ffl w introduc fw lij g given eq fact propag result error gener previou three step therefor final mathemat express finit precis error singl weight updat iter formul straightforward manner base exist deriv given eq statist evalu valu averag sum squar weight updat error ffl w due finit precis comput singl learn iter thu comput backpropag learn discuss simpli nonlinear optim problem base simpl gradient descent search eleg way comput gradient use chain rule layer network gradient descent search updat weight base first deriv approxim error surfac updat individu weight independ other therefor even approach comput effici behav unwis converg slowli complex error surfac due strong influenc introduc gradient descent search approxim real effect learn converg accuraci due finit precis comput difficult measur therefor statist evalu averag sum squar ffl w itself determin network propens learn ratio finit precis error meaning measur ae potenti indic effect finit precis comput weight updat defin ratio statist averag sum squar finit precis weight updat error ffl w full precis weight updat magnitud w ratio serv use indic addit impact caus finit precis comput top caus gradient descent approxim backpropag learn ratio depend number bit assign finit precis comput current stage learn progress specifi distribut differ desir actual output g specif assum ffl x lj sinc abil learn depend abil learn finit precis valu base practic choic finit precis bit size given section vs number bit say k bit assign weight fw ij g weight updat fw ij g statist evalu ratio sever differ stage learn figur show statist evalu valu finit precis ratio weight connect neuron hidden output layer layer mlp differ valu use repres four differ stage learn earli stage middl stage converg stage converg stage figur show statist evalu valu finit precis ratio weight connect neuron hidden input layer four differ valu fi note finit precis ratio curv gradual along variou stage learn dive around region number bit weight bit soft converg stage hard converg stage learn get almost steadi number bit increas further dive point indic potenti strong disturb converg accuraci backpropag lean long run therefor give good guidelin number bit requir weight learn similar learn converg accuraci attain use high precis comput also interest note later stage learn impact finit precis error get larger due smaller valu fi network finetun weight weight bit finit precis figur statist evalu valu finit precis ratio toplay weight layer mlp four differ stage learn evalu simul result iter learn verifi theoret evalu impact caus finit precis comput simpl regress problem design map dimension input fx dimension output fyg mlp contain input neuron hidden neuron output neuron adopt pair randomli select data train finit precis learn simu weight bit finit precis figur statist evalu valu finit precis ratio hiddenlay weight layer mlp four differ stage learn evalu lation perform base choic bit size compon given section vs differ number bit assign fw ij g fw ij g figur show averag of train data squar differ desir actual output d regress problem network converg a hard converg usual requir kind nonlinear regress problem note that predict point around bit weight squar differ curv dive impli inabl converg desir map number bit weight less bit similar support result also observ xor classif problem use mlp input hidden output see figur due classif natur xor problem soft converg good enough termin train therefor predict point bit weight squar differ curv dive anoth interest observ worthwhil mention total finit precis error singl iter weight updat mainli gener final jam oper comput output delta hidden delta weight updat therefor even though requir least bit assign comput weight updat store total weight valu number weight bit comput forward retriev hidden delta step learn low bit without excess degrad learn converg accuraci weight bit averag squar figur averag squar differ desir actual output d regress problem network converg weight bit averag squar figur averag squar differ desir actual output problem network converg conclud remark paper devot deriv finit precis error analysi techniqu neural network implement especi analysi backpropag learn mlp analysi techniqu propos versatil prepar ground wider varieti neural network algorithm recurr neural network competit learn network etc network share similar comput mechan use backpropag learn forward retriev oper shown bit weight suffici maintain perform use high precis comput hand network learn least bit precis must use weight avoid train process divert much trajectori high precis comput r vlsi architectur highperform implement limit artifici neural network nonlinear optim neural network learn parallel architectur artifici neural net unifi architectur artifici neural network recurs least squar learn algorithm neural net work pizer victor l learn intern represent error propag beyond regress new tool predict analysi behavior scienc tr unifi systol architectur artifici neural network learn intern represent error propag ctr ming zhang stamati vassiliadi jo g delgadofria sigmoid gener neural comput use piecewis approxim ieee transact comput v n p septemb cesar alippi luciano briozzo accuraci vs precis digit vlsi architectur signal process ieee transact comput v n p april yongsoon lee seokbum ko fpgabas face detector use neural network scalabl float point unit proceed th wsea intern confer circuit system electron control signal process p novemb dalla texa cesar alippi random algorithm systemlevel polytim analysi robust comput ieee transact comput v n p juli