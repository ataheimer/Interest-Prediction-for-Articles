t choos multipl paramet support vector machin a problem automat tune multipl paramet pattern recognit support vector machin svm consid done minim estim gener error svm use gradient descent algorithm set paramet usual method choos paramet base exhaust search becom intract soon number paramet exce two experiment result assess feasibl approach larg number paramet more demonstr improv gener perform b introduct problem supervis learn one take set inputoutput pair attempt construct classier function f map input vector x x onto label y interest pattern recognit classic case set label simpli g goal nd f f minim error fx y futur exampl learn algorithm usual depend paramet control size class f way search conduct f sever techniqu exist perform select paramet idea nd paramet minim gener error algorithm hand error estim either via test data use learn hold test crossvalid techniqu via bound given theoret analysi tune multipl paramet usual multipl paramet tune time moreov estim error explicit function paramet applic strategi exhaust search paramet space becom intract sinc would correspond run algorithm everi possibl valu paramet vector up discret propos methodolog automat tune multipl paramet support vector machin svm take advantag specic algorithm svm algorithm support vector machin svm realiz follow idea map ndimension input vector x r n high dimension possibl innit dimension featur space h construct optim separ hyperplan space dierent map construct dierent svm train data separ optim hyperplan one maxim distanc in h space hyperplan closest imag vector x train data nonsepar train data gener concept use suppos maxim distanc equal imag train vector x within sphere radiu r follow theorem hold true theorem given train set size featur space h hyperplan w b margin mw b z radiu rz dene kwk maximum margin algorithm take input train set size return hyperplan featur space margin mw b z maxim note suppos train separ rest articl refer vector matric use bold notat mean assumpt probabl measur p underli data z expect misclass probabl bound expect taken random draw train set z size left hand side size right hand side theorem justi idea construct hyperplan separ data larg margin larger margin better perform construct hyperplan note howev accord theorem averag perform depend ratio efr m g simpli larg margin multipl paramet svm algorithm usual depend sever paramet one them denot c control tradeo margin maxim error minim paramet appear nonlinear map featur space call kernel param ter simplic use classic trick allow consid c kernel paramet paramet treat uni framework wide acknowledg key factor svm perform choic kernel howev practic dierent type kernel use due diculti appropri tune paramet present techniqu allow deal larg number paramet thu allow use complex kernel anoth potenti advantag abl tune larg number paramet possibl rescal attribut inde priori knowledg avail mean attribut choic use spheric kernel ie give weight attribut one may expect better choic shape kernel sinc mani realworld databas contain attribut dierent natur may thu exist appropri scale factor give right weight right featur exampl see use radial basi function kernel rbf mani dierent scale factor input dimens usual approach consid tri pick best valu howev use propos method choos automat good valu scale factor inde factor precis paramet kernel moreov demonstr problem featur select address framework sinc correspond nding attribut rescal zero factor without harm gener thu see tune kernel paramet someth extrem use procedur allow would versatil tool variou task nding right shape kernel featur select nding right tradeo error margin etc give rational develop techniqu approach thu see goal nd hyperplan maxim margin also valu map paramet yield best gener error so propos minimax ap maxim margin hyperplan coecient minim estim gener error set kernel paramet last step perform use standard gradient descent approach kind error estim consid sever way assess gener error valid error procedur requir reduc amount data use learn order save valid moreov estim smooth proper gradient descent leaveoneout error estim procedur give estim expect gener analyt function paramet examin accuraci estim uenc whole procedur nding optim paramet particular show realli matter variat estim relat variat test error rather valu relat outlin paper organ follow next section introduc basic svm dierent possibl estim gener error describ section section explain smooth these estim introduc section framework minim estim gradient descent section deal comput gradient error estim respect kernel paramet final section present experiment result method appli varieti databas dierent context section deal nding right penal along right radiu kernel nding right shape kernel section present result appli method featur select learn introduc standard notat svm complet descript see let fx set train exampl x r n belong class label g svm methodolog map vector featur space use kernel function kx dene inner product featur space here consid kernel k depend set paramet decis function given is coecient obtain maxim follow function constraint coecient dene maxim margin hyperplan highdimension featur space data map nonlinear function formul svm optim problem call hard margin formul sinc train error allow everi train point satis inequ correspond equal satis point call support vector notic one may requir separ hyperplan pass origin choos xed variant call hard margin svm without threshold case optim problem remain except constraint disappear deal nonsepar nonsepar case one need allow train error result call soft margin svm algorithm shown soft margin svm quadrat penal error consid special case hard margin version modi kernel ident matrix c constant penal train error focu remaind hard margin svm use whenev deal nonsepar data thu c consid anoth paramet kernel function estim perform svm ideal would like choos valu kernel paramet minim true risk svm classier unfortun sinc quantiti access one build estim bound it section present sever measur expect error rate svm singl valid estim one enough data avail possibl estim true error valid set estim unbias varianc get smaller size valid set increas valid set fx estim step function leaveoneout bound leaveoneout procedur consist remov train data one element construct decis rule basi remain train data test remov element fashion one test element train data use dierent decis rule let us denot number error leaveoneout procedur lx known leaveoneout procedur give almost unbias estim expect gener error err probabl test error machin train sampl size expect taken random choic sampl although lemma make leaveoneout estim good choic estim gener error nevertheless costli actual comput sinc requir run train algorithm time strategi thu upper bound approxim estim easi comput quantiti have possibl analyt express denot f classier obtain train exampl present f one obtain exampl remov also written thu u p upper bound p f get follow upper bound leaveoneout error sinc hard margin svm monoton increas support vector count sinc remov nonsupport vector train set chang solut comput machin ie u nonsupport vector restrict preced sum support vector upper bound term sum give follow bound number error made leaveoneout procedur n sv denot number support vector jaakkolahaussl bound svm without threshold analyz optim perform svm algorithm comput leaveoneout error jaakkola haussler prove inequ lead follow upper bound note wahba et al propos estim number error made leaveoneout procedur hard margin svm case turn seen upper bound jaakkolahaussl one sinc opperwinth bound hard margin svm without threshold opper winther use method inspir linear respons theori prove follow assumpt set support vector chang remov exampl p k sv matrix dot product support vector lead follow estim radiusmargin bound svm without threshold train error vapnik propos follow upper bound number error leaveoneout procedur r radiu margin dene theorem span bound vapnik chapel deriv estim use concept span support vector assumpt set support vector remain leaveoneout procedur follow equal true distanc point set p give exact number error made leaveoneout procedur previou assumpt span estim relat approxim link jaakkolahaussl bound consid svm without threshold constraint remov denit span easili upper bound valu span thu recov jaakkolahaussl bound link r m support vector number error made leaveoneout procedur bound by x shown span p bound diamet smallest sphere enclos train point sinc number error made leaveoneout procedur bound link opperwinth support vector chang hard margin case without threshold give valu opperwinth bound name smooth test error estim estim perform svm valid error leaveoneout error requir use step function howev would like use gradient descent approach minim estim test error unfortun step function dierenti alreadi mention section possibl bound x x x bound r m deriv leaveoneout error nevertheless so larg error count one therefor might advantag instead use contract function form howev choic constant b dicult small estim accur larg result estim smooth instead tri pick good constant b one tri get directli smooth approxim test error estim posterior probabl recent platt propos follow estim posterior distribut p svm output fx figur valid error dierent valu width rbf kernel top left step function otherwis note bottom pictur minimum right place fx output svm constant b found minim kullbackleibl diverg p empir approxim p built valid set optim carri use second order gradient descent algorithm accord estim best threshold svm classier f pa b x note b obtain correct compar usual svm threshold denit gener error classier z z error empir estim min note label valid set use directli last step indirectli estim constant b appear parametr form pa b better understand estim let us consid extrem case error valid set maximum likelihood algorithm go yield pa b x take binari valu consequ estim error probabl zero optim kernel paramet let go back svm algorithm assum kernel k depend one sever paramet encod vector thu consid class decis function parametr b want choos valu paramet w see equat maxim maximum margin algorithm model select criterion minim best kernel paramet precis xed want choos one dimension paramet one typic tri nite number valu pick one give lowest valu criterion note abbrevi pa b x svm solut continu respect better approach propos cristianini et al use increment optim algorithm one train svm littl eort chang small amount howev soon one compon comput everi possibl valu becom intract one rather look way optim along trajectori kernel paramet space use gradient model select criterion optim model paramet propos demonstr case linear regress timeseri predict also propos optim regular paramet neural network propos algorithm altern svm optim gradient step direct gradient paramet space achiev follow iter procedur initi valu use standard svm algorithm find maximum quadrat form w updat paramet minim typic achiev gradient step see below go step stop minimum reach solv step requir estim vari thu restrict case k dierenti respect moreov consid case gradient respect comput or approxim note depend implicitli sinc dene maximum w then n kernel paramet deriv respect p is xed comput gradient r way perform step make gradient step small eventu decreas converg improv use second order deriv newton method laplacian oper dene formul addit constraint impos project gradient comput gradient section describ comput gradient with respect kernel paramet dierent estim gener error first bound r m see theorem obtain formul deriv margin section radiu section valid error see equat show calcul deriv hyperplan paramet b see section final comput deriv span bound present section rst begin use lemma suppos given n vector v nn matrix smoothli depend paramet consid function let x vector x maximum l attain x word possibl dierenti l respect x depend note also true one or both constraint denit f remov proof rst need express equal constraint lagrang multipli inequ constraint lagrang multipli maximum follow condit veri v p x last term written follow v p b x use deriv optim condit name fact either henc v p result follow comput deriv margin note featur space separ hyperplan follow expans normal min follow denit margin theorem latter kwk thu write bound r m r kwk previou lemma enabl us comput deriv kwk in deed shown lemma appli standard svm optim problem give comput deriv radiu comput radiu smallest sphere enclos train point achiev solv follow quadrat problem constraint use previou lemma comput deriv radiu maxim previou quadrat form comput deriv hyperplan paramet let us rst comput deriv respect paramet kernel purpos need analyt formul first suppos point support vector remov train set assumpt done without loss gener sinc remov point support vector aect solut then fact point lie margin z k n support vector h n matrix paramet svm written as abl comput deriv paramet respect kernel paramet p inde sinc deriv invers matrix depend paramet p written follow nalli easili use result calcul recov comput p inde denot h turn that inequ easili prove dierenti mm comput deriv spanrul let us consid span valu recal span support vector x p dene distanc point set dene valu span written as note introduc lagrang multipli enforc constraint introduc extend vector extend matrix dot product support vector valu span written as h submatrix k sv row column p remov v pth column k sv fact optim valu h v follow last equal come follow block matrix ident known woodburi formula close form obtain particularli attract sinc comput valu span support vector invert matrix k sv combin equat get deriv span pp thu complex comput deriv spanrul respect paramet p kernel requir comput invers matrix k sv complex oper larger quadrat optim problem itself howev problem approach valu given span rule continu chang smoothli valu paramet coecient p chang continu span not actual discontinu support vector set support vector chang easili understood equat suppos chang valu paramet point xm support vector anymor support vector set p go smaller discontinu like appear valu situat explain gure plot valu span support vector x p versu width rbf kernel almost everywher span decreas henc neg deriv jump appear correspond chang set support vector moreov span global increas valu deriv give us good indic global evolut span one way solv problem tri smooth behavior span done impos follow addit constraint denit p equat constant given constraint point xm leav enter set support vector larg uenc span support vector sinc small eect constraint make set p becom continu set support vector chang howev new constraint prevent us comput span ecient equat possibl solut replac constraint eeee figur valu p sum span train point dierent valu width rbf kernel vari small vicin regular term comput span new denit span equat becom diagon matrix element shown gure span much smoother minimum still right place experi took note comput deriv new express dicult previou span express interest look leaveoneout error svm without threshold case valu span regular write alreadi point section valu span is eee figur left minima span regular dash line without regular solid line close right detail behavior span dierent valu regular recov opperwinth bound hand case span bound ident jaakkolahaussl one way span bound regular bound opperwinth jaakkolahaussl experi experi dierent natur carri assess perform feasibl method rst set experi consist nding automat optim valu two paramet width rbf kernel constant c equat second set experi correspond optim larg number scale factor case handwritten digit recognit show optim scale factor lead natur featur select demonstr applic method select relev featur sever databas optim detail core techniqu present gradient descent algorithm use optim toolbox matlab perform it includ second order updat improv converg speed crossvalid r m spanbound breast cancer diabeti heart thyroid tabl test error found dierent algorithm select svm paramet c rst column report result second last column paramet found minim r m spanbound use gradient descent algorithm benchmark databas rst set experi tri select automat width rbf kernel along constant c penal train error appear equat order avoid ad posit constraint optim problem for constant c width rbf kernel use parameter turn give stabl optim use benchmark databas describ databas long split train test set avail httpidafirstgmdderaetschdatabenchmarkshtm follow experiment setup rst train set kernel paramet estim use either fold crossvalid minim r m spanbound final kernel paramet comput median estim result shown tabl turn minim r m span estim yield approxim perform pickingup paramet minim crossvalid error surpris sinc crossvalid known accur method choos hyperparamet learn algorithm interest compar comput cost meth crossvalid r m spanbound breast cancer diabeti heart thyroid titan tabl averag number svm train one train set need select paramet c use standard crossvalid minim spanbound od tabl show mani svm train averag need select kernel paramet split result crossvalid one report tri dierent valu c perform fold crossvalid number svm train train set need method gain complex impress averag time less svm train requir nd kernel paramet main reason gain two paramet optim comput reason exhaust search crossvalid handl select paramet wherea method can highlight next section discuss explain section r m seem rough upper bound spanbound accur estim test error howev process choos kernel paramet matter bound whose minimum close optim kernel paramet even r m cannot use estim test error previou experi show minim yield quit good result gener error obtain minim spanbound cf gure slightli better sinc minim latter dicult implement control more local minima recommend practic minim r m experi follow section relat experi bound similar result obtain spanbound automat select scale factor experi tri choos scale factor rbf polynomi kernel degre precis consid kernel follow experi carri usp handwritten digit recognit databas databas consist train exampl test exampl digit imag size x pixel tri classifi digit train set split subset exampl subset use experi assess feasibl gradient descent approach nding kernel paramet rst use paramet one correspond scale factor squar tile pixel shown gure figur tile scale factor pixel ident scale paramet initi evolut test error bound r m plot versu number iter gradient descent procedur gure polynomi kernel rbf note polynomi kernel test error went wherea best test error one scale paramet thu figur evolut test error left bound r m right gradient descent optim polynomi kernel figur evolut test error left bound r m right gradient descent optim rbf kernel take sever scale paramet manag make test error decreas might interest see scale coecient found purpos took scale paramet one per pixel minim polynomi kernel map scale coecient shown gure result quit consist one could expect situat coecient near border pictur smaller middl pictur coecient directli interpret measur relev correspond featur figur scale factor found optim procedur darker mean smaller scale factor discuss experi consid saniti check experi ment inde prove feasibl choos multipl kernel paramet svm lead overt howev gain test error main motiv sinc expect signi cant improv problem featur play similar role take scale factor equal databas seem reason choic howev highlight gure method power tool perform featur select featur select motiv featur select threefold improv gener error determin relev featur for explanatori purpos reduc dimension input space for realtim applic find optim scale paramet lead featur select algo rithm inde one input compon useless classica tion problem scale factor like becom small scale becom small enough mean possibl remov without aect classic algorithm lead follow idea featur select keep featur whose scale factor largest also perform princip compon space scale princip compon scale factor consid two dierent parametr kernel rst one correspond rescal data input space r n second one correspond rescal princip compon space matrix princip compon comput use follow iter procedur initi case princip compon scale perform princip compon analysi comput matrix solv svm optim problem minim estim error respect gradient step discard dimens correspond small element return step demonstr idea two toy problem show featur select reduc gener error appli featur select algorithm dna microarray data import nd gene relev perform classic also seem type algorithm featur select improv perform lastli appli algorithm face detect show greatli reduc input dimens without sacric perform toy data compar sever algorithm standard svm algorithm featur select featur select algorithm estim r m span estim standard svm appli featur select via lter method three lter method use choos largest featur accord to pearson correl coecient fisher criterion score kolmogorovsmirnov test note pearson coecient fisher criterion cannot model nonlinear depend two follow artici dataset object assess abil algorithm select small number target featur presenc irrelev redund featur rst exampl six dimens relev probabl equal rst three featur drawn second three featur fx drawn probabl otherwis rst three drawn second three x remain featur nois x second exampl two dimens relev probabl equal data drawn follow drawn n abil drawn two normal distribut equal probabl befor rest featur nois x linear problem rst six featur redund rest featur irrelev nonlinear problem rst two featur irrelev use linear kernel linear problem second order polynomi kernel nonlinear problem impos featur select algorithm keep best two featur result shown gure variou train set size take averag test error sampl run train set size fisher score not shown graph due space constraint perform almost ident correl coecient problem clearli see method outperform classic method featur select nonlinear problem among r r mean valu rth featur posit neg class ri standard deviat ks tst fr denot rth featur train exampl p correspond empir distribut lter method kolmogorovsmirnov test improv perform standard svm rwbound gradient standard svm correl coeffici kolmogorovsmirnov test rwbound gradient standard svm correl coeffici kolmogorovsmirnov test a b figur comparison featur select method a linear problem b nonlinear problem mani irrelev featur xaxi number train point yaxi test error fraction test point dna microarray data next test idea two leukemia discrimin problem problem predict treatment outcom medulloblastoma rst problem classifi myeloid versu lymphoblast leukemia base express gene train set consist exampl test set exampl standard linear svm achiev error test set use gradient descent r achiev error use error use gene use fisher score select featur result error gene second leukemia classic problem discrimin b versu cell lymphoblast cell standard linear svm make error problem use either span bound gradient descent r result error made use gene use fisher score result error made use gene nal problem one predict treatment outcom patient medulloblastoma exampl express valu dataset use leaveoneout measur error rate standard svm gaussian kernel make error select gene use gradient descent r achiev error face detect trainabl system detect frontal nearfront view face gray imag present gave good result term detect rate system use gray valu imag input seconddegre polynomi kernel svm choic kernel lead featur featur space search imag face dierent scale took sever minut pc make system realtim reduc dimension input space featur space requir featur select princip compon space use reduc dimension input space method evalu larg cmu test set consist face nonfac pattern figur compar roc curv obtain dierent number select compon result show use compon improv perform system figur roc curv dierent number pca gray featur conclus propos approach automat tune kernel paramet svm base possibl comput gradient variou bound gener error respect param ter dierent techniqu propos smooth bound preserv accuraci predict locat minimum test error use smooth gradient abl perform gradient descent search kernel paramet space lead improv perform reduct complex solut featur select use method chose separ case appropri scale factor non separ case method allow us choos simultan scale factor paramet c see equat benet techniqu mani first allow actual optim larg number paramet previou approach could deal paramet most even case small number paramet improv run time larg amount moreov experiment result demonstr accur estim error requir simpl estim like r m good behaviour term allow nd right paramet way render techniqu even applic sinc estim simpl comput deriv final approach avoid hold data valid thu make full use train set optim paramet contrari crossvalid method approach fact proven success variou situat open new direct research theori practic support vector machin practic side approach make possibl use highli complex tunabl kernel tune scale factor adapt shape kernel problem select relev featur theoret side demonstr even larg number paramet simultan tune overt eect remain low cours lot work remain done order properli understand reason anoth interest phenomenon fact quantit accuraci estim use gradient descent margin relev rais question design good estim paramet tune rather accur estim futur investig focu tri understand phenomena obtain bound gener error overal algorithm along look new problem approach could appli well new applic acknowledg author would like thank jason weston elodi nedelec help comment discuss r medulloblastoma diagnosi outcom predict gene express pro model select support vector ma chine support vector network dynam adapt kernel support vector machin face detect still gray imag probabilist kernel regress model adapt regular neural network model estim charact obtain statist procedur recognit gaussian process svm mean probabl support vector machin featur select face detect robust bound gener margin distribut natur statist learn theori statist learn theori bound error expect support vector machin gener approxim crossvalid support vector machin anoth way look margin like quantiti featur select support vector machin tr ctr alex d holub max well pietro perona hybrid generativediscrimin visual categor intern journal comput vision v n p may dityan yeung hong chang guang dai learn kernel matrix maxim kfdbase class separ criterion pattern recognit v n p juli tobia glasmach christian igel gradientbas adapt gener gaussian kernel neural comput v n p octob kristin p bennett michinari momma mark j embrecht mark boost algorithm heterogen kernel model proceed eighth acm sigkdd intern confer knowledg discoveri data mine juli edmonton alberta canada yoram baram learn kernel polar neural comput v n p june carl gold alex holub peter sollich bayesian approach featur select paramet tune support vector machin classifi neural network v n p june koji tsuda shinsuk uda taishin kin kiyoshi asai minim cross valid error mix kernel matric heterogen biolog data neural process letter v n p februari carlo soar pavel b brazdil select paramet svm use metalearn kernel matrixbas metafeatur proceed acm symposium appli comput april dijon franc tristrom cook two variat fisher linear discrimin pattern recognit ieee transact pattern analysi machin intellig v n p februari carlo soar pavel b brazdil petr kuba metalearn method select kernel width support vector regress machin learn v n p march sayan mukherje qiang wu estim gradient coordin covari classif journal machin learn research p alain rakotomamonji variabl select use svm base criteria journal machin learn research malt kuss carl edward rasmussen assess approxim infer binari gaussian process classif journal machin learn research p keith m sullivan sean luke evolv kernel support vector machin classif proceed th annual confer genet evolutionari comput juli london england mingrui wu bernhard schlkopf gkhan bakir build spars larg margin classifi proceed nd intern confer machin learn p august bonn germani andrea argyri raphael hauser charl a micchelli massimiliano pontil dcprogram algorithm kernel select proceed rd intern confer machin learn p june pittsburgh pennsylvania huseyin inc theodor b trafali hybrid model exchang rate predict decis support system v n p novemb sayan mukherje dingxuan zhou learn coordin covari via gradient journal machin learn research p mingrui wu bernhard schlkopf gkhan bakr direct method build spars kernel learn algorithm journal machin learn research p liefeng bo ling wang licheng jiao featur scale kernel fisher discrimin analysi use leaveoneout cross valid neural comput v n p april alain rakotomamonji franci bach stphane canu yve grandvalet effici multipl kernel learn proceed th intern confer machin learn p june corvali oregon xuewen chen jong cheol jeong minimum refer set base featur select small sampl classif proceed th intern confer machin learn p june corvali oregon train algorithm fuzzi support vector machin noisi data pattern recognit letter v n p octob franci r bach gert r g lanckriet michael i jordan multipl kernel learn conic dualiti smo algorithm proceed twentyfirst intern confer machin learn p juli banff alberta canada r kumar a kulkarni v k jayaraman b d kulkarni symbol assist svm classifi noisi data pattern recognit letter v n p march sren sonnenburg gunnar rtsch christin schfer bernhard schlkopf larg scale multipl kernel learn journal machin learn research p kaiquan shen chongjin ong xiaop li einar p wildersmith featur select via sensit analysi svm probabilist output machin learn v n p januari zhihua zhang jame t kwok dityan yeung modelbas transduct learn kernel matrix machin learn v n p april kaimin chung weichun kao chialiang sun lilun wang chihjen lin radiu margin bound support vector machin rbf kernel neural comput v n p novemb mingwei chang chihjen lin leaveoneout bound support vector regress model select neural comput v n p may keem siah yap izham z abidin abdul rahim ahmad zahrul faizi hussien hooi loong pok fariq izwan ismail abdul malik mohamad abnorm fraud electr meter detect use hybrid support vector machin genet algorithm proceed third confer iast intern confer advanc comput scienc technolog p april phuket thailand olivi chapel train support vector machin primal neural comput v n p may guang dai dityan yeung kernel select forl semisupervis kernel machin proceed th intern confer machin learn p june corvali oregon yime ying dingxuan zhou learnabl gaussian flexibl varianc journal machin learn research p christian igel tobia glasmach britta mersch nico pfeifer peter meinick gradientbas optim kerneltarget align sequenc kernel appli bacteri gene start detect ieeeacm transact comput biolog bioinformat tcbb v n p april lior wolf amnon shashua featur select unsupervis supervis infer emerg sparsiti weightbas approach journal machin learn research p baback moghaddam minghsuan yang learn gender support face ieee transact pattern analysi machin intellig v n p may a rakotomamonji analysi svm regress bound variabl rank neurocomput v n p march yi wu edward y chang distancefunct design fusion sequenc data proceed thirteenth acm intern confer inform knowledg manag novemb washington dc usa abdul majid asifullah khan anwar m mirza combin support vector machin use genet program intern journal hybrid intellig system v n p januari qiang wu yime ying dingxuan zhou multikernel regular classifi journal complex v n p februari shin ando hitoshi iba classif gene express profil use combinatori method evolutionari comput machin learn genet program evolv machin v n p june giorgio valentini thoma g dietterich biasvari analysi support vector machin develop svmbase ensembl method journal machin learn research p fabien lauer ching y suen grard bloch trainabl featur extractor handwritten digit recognit pattern recognit v n p june r brunelli verif finger match comparison support vector machin gaussian basi function classifi pattern recognit letter v n p decemb r brunelli verif finger match comparison support vector machin gaussian basi function classifi pattern recognit letter v n p decemb gavin c cawley nicola l c talbot construct bayesian formul spars kernel learn method neural network v n p june gavin c cawley nicola l c talbot fast exact leaveoneout crossvalid spars leastsquar support vector machin neural network v n p decemb piyush kumar joseph s b mitchel e alper yildirim approxim minimum enclos ball high dimens use coreset journal experiment algorithm jea saher esmeir shaul markovitch anytim learn decis tree journal machin learn research p sbastien gadat laurent youn stochast algorithm featur select pattern recognit journal machin learn research p k pelckman j a suyken b moor addit regular tradeoff fusion train valid level kernel method machin learn v n p march gavin c cawley nicola l c talbot prevent overfit model select via bayesian regularis hyperparamet journal machin learn research p mathia m adankon moham cheriet optim resourc model select support vector machin pattern recognit v n p march mingkun li ishwar k sethi confidencebas classifi design pattern recognit v n p juli ataollah abrahamzadeh sey alireza seyedin mehdi dehghan digitalsignaltyp identif use effici identifi eurasip journal appli signal process v n p januari ron meir gunnar rtsch introduct boost leverag advanc lectur machin learn springerverlag new york inc new york ny zexuan zhu yewsoon ong manoranjan dash markov blanketembed genet algorithm gene select pattern recognit v n p novemb