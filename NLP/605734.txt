t parallel two level block ilu precondit techniqu solv larg spars linear system a discuss issu relat domain decomposit multilevel precondit techniqu often employ solv larg spars linear system parallel comput implement parallel precondition solv gener spars linear system base two level block ilu factor strategi give new data structur strategi construct local coeffici matrix local schur complement matrix processor precondition construct fast robust solv certain larg spars matric numer experi show domain base two level block ilu precondition robust effici publish ilu precondition base schur complement techniqu parallel spars matrix solut b introduct high perform comput techniqu includ parallel distribut computa tion undergon gradual matur process past two decad move experiment laboratori studi mani engin scientif appli cation although share memori parallel comput rel easi program commonli use architectur parallel comput practic distribut technic report no depart comput scienc univers kentucki lexington ky research support part us nation scienc foundat grant ccr ccr part univers kentucki center comput scienc univers kentucki colleg engin email cshencsukyedu z email jzhangcsukyedu url httpwwwcsukyedujzhang memori comput use mpi pvm messag pass even share memori parallel comput use mpi code portabl made distribut program style preval result develop effici numer linear algebra algorithm specif aim high perform comput becom challeng issu mani numer simul model problem cpu consum part comput solv larg spars linear system accept that solv larg spars linear system iter method becom method choic due favor memori comput cost compar direct solut method base gaussian elimin one drawback mani iter method lack robust ie iter method may yield accept solut given problem common strategi enhanc robust iter method exploit precondit techniqu howev robust precondition deriv certain type incomplet lu factor coeffici matrix effici implement parallel comput nontrivi challeng recent trend parallel precondit techniqu gener spars linear system use idea domain decomposit concept processor assign certain number row linear system solv discuss relat point view comparison differ domain decomposit strategi see refer therein simpl parallel precondition deriv use simpl parallel iter method commonli use parallel precondition engin comput point block jacobi precondition precondition easi implement effici sens number precondit iter requir solv realist problem still larg sophist approach parallel precondit use domain decomposit schur complement strategi construct parallel precondition precondition construct approach may scalabl ie number precondit iter increas rapidli number processor increas techniqu class includ variou distribut schur complement method solv gener spars linear system develop spars matric aris finit differ discret partial differenti equat pde level set techniqu usual employ extract inher parallel discret scheme ilu factor perform forward backward triangular solv associ precondit parallel within level set approach seem suitabl implement share memori machin small number processor mani realist problem unstructur mesh parallel extract level set strategi inadequ furthermor ilu precondition may accur enough subsequ precondit iter may converg slowli may converg all thu higher accuraci precondition advoc author increas robust howev higher accuraci precondition usual mean fillin entri kept precondition coupl among node increas well increas coupl reduc inher parallel new order techniqu must employ extract parallel higher accuraci precondition addit standard domain decomposit concept precondit techniqu design specif target parallel comput includ spars approxim invers multilevel treatment although claim inher parallel precon dition effici spars approxim invers techniqu run respect distribut parallel comput scarc recent class high accuraci precondition combin inher parallel domain decomposit robust ilu factor scalabl potenti multigrid method develop multilevel block ilu precondition bilum bilutm test show promis converg rate scalabl solv certain problem construct precondition base block independ set order recurs block ilu factor schur complement although class precondition contain obviou parallel within level parallel implement yet report studi mainli address issu implement multilevel block ilu precondition distribut environ use distribut spars matrix templat bilutm precondition saad zhang modifi implement two level block ilu precondition distribut memori parallel architectur pbilu use saad psparslib librari mpi basic commun routin pbilu precondition compar one favor schur complement base precondition numer experi articl organ follow section background block independ set order bilutm precondition given section outlin distribut represent gener spars linear system section discuss construct precondition pbilu base two level block ilu factor numer experi comparison two schur complement base precondition solv variou distribut linear system present section demonstr merit two level block ilu precondition conclud remark comment futur work given section independ set bilutm distribut spars matrix solver reli classic domain decomposit concept partit adjac graph coeffici matrix graph partit psparslib librari avail onlin httpwwwcsumneduresearcharpap sparslibpspabshtml algorithm softwar packag avail techniqu extract parallel incomplet lu factor bilum bilutm usual relay fact mani row spars matrix elimin simultan given stage gaussian elimin set consist row call independ set larg scale matrix comput degre parallel extract tradit point independ set order inadequ concept block independ set propos thu block independ set set group block unknown coupl unknown two differ group block variou heurist strategi find point independ set may extend find block independ set differ properti simpl usual effici strategi socal greedi algorithm group nearest node togeth consid gener spars linear system form unstructur realvalu matrix order n greedi algorithm or graph partition use find block independ set adjac graph matrix a initi candid node block includ node correspond row matrix a given block size k greedi algorithm start first node group nearest k neighbor node drop node link group k node vertex cover set vertex cover set set node least one link least one node least one block block independ set process repeat time candid node gone either one independ block vertex cover set if number remain candid node less k put vertex cover set mean vertex cover set gener cover case detail algorithm descript see remark necessari independ block number node chosen cardin sake load balanc parallel comput sake easi program parallel implement graph partition similar greedi algorithm describ first invok partit adjac graph a base result partit matrix correspond right hand side unknown vector b x distribut individu processor suppos block independ set uniform block size k found matrix symmetr permut two two block matrix form p permut matrix diagon matrix dimens ks number uniform block size k block usual dens k small spars k larg implement bilum exact invers techniqu use comput b gamma invert small independ note direct invers strategi usual produc dens invers matric even origin block highli spars larg size sever sparsif strategi propos maintain sparsiti b gamma addit spars approxim invers base multilevel block ilu precondition propos articl employ ilu factor strategi comput spars incomplet lu factor b approach similar one use bilutm construct bilutm precondition base restrict ilu factor dual drop strategi ilut multilevel block ilu precondition bilutm retain robust flexibl ilut also power ilut solv difficult problem offer inher parallel exploit parallel distribut architectur distribut spars linear system slu precondition distribut spars linear system collect set equat assign differ processor parallel solut spars linear system begin partit adjac graph coeffici matrix a base result partit data distribut processor pair equationsunknown assign processor type distribut matrix data structur base subdomain decomposit concept propos also see base concept matrix assign processor unknown processor divid three type interior unknown coupl local equat local interfac unknown coupl nonloc extern local equat extern interfac unknown belong subdomain coupl local equat submatrix assign certain processor say processor i split two part local matrix act local variabl interfac matrix x act extern variabl accordingli local equat given processor written local matrix reorder way interfac point list last interior point local system written block format n indic subdomain neighbor refer subdomain i exactli set processor refer processor need commun receiv inform part product x iext reflect contribut local equat neighbor subdomain j sum contribut result multipli x extern interfac unknown ie precondition built upon distribut data structur origin matrix form approxim global schur complement explicitli domain decomposit base precondition exploit simplest one addit schwarz procedur form block jacobi bj iter block refer submatric associ subdomain ie even though construct easili block jacobi precondit robust ineffici compar schur complement type precondition one best among schur complement base precondition slu distribut approxim schur lu precondition precondit global matrix defin term block lu factor involv solv global schur complement system precondit step incomplet lu factor use slu approxim local schur complement numer result report show schur ilu precondition demonstr superior scalabl perform block jacobi precondition effici latter term parallel run time class two level block precondit techniqu pbilu two level block ilu precondition base bilutm techniqu describ note befor bilutm offer good parallel robust due larg size block independ set graph partition bilutm greedi algorithm find block independ set distribut matrix base block independ set implement block size block independ set must given search algorithm start choic block size k base problem size densiti coeffici matrix a choic k may also depend upon number avail processor assum block independ set uniform block size k found coeffici matrix permut block form small independ block divid sever group accord number avail processor sake load balanc processor group hold approxim number independ block the number independ block differ group may differ time global vector unknown x split two subvector right hand side vector b also conform split subvector f g reorder lead block systemb bm fm um number processor use comput block diagon contain sever independ block note submatrix f row number block submatrix b submatric e c also divid part accord load balanc criterion order approxim amount load processor e c also row number submatric assign processor i u local part unknown vector f g local part right hand side vector partit assign certain processor time matrix distribut processordata assign done processor hold sever row equat local system equat processor written as u part unknown subvector act submatric anoth part unknown subvector act f c onli b act complet local vector u take u local unknown complet interior vector precondition base type block independ set order domain decomposit differ straightforward domain decomposit base rowwis strip partit use obviou differ partit action f complet local local howev sinc natur submatric differ two decomposit strategi easi say one better stage deriv schur complement techniqu key idea domain decomposit techniqu develop precondition global system exploit method approxim solv schur complement system parallel parallel construct pbilu precondition base block independ set domain decomposit comput approxim global schur complement describ deriv global schur complement anoth part coeffici matrix need partit sent certain processor rewrit reorder coeffici matrix system bm fm thu two way partit submatrix e one partit e row column submatric also assign processor i number column block diagon submatrix number row submatrix c remark clear potenti confus two represent submatrix e row partit e use repres local matrix form differ local matrix kept throughout comput process column partit e conveni comput schur complement matrix parallel column partit e kept construct schur complement matrix case submatrix e small highli spars b larg consid block lu factor form global schur complement now suppos invert b mean rewrit equat as fmc processor comput one compon sum independ partit row submatrix part row must conform submatrix c m scatter processor there global commun need scatter final local part schur complement matrix construct independ processor simplest implement approach construct distribut schur complement matrix incomplet lu factor use parallel block restrict ikj version gaussian elimin similar sequenti algorithm use bilutm method decreas commun among processor offer flexibl control amount fillin ilu factor parallel restrict gaussian elimin bilutm high accuraci precondition base incomplet lu factor util dual drop strategi ilut control comput storag memori cost implement base restrict ikj version gaussian elimi nation discuss detail remain part subsect outlin parallel implement restrict ikj version gaussian elimin use distribut data structur discuss previou subsect ith processor local submatrix form base submatric assign processor ilu factor local matrix perform local matrix processor look like pbilu local matrix mean store local processor i necessarili mean act interior unknown note submatrix c size submatrix c equat submatrix element correspond nonzero entri submatrix may zero other zero element recal permut matrix left hand side let submatric c submatrix c obtain perform restrict gaussian elimin local matrix slightli differ elimin procedur first perform ilu factor gaussian elimin upper part local matrix ie submatrix b f continu gaussian elimin lower part m elimin perform respect nonzero and accept fillin entri submatrix entri modifi accordingli perform oper lower part upper part matrix access modifi see figur f process access process access access modifi access modifi access modifi figur illustr restrict ikj version gaussian elimin submatric respect equat done three kind submatric form use later iter upper part matrix upper part gaussian elimin ub l gamma upper part matrix also perform ilu factor block diagon submatrix b b lb thu extract submatric lb ub upper part factor local matrix later use restrict factor lower part obtain new reduc submatrix repres c form piec global schur complement matrix fact submatrix note b gamma f factor matrix l gamma f alreadi avail factor upper part comput processor first solv auxiliari matrix q ub follow matrixmatrix multipl q howev part comput done implicitli restrict ikj gaussian elimin process sens comput construct piec schur complement matrix s processor i done restrict ilu factor lower part local matrix word c form without explicit linear system solv matrixmatrix multipl detail comput procedur see consid equat schur complement comput rewritten form of comput done parallel thank block diagon structur b gaussian elimin exact factor global schur complement matrix form sum submatric togeth submatrix part use partit origin submatrix c correspond part scatter relev processor receiv sum part submatric scatter differ processor local schur complement matrix form local mean row global schur complement held given processor note remark restrict ikj gaussian elimin yield block ilu factor local matrix form howev submatric l gamma longer need later comput discard strategi save consider storag space differ current implement slu psparslib librari induc global precondition possibl develop precondition global system exploit method approxim solv reduc system techniqu base reorder global system two two block form consid block lu factor equat block factor matrix precondit approxim lu factor approxim global schur complement matrix s form therefor global precondit oper induc schur complement solv equival solv lu f forward solv l backward substitut u comput procedur would consist follow three step with g use auxiliari comput schur complement right hand side approxim solv reduc system back substitut u variabl ie solv step comput parallel processor commun boundari inform exchang among processor matrix partit approach differ one use need commun among processor comput global schur complement right hand side g processor easi see mc b mc gammab local schur complement right hand side comput way rewrit approxim reduc schur complement system submatrix x ij boundari matrix act extern variabl numer way solv reduc system one option consid start replac approxim system form local approxim local schur complement matrix formul view block jacobi precondit version schur complement system system solv iter acceler gmre requir solv step current implement ilut factor perform purpos block jacobi precondit third step schur complement precondit perform without problem sinc b block diagon solut comput parallel iter step processor i y actual solv lb y factor lb avail need exchang boundari inform among processor sinc compon requir f processor i numer experi numer experi compar perform previous describ pbilu precondition distribut schur complement lu slu precondition solv spars matric discret two dimension convect diffus problem applic problem comput fluid dynam comput carri processor mhz subcomplex processor hp exemplar xclass supercomput univers kentucki super node interconnect high speed low latenc network super node processor attach it supercomput total gb share memori theoret oper speed gflop use mpi librari interprocessor commun major part code mainli written fortran program languag c routin handl dynam alloc memori mani commun subroutin slu precondition code taken psparslib librari tabl contain numer result n denot dimens matrix nnz repres number nonzero spars matrix np number processor use iter number precondit fgmre iter outer it erat ftime cpu time second precondit solut process fgmre ptime total cpu time second solv given spars matrix start initi distribut matrix data processor master processor processor ptime includ graph partit time initi permut time associ partit done sequenti processor thu ptime includ matrix distribut local reorder precondition construct iter process time ftime sratio stand sparsiti ratio ratio number nonzero precondition number nonzero origin matrix a k block size use pbilu p number nonzero allow l u factor ilu factor drop toler p mean use saad ilut precondition use flexibl variant restart gmre fgmre solv origin linear system sinc acceler permit chang precondit oper step current case sinc use iter process approxim solv schur complement matrix outer fgmre iter size krylov subspac set linear system form assum exact solut vector unit initi guess random vector compon converg achiev norm residu approxim solut reduc order magnitud use innerout iter process maximum number outer precondit fgmre iter inner iter solv schur complement system use gmre without restart block jacobi type precondition inner iter stop norm residu inner iter reduc number inner iter greater point point matric first compar parallel perform differ precondition solv point point matric point point matric gener discret follow convect diffus equat two dimension unit squar socal reynold number convect coeffici chosen px expgammaxi right hand side function use sinc gener artifici right hand side spars linear system state abov point matric gener use standard central differ discret scheme point matric gener use fourth order compact differ scheme two type matric use test bilum ilu type precondition comparison result parallel iter solver report cpu time result iter number howev gener difficult make fair comparison two differ precondit algorithm without list resourc cost achiev given result sinc accuraci precondition usual influenc fillin entri kept memori storag cost precondition import indic effici precondition precondition use memori space are gener faster use less memori space good precondition use much memori space still achiev fast converg end report paper number precondit iter parallel cpu time precondit solut process parallel cpu time entir comput process sparsiti ratio first chose point matrix block size chosen drop paramet chosen slu use one level overlap among subdomain suggest test result list tabl found pbilu precondition faster slu precondition solv problem pbilu take smaller number iter converg slu did converg rate pbilu slu strongli affect number processor employ indic good scalabl respect parallel system two precondition moreov pbilu took much less parallel cpu time slu need half memori space consum slu solv matrix see remark explan differ storag space pbilu slu also test matrix smaller valu case report two test case slu one level overlap subdomain nonoverlap subdomain test result list tabl experi found sparsiti ratio pbilu slu measur storag space use store precondition may case storag space slu could releas howev sparsiti ratio slu report articl base slu code distribut psparslib librari version download httpwwwcsumneduresearcharpap sparslibpspabshtml novemb tabl point matrix one level overlap slu precondition np iter ftime ptime sratio pbilu slu pbilu tabl point matrix one level overlap nonoverlap result bracket slu precondition np iter ftime ptime sratio slu overlap nonoverlap subdomain make much differ term parallel run time onli parallel cpu time overlap case report tabl observ agreement made howev overlap version slu converg faster nonoverlap version iron nonoverlap version slightli larger sparsiti ratio storag space precondition primarili determin dual drop paramet p overlap make local submatrix look larger thu reduc sparsiti ratio rel number nonzero coeffici matrix remark pbilu seen converg faster take less parallel run time slu overlap nonoverlap solv point matrix use given paramet sinc cost perform overlap nonoverlap slu close report result overlap version slu remain numer test compar result tabl see higher accuraci pbilu precondition use larger p perform better lower accuraci pbilu term iter count parallel run time higher accuraci one cours take memori space store slu precondition overlap test compar precondition bj block jacobi si pure schur complement iter sapinv distribut approxim block lu factor spars approxim etc numer experi show slu retain superior perform bj si precondition compar schur complement precondit with local schur complement invert sapinv how ever parallel pbilu precondition shown effici slu explain commun cost pbilu experiment data correspond numer result tabl exampl total parallel comput time ptime second pbilu commun time construct schur complement matrix second commun construct pbilu case cost total parallel comput time total parallel comput time ptime second commun time second commun time construct pbilu total parallel comput time cost commun construct pbilu precondition high also use larger n vari gener larger point point matric comparison result given tabl result compar result list tabl howev parallel run time ptime slu tabl increas dramat more tripl number processor increas result point matrix given tabl again see pbilu perform much better slu furthermor scalabl slu degener test problem number iter processor use increas processor use pbilu precondition number iter almost constant number processor increas larg ptime result slu especi distribut larg amount data given parallel comput use partit strategi slu may present problem anoth set test run solv point matrix parallel iter time ftime respect differ number processor pbilu slu plot figur again pbilu solv point matrix faster slu did figur number precondit fgmre iter pbilu slu compar respect number tabl point matrix gamma one level overlap slu precondition np iter ftime ptime sratio slu pbilu slu tabl point matrix gamma one level overlap slu precondition np iter ftime ptime sratio tabl point matrix one level overlap slu precondition np iter stime ptime sratio k slu slu time number processor dash line slu iter solid line pbilu iter figur comparison parallel iter time ftime pbilu slu precondition solv point matrix paramet use processor employ solv point matrix figur indic converg rate pbilu improv number processor increas converg rate slu deterior number processor increas summar comparison result subsect point point matric finit differ discret convect diffus problem test seen pbilu need less half storag space requir slu paramet chosen compar storag space consum slu pbilu still outperform slu faster converg rate less parallel run time meanwhil see number processor increas parallel cpu time decreas number iter affect significantli pbilu fidap matric set test matric extract test problem provid fidap packag mani matric small zero diagon difficult solv standard ilu precondition test fidap matric precondition found pbilu solv twice mani fidap matric slu doe test pbilu solv fidap matric slu solv test show parallel two level block ilu precondition matric avail onlin matrixmarket nation institut standard technolog httpmathnistgovmatrixmarket number number processor dash line slu iter solid line pbilu iter figur comparison number precondit fgmre iter pbilu slu precondition solv point matrix paramet use robust slu precondition approach also shown merit term smaller construct parallel solut cost smaller memori cost smaller number iter compar slu precondition sake breviti list result three repres larg test matric tabl figur note tabl mean precondit iter method converg number iter greater vari paramet fillin p drop toler tabl precondition adjust size block independ set pbilu approach pbilu clearli shown robust slu solv fidap matrix fidap matrix larger fidapm test also adjust fillin drop toler paramet p slu pbilu test result pbilu converg report tabl note small valu requir ilu factor seem difficult slu converg test problem paramet pair list tabl paramet pair test slu result list tabl figur show parallel iter time ftime respect number processor pbilu solv fidap matrix see parallel iter time decreas number processor increas demonstr good speedup solv unstructur gener spars matrix even fidap matric pbilu slu converg pbilu usual show superior perform slu term number iter tabl precondition np p iter ftime ptime sratio tabl fidap matrix precondition np p k iter ftime ptime sratio iter time number processor solid line pbilu iter figur parallel iter time ftime pbilu fidap matrix paramet use pbilu sparsiti ratio approxim tabl flata matrix precondition np k iter ftime ptime sratio tabl flata matrix precondition np k iter sratio sparsiti ratio flat matric flat matric fulli coupl mix finit element discret three dimension navierstok equat flata mean matrix first newton step nonlinear iter element x coordin direct element z coordin direct one element z coordin direct limit comput memori use gener matric explan hold flata matrix use element x coordin direct matric gener keep variabl structur coupl navierstok equat may nonzero entri actual numer zero valu note two matric actual symmetr sinc first newton step veloc vector set zero howev symmetri inform util comput see tabl pbilu abl solv two cfd matric small valu two matric difficult slu converg small sparsiti ratio reflect previou remark two flat matric mani numer zero entri ignor threshold base ilu factor count toward sparsiti ratio calcul matric fulli coupl mix finit element discret navierstok equat notori difficult solv precondit iter method standard ilu type precondition tend fail produc unstabl factor unless flat matric avail second author variabl order properli suitabl order difficult implement sequenti environ seem howev nontrivi task perform analog order parallel environ conclud remark futur work implement parallel two level block ilu precondition base schur complement precondit discuss detail distribut small independ block form subdomain processor gave comput procedur construct distribut schur complement matrix parallel compar parallel precondition pbilu scalabl parallel two level schur lu precondition publish recent numer experi show pbilu demonstr good scalabl solv larg spars linear system parallel comput also found pbilu faster comput effici slu test case pbilu also effici term memori consumpt sinc use less memori space slu achiev better converg rate fidap flat matric test section small zero main diagon entri poor converg perform pbilu slu mainli due instabl associ ilu factor matric diagon threshold strategi employ pbilu exclud row small diagon submatrix b ilu factor stabl parallel implement diagon threshold pbilu investig futur studi plan extend parallel two level block ilu precondition truli parallel multilevel block ilu precondition futur research also plan test parallel precondition emerg high perform comput platform pc cluster r mpi implement spai precondition te parallel nonoverlap domain decomposit algorithm compress fluid flow problem triangul do main comparison domain decomposit ilu precondit iter method nonsymmetr ellipt problem parallel finit element solut threedimension rayleigh benardmarangoni flow parpr parallel precondition packag refer manual version precondit conjug gradient method incompress navierstok equat priori sparsiti pattern parallel spars approxim invers precondi tioner toward cost effect ilu precondition high level fill numer linear algebra highperform comput develop trend parallel solut linear system parallel ilu precondition cfd problem sharedmemori comput fidap exampl manual comput solut larg spars posit definit system parallel precondit approxim invers connect machin singl cell high order scheme convectiondiffus equat variabl coeffici chaco user guid scalabl parallel comput parallel multilevel kway partit scheme irregular graph comparison domain decomposit techniqu ellipt partial differenti equat parallel implement introduct parallel comput direct method spars matric partit spars matric eigenvector graph flexibl innerout precondit gmre algorithm ilut dual threshold incomplet lu precondition parallel spars matrix librari p sparslib iter solver modul iter method spars linear system distribut schur complement techniqu gener spars linear system domain decomposit multilevel type techniqu gener spars linear system design iter solut modul parallel spars matrix librari p sparslib bilum block version multielimin multilevel ilu precondition gener spars linear system bilutm domainbas multilevel block ilut precondition gener spars matric diagon threshold techniqu robust multilevel ilu precondition gener spars linear system enhanc multilevel block ilu precondit strategi gener spars linear system domain decomposit parallel multilevel method ellipt partial differenti equat high perform precondit parallel comput incompress flow materi process numer experi diagon precondit applic spars matrix solver effect precondition multilevel dual reorder strategi robust incomplet lu factor indefinit matric paralleliz precondition base factor spars approxim invers techniqu spars approxim invers parallel precondit spars matric precondit iter method finit differ scheme convectiondiffus precondit krylov subspac method solv nonsymmetr matric cfd applic spars approxim invers multilevel block ilu precondit techniqu gener spars matric perform studi incomplet lu precondition solv linear system fulli coupl mix finit element discret navierstok equat use iter refin solut spars linear system tr comparison domain decomposit techniqu ellipt partial differenti equat parallel implement high perform precondit applic spars matrix solver effect precondition partit spars matric eigenvector graph introduct parallel comput flexibl innerout precondit gmre algorithm toward costeffect ilu precondition highlevel fill domain decomposit parallel comput incompress flow materi process parallel multilevel seri ikiway partit scheme irregular graph develop trend parallel solut linear system precondit iter method finit differ scheme convectiondiffus distribut schur complement techniqu gener spars linear system priori sparsiti pattern parallel spars approxim invers precondition spars approxim invers multilevel block ilu precondit techniqu gener spars matric enhanc multilevel block ilu precondit strategi gener spars linear system scalabl parallel comput numer linear algebra high perform comput comput solut larg spars posit definit multilevel dual reorder strategi robust incomplet lu factor indefinit matric iter method spars linear system ctr chi shen jun zhang fulli parallel block independ set algorithm distribut spars matric parallel comput v n p novemberdecemb jun zhang tong xiao multilevel block incomplet choleski precondition solv normal equat linear least squar problem korean journal comput appli mathemat v n p januari chi shen jun zhang kai wang distribut block independ set algorithm parallel multilevel ilu precondition journal parallel distribut comput v n p march