t analysi comparison two gener spars solver distribut memori comput a paper provid comprehens studi comparison two stateoftheart direct solver larg spars set linear equat largescal distributedmemori comput one multifront solver call mump supernod solver call superlu describ main algorithm featur two solver compar perform characterist respect uniprocessor speed interprocessor commun memori requir solver preorder numer stabil sparsiti play import role achiev high parallel effici analys result variou order algorithm perform analysi base data obtain run processor cray te use set matric real applic also use regular grid problem studi scalabl two solver b introduct consid direct solut spars linear equat distribut memori comput commun messag pass normal use mpi studi detail two stateoftheart solver mump amestoy duff lexcel koster amestoy duff lexcel superlu li demmel first use multifront approach dynam pivot stabil second base supernod techniqu static pivot iter refin discuss detail algorithm use two code section two import factor affect perform code use preprocess preorder matrix diagon entri larg rel offdiagon strategi use comput order row column matrix preserv sparsiti discuss aspect detail section compar perform two code section show comparison fraught difficulti even though author code involv studi section regular grid problem use illustr analys differ two approach origin plan comparison spars code but given difficulti found assess code know well moment shelv ambiti project howev feel lesson learn present exercis invalu us futur wider studi given us insight behaviour spars direct code feel use share wider audienc stage addit valuabl inform compar merit multifront versu supernod approach examin paramet space comparison exercis identifi sever key paramet influenc differ degre two approach test environ throughout paper use set test problem evalu perform algorithm test matric come forthcom rutherfordbo spars matrix collect duff grime lewi industri partner project tim davi collect sparsekit eec depart uc berkeley parasol test matric avail parallab bergen norway two smaller matric garon lnsp includ set matric use section illustr differ numer behaviour two solver note that experi consid symmetr matric test set superlu cannot exploit symmetri unabl produc ldl factor howev sinc test exampl section symmetr web page httpwwwcseclrcacukactivitysparsematric project web page httpwwwciseufledudavisspars web page httpmathnistgovmatrixmarketdatasparskit matrix ecl includ rutherfordbo collect web page httpwwwparallabuibnoparasol real unsymmetr assembl rua matrix name order no entri bbmat rutherfordbo cfd depart uc berkeley garon davi collect cfd lhrc davi collect chem eng rutherfordbo cfd mixtank parasol polyflow sa collect cfd twoton rutherfordbo circuit sim wang rutherfordbo semiconductor tabl test matric strsym number nonzero match nonzero symmetr locat divid total number entri so structur symmetr matrix valu show result symmetr unsymmetr factor version mump matric mixtank invextr modifi outofrang underflow valu matrix file keep sparsiti pattern want replac underflow valu zero instead replac entri expon smaller number mantissa expon linear system righthand side vector gener true solut vector one result present paper obtain cray te dec ev processor mbyte memori per processor peak megaflop rate per processor nersc lawrenc berkeley nation laboratori also refer experi processor ibm sp mhertz processor mbyte physic memori mbyte virtual memori peak megaflop rate per processor gmd bonn germani use parasol project perform characterist two machin list tabl comput cray te ibm sp frequenc processor mhertz mhertz peak uniproc perform mflop mflop effect uniproc perform mflop mflop peak commun bandwidth mbytessec mbytessec latenc bandwidtheffect perform tabl characterist cray te ibm sp factor matrix wang use mump use estim effect uniprocessor perform comput descript algorithm use section briefli describ main characterist algorithm use solver highlight major differ them complet descript algorithm reader consult previou paper author algorithm amestoy et al amestoy et al li demmel li demmel algorithm describ comput tree whose node repres comput whose edg repres transfer data case multifront method mump step gaussian elimin perform dens frontal matrix node schur complement or contribut block remain pass assembl parent node case supernod code superlu distribut memori version use rightlook formul which comput factor block column correspond node tree immedi send data updat block column correspond ancestor tree code accept pivot order builtin capabl gener order base analysi pattern summat perform symbol howev present version mump symbol factor markedli less effici input order given sinc differ logic use case default order use mump approxim minimum degre amd amestoy davi duff a default superlu multipl minimum degre mmd liu howev experi use minimum degre order consid amd order sinc code gener use subroutin mc hsl usual far quicker mmd produc symbol factor close produc mmd also use nest dissect order nd sometim use onmeti order meti karypi kumar sometim nest dissectionhaloamd order scotch pellegrini roman amestoy depend perform better particular problem addit sometim benefici preced order perform unsymmetr permut place larg entri diagon scale matrix diagon modulu one offdiagon modulu less equal one use mc code hsl perform preorder scale duff koster indic clearli done effect use preorder matrix discuss detail section final mc use matric alway scale approach pivot order defin analysi symbol factor stage mump modulu prospect pivot compar largest modulu entri row accept greater threshold valu typic our default valu note that although mump choos pivot diagon largest entri column might unavail pivot stage entri row fulli sum threshold pivot strategi common spars gaussian elimin help avoid excess growth size entri matrix factor directli reduc bound backward error prospect pivot fail test happen kept schur complement pass parent node eventu row entri column avail pivot root befor pivot chosen column thu numer factor respect threshold criterion cost increas size frontal matric caus work fillin forecast superlu approach static pivot strategi use keep pivot sequenc chosen analysi magnitud potenti pivot test threshold ffl jjajj ffl machin precis jjajj norm a less valu immedi set valu with sign modifi entri use pivot correspond halfprecis perturb origin matrix entri result factor exact iter refin may need note that iter refin obtain accur solut case test problem still occur extend precis bla li demmel bailey henri hida iskandar kahan kapur martin tung yoo could use mump main parallel featur parallel within mump two level first use structur assembl tree exploit fact comput node ancestor descend independ initi parallel sourc tree parallel number leaf node reduc one root second level subdivis elimin oper block frontal matrix block give rise node parallel either row refer dnode parallel row column at root refer dnode parallel node parallel depend size frontal matrix which delay pivot known factor time therefor determin dynam tree node assign processor priori subassign block frontal matrix done dynam machin depend paramet mump control effici code design take account uniprocessor multiprocessor characterist comput dynam distribut schedul approach need precis descript perform characterist comput approach base static schedul pastix henon ramet roman machin depend paramet mump associ block size involv parallel block factor algorithm dens frontal matric main object maintain minimum granular effici exploit potenti processor provid suffici task exploit avail parallel target machin differ sever respect import one illustr tabl found smaller granular task could use cray te ibm sp rel faster rate commun megaflop rate cray te ibm sp see tabl say commun rel effici cray te dynam schedul major origin featur approach use mump critic part algorithm process associ tree node decid reassign work correspond partit row set socal worker process call node onedimension parallel node earlier version mump fix block size use partit row work distribut process start least load process the load process determin amount work number oper alloc yet process determin cheapli sinc block size fix possibl process charg onedimension parallel node give addit work process alreadi load itself happen near leaf node tree sparsiti provid enough parallel keep process busi hand insuffici task might creat provid work idl process situat like occur close root tree new algorithm avail sinc version mump block size onedimension partit dynam adjust process charg node earli process tree that is near leav give rel bigger block size reduc number worker process wherea close root tree block size automat reduc compens lack parallel assembl tree bound block size partit onedimension parallel node interv lower bound need maintain minimum task granular control volum messag upper bound interv less critic it default chosen eight time lower bound use estim maximum size commun buffer factor larg all dynam strategi partit distribut work onto processor could caus troubl larg number processor more case quit benefici take account global inform help local decis exampl one could restrict choic worker process set candid processor determin static analysi phase notion commonli use design static schedul algorithm henon et al could reduc overhead dynam schedul algorithm reduc increas commun volum increas number processor improv local decis tune paramet control block size partit would easier estim memori requir factor would accur larg number processor perform softwar improv could thu expect featur avail current version mump implement futur releas see ad featur one could address current limit mump approach see section solut phase also perform parallel use asynchron commun forward elimin back substitut case forward elimin tree process leav root similar way factor back substitut requir differ algorithm process tree root leav pool readytobeactiv task use chang distribut factor gener factor phase henc type node parallel also use solut phase superlu main parallel featur superlu also use two level parallel although advantag taken node parallel block supernod pivot order fulli determin analysi phase assign block processor done static priori factor commenc blockcycl layout use execut pipelin sinc sequenc predetermin matrix partit base notion unsymmetr supernod introduc demmel eisenstat gilbert li liu supernod defin matrix factor l supernod rang column l triangular block diagon full nonzero structur elsewher thi either full zero supernod partit use block partit row column dimens diagon block squar n supernod nbyn matrix n block nonuniform size figur illustr block partit offdiagon block may rectangular need full furthermor column block u necessarili row structur call dens subvector block u segment p process also arrang mesh dimens blockcycl layout mean block i j of l u map onto process coordin process mesh factor block li j need process process row similarli block ui j need process process column partit map control user first user set maximum block size paramet symbol factor algorithm identifi supernod chop larg supernod smaller one size exceed paramet supernod may smaller paramet due sparsiti block defin supernod boundari that is supernod smaller maximum block size never larger experi shown good valu paramet ibm sp around cray te around second user set shape process grid theta theta squar grid better perform expect rule thumb use cray te defin grid shape global matrixprocess mesh figur blockcycl layout use superlu map block column l resid one process name column process exampl figur second block column l resid column process f g process own two nonzero block contigu global matrix main numer kernel involv numer factor block updat correspond rankk updat schur complement see figur earlier version superlu comput base level bla is call level bla routin gemv matrixvector product multipl vector segment matrix li k kept cach across multipl call extent mimic level bla gemm matrixmatrix product perform howev differ level level still quit larg mani machin eg ibm sp motiv us modifi kernel follow way order use level bla best perform distinguish two case correspond two shape ukj block ffl segment ukj height shown figur a sinc nonzero segment store contigu memori call gemm directli without perform oper zero ffl segment ukj differ height shown figur b case first copi segment temporari work array lead zero pad necessari call gemm use li k instead ukj perform extra floatingpoint oper pad zero copi incur run time cost data must load cach anyway work storag bound maximum block size tunabl paramet exampl usual use theta ibm sp theta cray te depend matrix level bla kernel improv uniprocessor factor time ibm sp perform gain also observ cray te clear extra oper well offset benefit effici level bla routin b uk ai j li k uk j a uk figur illustr numer kernel use superlu current factor algorithm two limit parallel explain exampl problem specul algorithm may improv futur follow matrix notat zero block left blank nonzero block mark box process own block ffl parallel sparsiti consid matrix by block map onto by process mesh although node parent node elimin tree associ a process column depend column process depend l block process process could start factor column time process factor column process start factor column current algorithm requir column process factor column synchron therebi introduc idl time process relax constraint allow diagon process case factor diagon block send factor block offdiagon process use mpi isend even offdiagon process readi column would elimin artifici interprocess depend potenti reduc length critic path note kind independ come sparsiti also processtomatrix map even interest studi would formal task depend task graph perform optim schedul it ffl parallel direct acycl elimin graph gilbert liu often refer elimin dag edag consid anoth matrix by block map onto by process mesh column independ elimin dag column process set f g f g could start factor column simultan howev sinc process also involv updat task block associ step algorithm give preced task step task step process factor column immedi may chang task preced give factor task later step higher prioriti updat task previou step former like critic path would exploit better task independ come elimin dag expect improv larg impact spars andor unsymmetr matric order give wide bushi elimin tree nest dissect triangular solut algorithm also design around distribut data structur forward substitut proce bottom elimin tree root wherea back substitut proce root bottom algorithm base sequenti variant call inner product formul execut program complet messagedriven process selfschedul loop perform appropri local comput depend type messag receiv entir asynchron approach enabl larg overlap commun comput help overcom much higher commun comput ratio phase first comment algorithm differ approach use level bla perform elimin oper howev mump frontal matric alway squar possibl zero frontal matrix especi delay pivot matrix structur markedli asymmetr present implement take advantag sparsiti count measur assum frontal matrix dens shown amestoy puglisi one detect exploit structur asymmetri frontal matric new algorithm signific gain memori time perform factor obtain exampl use mump new algorithm number oper factor matric lhrc twoton would reduc respect approach test share memori multifront code amestoy duff hsl howev yet avail current version mump superlu advantag taken sparsiti block usual dens matrix block smaller use mump addit superlu use sophist data structur keep track irregular sparsiti thu uniprocessor megaflop rate superlu much wors mump perform penalti extent allevi reduct floatingpoint oper better exploit sparsiti rule thumb mump tend perform particularli well matrix structur close symmetr superlu better exploit asymmetri note that even order input two code comput tree gener case differ case mump assembl tree gener mc use drive mump factor phase while superlu direct acycl comput graph dag built implicitli figur use vampir trace nagel arnold weber hopp solchenbach illustr typic parallel behaviour approach trace correspond zoom middl factor phase matrix bbmat processor cray te black area correspond time spent commun relat mpi call line two process correspond one messag transfer plot see superlu distinct phase local comput interprocess commun wherea mump hard distinguish process perform comput transfer messag due asynchron schedul algorithm use mump may better chanc overlap commun comput impact preprocess numer issu section first studi impact solver preprocess matrix preprocess first use row column permut permut larg entri onto diagon section report compar structur numer impact preprocess phase perform accuraci solver phase symmetr order minimum degre nest dissect use studi rel influenc order perform solver section also comment rel cost analysi phase two solver use preorder place larg entri onto diagon cost analysi phase duff koster develop algorithm permut spars matrix diagon entri larg rel offdiagon entri also written comput code mc avail hsl implement algorithm here use option mc maxim product modulu diagon entri scale permut matrix diagon entri modulu one offdiagon modulu less equal one import preorder scale clear mump limit amount numer pivot factor increas overal cost factor superlu expect permut even crucial reduc amount small pivot modifi set jjajj mc code duff koster quit effici normal requir littl time rel matrix factor even latter execut mani processor mc run one processor result section show alway case moreov matric unsymmetr symmetr nearli symmetr structur common problem class problem mc perform unsymmetr permut tend destroy symmetri pattern sinc code use symmetr pattern sparsiti order see section mump use one also symbol numer factor overhead markedli unsymmetr pattern high convers initi matrix unsymmetr as exampl lhrc unsymmetr permut may actual help increas structur symmetri thu give second benefit subsequ matrix factor show effect use mc exampl tabl tabl illustr rel cost main step analysi phase mc use preprocess matrix see tabl that unsymmetr matric lhrc twoton mc realli need mump superlu factor matric effici matric zero diagon static pivot approach use superlu unless zero made nonzero fillin larg enough perturb process process process process process process process process mpi applic s s s s figur illustr asynchron behaviour mump factor phase process process process process process process process process mpi vt_api comm s s s figur illustr rel synchron behaviour superlu factor phase matrix solver order strsym nonzero flop factor bbmat mump amd fidapm mump amd garon mump amd mixtank mump amd twoton mump amd wang mump amd tabl impact permut larg entri onto diagon use mc size factor number oper estim given analysi not enough memori perform factor strsym denot structur symmetri order factor factor nearbi matrix obtain case mump dramat higher fillin obtain without mc make also necessari use mc mump main benefit use mc structur numer permut matrix fact larger structur symmetri see column tabl symmetr permut obtain permut matrix effici preserv sparsiti superlu benefit similar way symmetr comput symmetr permut base assumpt even superlu preserv better asymmetr structur factor perform symbol analysi direct acycl graph exploit asymmetri factor phase compar exampl result mump superlu matric lhrc mixtank twoton matrix iter bbmat tabl illustr converg iter refin use mc also improv qualiti factor numer behaviour factor phase reduc number step iter refin requir reduc backward error machin precis illustr tabl show number step iter refin requir reduc componentwis rel backward error arioli demmel duff machin precis theta gamma cray te iter refin stop either requir accuraci reach converg rate slow berr decreas least factor two true error report jjx true jj tabl illustr impact use mc qualiti matrix solver without iter ref iter refin berr bbmat mump e e e e lhrc mump enough memori superlu enough memori mixtank mump e e e e twoton mump e e e e matrix solver without iter ref iter refin berr bbmat mump e e e e lhrc mump e e e e mixtank mump e e e e twoton mump e e e e tabl comparison numer behaviour backward error berr forward error err solver nb indic number step iter refin initi solut obtain solver prior iter refin addit show that thank numer partial pivot initi solut almost alway accur mump superlu usual markedli so observ confirm larger number test matric tabl stop criterion appli run run tabl case mump mc also result reduct number offdiagon pivot number delay pivot exampl matrix invextr number offdiagon pivot drop number delay pivot drop one also see tabl eg bbmat mc alway improv numer accuraci solut obtain superlu expect see that matric fairli symmetr pattern eg matrix fidapm tabl use mc lead signific decreas symmetri which solver result signific increas number oper factor addit recollect time spent mc domin analysi time either solver see tabl even matric fidapm invextr provid gain subsequ step thu solver default use mc fairli symmetr matric practic default option mump packag mc automat invok structur symmetri found less superlu zero diagon numer issu must also consid automat decis analysi phase difficult final compar figur time spent two solver analysi phase reorder base amd mc invok sinc time spent bbmat ecl invextr fidapm mixtank rma wangsecond mump figur time comparison analysi phase mump superlu mc preprocess use amd order use amd similar case give good estim cost differ matrix solver preprocess total mc amd bbmat mump amd mcamd mixtank mump amd twoton mump amd tabl influenc permut larg entri onto diagon use mc time in second analysi phase mump superlu analysi phase two solver note superlu current tie specif order code take advantag inform avail order algorithm tighter coupl order case mump amd reduc analysi time superlu howev analysi phase superlu asymmetr structur need factor comput direct acycl graph gilbert liu unsymmetr matrix must built map onto processor mump main data structur handl analysi assembl tree produc directli byproduct order phase data structur introduc phase dynam schedul use factor simpl massag tree partial map comput task onto processor perform analysi use order preserv sparsiti matric mc use show tabl impact choic symmetr permut fillin floatingpoint oper factor observ amestoy et al use nest dissect significantli improv perform mump see superlu also although lesser extent benefit use nest dissect order examin influenc order perform section also notic that order superlu exploit asymmetri matrix somewhat better mump see bbmat structur symmetri expect asymmetri problem better exploit mump approach describ amestoy puglisi implement matrix order solver nz lu flop bbmat amd mump nd mump nd mump nd mump mixtank amd mump nd mump tabl influenc symmetr sparsiti order fillin floatingpoint oper factor unsymmetr matric mc use perform analysi gener matric perform numer phase section compar perform studi behaviour numer phase factor solv two solver sake clariti report result best in term factor time sparsiti order approach best order mump differ superlu result order provid mean result nest dissect minimum degre order given illustr differ sensit code choic order note that even order given solver usual perform number oper gener superlu perform fewer oper mump exploit better asymmetri matrix although execut time less mump level bla effect although result often matrix depend tri much possibl identifi gener properti two solver point maximum dimens unsymmetr test matric see tabl studi factor phase show tabl factor time solver smaller matric report tabl result processor observ mump usual faster superlu significantli small number processor believ two reason first mump handl symmetr regular data structur better superlu mump use level bla kernel bigger block use within superlu result megaflop rate mump one processor averag twice superlu factor also evid result smaller test problem tabl result grid problem section note that even matrix twoton perform three time fewer oper mump mump time faster superlu four processor small number processor also notic superlu alway fulli benefit reduct number oper due use nest dissect order see bbmat superlu use processor furthermor one notic that matric structur asymmetr superlu much less scalabl mump exampl matrix lhrc tabl speedup obtain superlu mump respect due two parallel limit current superlu algorithm describ section first superlu fulli exploit parallel elimin dag second pipelin mechan fulli benefit sparsiti factor a block column factor implement also explain superlu fulli benefit case mump better balanc tree gener nest dissect order see order significantli influenc perform code see result matric bbmat ecl and particular mump gener outperform superlu even larg number processor nest dissect order use hand use minimum degre order superlu faster mump larg number processor also see that unsymmetr problem neither solver provid enough parallel benefit use processor except matrix ecl use amd order requir flop factor superlu continu decreas factor time processor lack larg unsymmetr system give us data point regim one might expect that independ order distribut use superlu provid better scalabl and henc eventu better perform larg number processor mix distribut use mump analys scalabl solver consid three dimension regular grid problem section matrix ord solver number processor bbmat amd mump superlu mixtank nd mump twoton mc mump tabl factor time in second larg test matric cray te indic enough memori matrix order solver number processor fidapm amd mump lhrc mcamd mump rma amd mump wang amd mump tabl factor time in second small test matric cray te indic enough memori better understand perform differ observ tabl identifi main characterist solver show tabl averag commun volum speed commun depend much number size messag also indic maximum size messag averag number messag overlap commun comput mump use fulli asynchron commun dure send receiv use nonblock send synchron schedul approach use superlu also enabl overlap commun comput matrix ord solver number processor max vol mess max vol mess max vol mess bbmat amd mump nd mump fidapm amd mump mixtank nd mump twoton mc mump tabl maximum size messag max mbyte averag volum commun vol mbyte number messag per processor mess larg matric factor result tabl difficult make definit comment averag volum commun overal broadli compar sometim mump sometim superlu lower volum occasion signific amount howev although averag volum messag processor compar solver one two order magnitud differ averag number messag therefor averag size messag due much larger number messag involv fanout approach superlu compar multifront approach mump note that mump number messag includ messag one integ requir dynam schedul algorithm updat load process averag volum commun per processor solver depend much number processor while superlu increas number processor gener decreas commun volum per processor alway case mump note ad global inform local dynam schedul algorithm mump help increas granular level node subtask without lose parallel see section thu result decreas averag volum commun larg number processor studi solv phase alreadi discuss section differ numer behaviour two solver show that gener superlu involv step iter refin mump obtain accuraci solut section focu time spent obtain solut appli enough step iter refin ensur componentwis rel backward error berr less p gamma step iter refin involv forward backward solv also matrixvector product origin matrix mump user provid input matrix gener distribut format amestoy et al function use parallel matrixvector product superlu parallel matrixvector product easier input matrix duplic processor tabl report time perform one solut step use factor matrix solv necessari berr greater p time improv solut use iter refin line ir superlu except ecl mixtank requir iter refin one step iter refin requir alway enough reduc backward error mump iter refin requir matrix invextr backward error alreadi close on one processor processor step iter refin requir berr initi solut alreadi equal case time report row ir correspond time perform comput backward error first observ compar exampl tabl that small number processor less solv phase almost two order magnitud less costli factor larg number processor solv phase rel less scalabl factor phase differ drop one order magnitud applic larg number solv might requir per factor could becom critic perform might address futur show solut time smaller matric tabl run iter refin perform report tabl would appear suggest regular structur matrix factor gener factor phase mump respons faster solv phase superlu processor processor solv phase superlu sometim faster mump although case fastest solv time record mump usual fewer number processor cost iter refin significantli increas cost obtain solut superlu static pivot like iter refin requir obtain accur solut numer difficult matric see bbmat twoton mump use partial pivot factor reduc number matric iter refin requir in set invextr requir iter refin solver use mc preprocess matrix also consid reduc number step iter refin even avoid need use case see section matrix order solver number processor bbmat amd mump twoton mc mump tabl solv time in second larg matric cray te show time spent improv initi solut use iter refin indic enough memori matrix ord solver number processor tabl solv time in second small matric cray te memori usag section studi memori use factor function solver use number processor see tabl want first point that dynam schedul approach threshold pivot use mump analysi phase cannot fulli predict space requir processor upper bound therefor use memori alloc static task map approach use superlu memori use predict analysi phase section compar memori actual use solver factor phase includ real integ commun buffer storag initi matrix is howev includ seen amestoy et al input matrix also provid gener distribut format handl effici solver option avail mump superlu initi matrix current duplic processor matrix order solver number processor avg max avg max avg max bbmat amd mump nd mump nd mump mixtank nd mump twoton mc mump tabl memori use factor in megabyt per processor notic tabl signific reduct memori requir increas number processor also see that gener superlu usual requir less memori mump although less appar mani processor use show better memori scalabl mump one observ littl differ mump note storag report still includ anoth intern copi initi matrix distribut arrowhead form necessari assembl oper multifront algorithm averag maximum memori usag show algorithm well balanc superlu better two note memori scalabl critic global address platform parallel increas total memori use pure distribut machin te main factor remain memori use per processor allow larg problem solv enough processor avail perform analysi d grid problem analys understand scalabl solver report section result obtain point discret laplacian oper threedimension nx ny nz grid problem consid set cubic nxnynz rectangular nx nx nx grid nest dissect order use size grid use number oper time report tabl increas number processor tri much possibl maintain constant number oper per processor keep much possibl shape grid possibl satisfi constraint thu number oper per processor complet constant nproc grid size ldl factor lu factor flop time flop time flop time cubic grid nest dissect rectangular grid nest dissect tabl factor time in second cray te lu factor perform mumpsun superlu ldl mumpssym sinc test matric symmetr use mump comput either ldl factor refer mumpssym lu factor refer mumpsun comput lu factor note that given matrix unsymmetr solver superlu mumpsun perform roughli twice mani oper mumpssym overcom problem number oper per processor nonconst first report figur megaflop rate per processor three approach cubic rectangular grid respect context megaflop rate meaning grid problem number oper almost ident mumpsun superlu see tabl thu correspond absolut perform approach use given problem first notic processor independ grid shape mumpsun twice fast superlu also much higher megaflop rate mumpssym processor rectangular cubic grid three solver similar megaflop rate per processor figur show parallel effici cubic rectangular grid respect effici solver p processor comput ratio megaflop rate per processor p processor megaflop rate processor term effici superlu gener effici cubic grid mumpsun even rel small number processor mumpssym rel effici mumpsun mumpssym effici compar superlu larg number processor superlu significantli effici mumpsun peak ratio method reach cubic grid processor superlu three two time effici mumpsun mumpssym respect final report tabl quantit evalu overhead due parallel cubic grid use analysi tool vampir nagel et al row comput report percentag time spent numer factor mpi call idl time due commun synchron report row overhead tabl nproc grid size mumpssym mumpsun superlunx comput overhead nx comput overhead nx comput overhead tabl percentag factor time cubic grid spent comput overhead due commun synchron tabl show superlu less overhead either version mump also observ better parallel behaviour mumpssym respect mumpsun analys processor rate mumpssym mumpsun figur megaflop rate per processor cubic grid nest dissect processor rate mumpssym mumpsun figur megaflop rate per processor rectangular grid nest dissect processor effici mumpssym mumpsun figur parallel effici cubic grid nest dissect processor effici mumpssym mumpsun figur parallel effici rectangular grid nest dissect amestoy et al mainli due fact node level parallel provid rel parallel symmetr context conclud remark paper present detail analysi comparison two stateofth art parallel spars direct solversa multifront solver mump supernod solver superlu analysi base experi use massiv parallel distributedmemori machineth cray te dozen matric differ applic analysi address effici solver mani respect includ role preorder step cost accuraci solut sparsiti preserv total memori requir amount interprocessor commun time factor triangular solv scalabl found solver strength weak summar observ follow ffl solver benefit numer preorder scheme implement mc although superlu benefit greater extent mump mump help reduc number offdiagon pivot number delay pivot superlu may reduc need small diagon perturb number iter refin howev sinc permut asymmetr may destroy structur symmetri origin matrix caus fillin oper tend introduc greater perform penalti mump superlu although recent work amestoy puglisi might affect conclus default mump use mc fairli symmetr matric ffl mump usual provid better initi solut due effect dynam versu static pivot one step iter refin superlu usual obtain solut level accuraci ffl solver accept input fillin reduc order appli symmetr row column mump perform better nest dissect minimum degre exploit better tree parallel provid nest dissect order wherea superlu exploit level parallel parallel effici less sensit differ order ffl given order superlu preserv sparsiti asymmetri l u factor better superlu usual requir less memori mump smaller number processor processor mump requir memori averag ffl although total volum commun compar solver mump requir mani fewer messag especi larg number processor differ two order magnitud partli intrins algorithm multifront versu fanout partli due mump versu superlu matrix partit ffl mump usual faster factor solv phase speed penalti partli come code complex order preserv irregular sparsiti pattern partli due commun messag processor superlu show better scalabl partit scheme better job keep processor busi despit fact introduc messag said introduct start exercis intent compar wider rang spars code howev demonstr preced section task conduct comparison complex feel though experi gain task use extend comparison futur follow tabl summar major characterist parallel spars direct code awar clear descript term use tabl given heath ng peyton code techniqu scope avail refer capss multifront spd wwwnetliborgscalapack heath raghavan mump multifront symun wwwenseeihtfrapomump amestoy et al pastix fanin spd see caption x henon et al pspase multifront spd wwwcsumnedumjoshipspas gupta karypi kumar spool fanin symun wwwnetliborglinalgspool ashcraft grime superlu fanout un wwwnerscgovxiaoyesuperlu li demmel s fanout un wwwcsucsbeduresearch fu jiao yang wsmp z multifront sym ibm product gupta tabl distribut memori code x wwwdeptinfolabriubordeauxfrrametpastix use qr storag static accommod lu fillin z object code ibm avail numer pivot perform code techniqu scope avail refer gspar interpret un grund borchardt grund horn multifront un wwwcseclrcacukactivityhsl amestoy duff multifront qr rect wwwcseclrcacukactivityhsl amestoy duff puglisi b panelllt leftlook spd ng ng peyton pardiso leftright look un schenk schenk gartner fichtner psldlt leftlook spd sgi product rothberg psldu leftlook un sgi product rothberg leftlook un wwwnerscgovxiaoyesuperlu demmel et al tabl share memori code object code sgi avail acknowledg want thank jame demmel jacko koster rich vuduc help discuss grate chiara puglisi comment earli version articl help present also want thank john reid comment first version paper r unsymmetr multifront lu factor fulli asynchron multifront solver use distribut dynam schedul spool objectori spars matrix librari parallel numer method larg system differentialalgebra equat industri applic algorithm permut larg entri diagon spars matrix appear siam journal matrix analysi applic wsmp watson spars matrix packag part direct solut symmetr spars system version httpwww map schedul algorithm parallel spars fanin numer factor httpwww make spars gaussian elimin scalabl static pivot scalabl spars direct solver use static pivot hybrid nest dissect halo approxim minimum degre effici spars matrix order effici spars choleski factor distributedmemori multiprocessor tr parallel algorithm spars linear system elimin structur unsymmetr spars italicluital factor supernod choleski factor algorithm sharedmemori multiprocessor modif minimumdegre algorithm multipl elimin approxim minimum degre order algorithm highli scalabl parallel algorithm spars matrix factor effici spars lu factor partial pivot distribut memori architectur supernod approach spars partial pivot design use algorithm permut larg entri diagon spars matric asynchron parallel supernod algorithm spars gaussian elimin make spars gaussian elimin scalabl static pivot precondit highli indefinit nonsymmetr matric algorithm permut larg entri diagon spars matrix fulli asynchron multifront solver use distribut dynam schedul map schedul algorithm parallel spars fanin numer factor ctr laura grigori xiaoy s li new schedul algorithm parallel spars lu factor static pivot proceed acmiee confer supercomput p novemb baltimor maryland mark baertschi xiaoy li solut threebodi problem quantum mechan use spars linear algebra parallel comput proceed acmiee confer supercomput cdrom p novemb denver colorado olaf schenk klau grtner solv unsymmetr spars system linear equat pardiso futur gener comput system v n p april patrick r amestoy iain s duff jeanyv lexcel xiaoy s li impact implement mpi pointtopoint commun perform two gener spars solver parallel comput v n p juli xiaoy s li jame w demmel superlu_dist scalabl distributedmemori spars direct solver unsymmetr linear system acm transact mathemat softwar tom v n p june anshul gupta recent advanc direct method solv unsymmetr spars system linear equat acm transact mathemat softwar tom v n p septemb timothi a davi column preorder strategi unsymmetricpattern multifront method acm transact mathemat softwar tom v n p june patrick r amestoy iain s duff stphane pralet christof vmel adapt parallel spars direct solver architectur cluster smp parallel comput v n p novemberdecemb