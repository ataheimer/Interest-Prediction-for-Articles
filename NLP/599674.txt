t pac analogu perceptron winnow via boost margin a describ novel famili pac model algorithm learn linear threshold function new algorithm work boost simpl weak learner exhibit sampl complex bound remark similar known onlin algorithm perceptron winnow thu suggest wellstudi onlin algorithm sens correspond instanc boost show new algorithm view natur pac analogu onlin pnorm algorithm recent studi grove littleston schuurman proceed tenth annual confer comput learn theori pp gentil littleston proceed twelfth annual confer comput learn theori pp special case algorithm take p equal p equal obtain natur boostingbas pac analogu perceptron winnow respect p equal case algorithm also view gener with improv sampl complex bound jackson craven pacmodel boostingbas algorithm learn spars perceptron jackson craven advanc neural inform process system mit press analysi gener error new algorithm reli techniqu theori larg margin classif b introduct one fundament problem comput learn theori learn unknown linear threshold function label exampl mani differ learn algorithm problem consid past sever decad particular recent year mani research studi simpl onlin addit multipl updat algorithm name perceptron winnow algorithm variant thereof paper take differ approach describ natur parameter famili boostingbas pac algorithm learn linear threshold function weak hypothes use linear function strong classifi obtain linear threshold function although new algorithm support part nsf graduat fellowship nsf grant ccr onr grant n conceptu algorithm differ perceptron winnow establish perform bound new algorithm remark similar perceptron winnow thu refer new algorithm pac analogu perceptron winnow hope analysi new algorithm yield fresh insight relationship boost onlin algorithm give unifi analysi perceptron winnow analogu includ mani algorithm well grove littleston schuurman shown perceptron a version of winnow view case gener onlin pnorm linear threshold learn algorithm p real number present pacmodel boostingbas analogu onlin pnorm algorithm valu p pacmodel perceptron winnow analogu mention respect case gener algorithm case algorithm also view gener jackson craven pacmodel algorithm learn spars perceptron algorithm boost use weak hypothes singl boolean lit eral similar case algorithm doe analysi case gener algorithm deal realvalu rather boolean input variabl yield substanti stronger sampl complex bound establish section paper contain preliminari materi includ overview onlin pnorm algorithm section present simpl pacmodel pnorm algorithm prove weak learn algorithm section appli techniqu theori larg margin classif show learn algorithm boost strong learn algorithm small sampl complex final section compar pac algorithm analog onlin algorithm extend algorithm case discuss relationship case algorithm jacksoncraven algorithm learn spars perceptron relat work sever author studi linear threshold learn algorithm work combin weak predictor freund schapir describ algorithm combin intermedi perceptron algorithm hypothes use weight major vote so final classifi depth threshold circuit prove bound gener error result classifi algorithm use boost combin perceptron hypothes rather weight accord surviv time ji propos randomsearchandtest approach find weak classifi linear threshold function combin simpl major vote thu also obtain depth threshold circuit approach closest jackson craven use boost combin singl liter strong hypothesi linear threshold function describ section case algorithm strengthen gener result gener also note freund schapir schapir exhibit close relationship boost onlin learn start geometr definit point denot pnorm name norm x kxk qnorm dual pnorm henc norm norm dual norm dual itself paper p q alway denot dual norm follow fact well known eg pp h older inequ ju delta minkowski inequ ku throughout paper exampl space x subset linear threshold function x function f function signz take valu z take valu note standard definit linear threshold function allow nonzero threshold ie real number howev linear threshold function gener form n variabl equival linear threshold function threshold definit incur real loss gener write kxk p denot sup xx kxk use symbol ux denot quantiti xx measur separ exampl x hyperplan whose normal vector u assum throughout paper kxk p ie set x bound ffi nonzero lower bound separ hyperplan defin u exampl x pac learn denot exampl oracl which queri provid label exampl x drawn accord distribut x say algorithm strong learn algorithm u x satisfi follow condit function x distribut x make mffl ffi call exu d probabl least output hypothesi h g pr xd hx signu delta say hypothesi h fflaccur hypothesi function mffl ffi x sampl complex algorithm a main result describ strong learn algorithm care analyz sampl complex must consid algorithm satisfi strong learn properti still capabl gener hypothes slight advantag random guess such socal weak learn algorithm first consid kearn valiant let finit sequenc label exampl x let distribut s say say algorithm gamma flweak learn algorithm u follow condit hold finit set describ distribut given input output hypothesi gamma flapproxim u d thu purpos weak learn algorithm one alway find hypothesi outperform random guess fix sampl onlin learn pnorm algorithm onlin model learn take place sequenc trial throughout learn process learner maintain hypothesi h map x fgamma g trial proce follow upon receiv exampl x x learn algorithm output predict associ label y learn algorithm given true label fgamma g algorithm updat hypothesi h base new inform next trial begin perform onlin learn algorithm exampl sequenc measur number predict mistak algorithm make grove littleston schuurman gentil littleston studi famili onlin algorithm learn linear threshold function see figur refer algorithm parameter real valu onlin pnorm algorithm like wellknown perceptron algorithm onlin pnorm algorithm updat hypothesi make addit chang weight vector z howev shown step figur pnorm input paramet real number p initi weight vector posit valu set exampl avail get unlabel exampl predict get label fgamma g set enddo figur onlin pnorm algorithm algorithm use z vector directli predict rather predict use vector w transform version z vector name w w henc onlin norm algorithm perceptron algo rithm shown p onlin pnorm algorithm approach version winnow algorithm precis follow theorem give mistak bound onlin pnorm algorithm theorem let sequenc label exampl everi exampl hx yi s a onlin pnorm algorithm invok input paramet p z mistak bound exampl sequenc b mistak bound z c let suppos describ part b mistak bound given b converg ux log log onlin pac learn variou gener procedur propos automat convert onlin learn algorithm pacmodel algorithm procedur sampl complex result pac algorithm depend mistak bound origin onlin learn algorithm strongest gener result type in term minim sampl complex result pac algorithm longestsurvivor convers due kearn li pitt theorem let onlin learn algorithm guarante make mistak pacmodel learn algorithm use logffi log exampl output fflaccur hypothesi probabl theorem yield sampl complex bound gener pacmodel convers onlin pnorm algo rithm describ complet differ pacmodel algorithm remark similar sampl complex bound pacmodel pnorm weak pnorm weak learn algorithm motiv follow simpl idea suppos collect label exampl replac neg exampl equival posit exampl obtain new collect exampl let averag locat exampl ie z center mass everi exampl must lie side hyperplan vector u clear z must also lie side hyper plane one might even hope z relat vector point approxim direct vector u pnorm weak learn algorithm call wla present figur onlin pnorm algorithm wla transform vector z vector w use map show simpl algorithm fact weak learner theorem wla gamma flweak learn algorithm littleston give convers procedur yield pac sampl complex bound offl although improv result log factor littleston procedur requir exampl space x finit stronger assumpt make paper input paramet real number p sequenc label exampl distribut set return hypothesi hx j figur pnorm weak learn algorithm wla proof let sequenc label exampl x x x everi let distribut s show hypothesi h wlap s d return see h map x gamma note holder inequ impli show inequ section hold thu wk q thu suffic show wk q first note x ja henc lefthand side desir inequ equal second equal use fact p gamma p consequ lefthand side simplifi kzk p thu goal show kzk p ffi x ja last line follow holder inequ theorem prove shown simpl wla algorithm weak learn algorithm halfspac learn problem section use techniqu boost larg margin classif obtain strong learn algorithm small sampl complex boost achiev high accuraci seri import paper schapir freund given boost algorithm transform weak learn algorithm strong one paper use adaboost algorithm shown figur notat algorithm similar input adaboost sequenc label exampl weak learn algorithm wl two paramet given distribut output hypothesi h adaboost success gener new distribut use wl obtain hypothes h ultim output final hypothesi linear threshold function h s freund schapir prove algorithm wl gamma flweak learn algorithm ie call wl adaboost gener hypothesi h ffl fraction exampl misclassifi final hypothesi h given result one straightforward way obtain strong learn algorithm halfspac learn problem draw suffici larg as specifi below sampl exampl oracl exu d run adaboost use wla weak learn algorithm fl given theorem choic ensur adaboost final hypothesi make error s moreov sinc hypothesi gener wla form h v final hypothesi use wellknown fact vc dimens class zerobia input paramet sequenc label exampl weak learn algorithm wl gamma two real valu set log let h output wld s set ffl normal factor so t distribut enddo output final hypothesi hx j signfx figur adaboost algorithm linear threshold function n n main result impli probabl least gamma ffi final hypothesi h fflaccur hypothesi u provid jsj cffl gamma n logffl c analysi though attract simpl yield rather crude bound sampl complex depend particular learn problem ie u x rest section use recent result adaboost abil gener largemargin classifi gener abil largemargin classifi give much tighter bound sampl complex learn algorithm boost achiev larg margin suppos classifi form say margin h label exampl hx yi yfx note quantiti nonneg h correctli predict label associ x follow theorem extens theorem show adaboost gener larg margin hypothes theorem suppos adaboost run exampl sequenc use weak learn algorithm wl gamma valu theorem state cover case wl map fgamma g need gener version weak hypothes theorem map gamma rather fgamma g proof theorem given appendix a result section impli wla use learn algorithm adaboost valu ffl alway gamma fl upper bound theorem becom gammafl gamma fl easi lemma prove appendix b set appli lemma upper bound theorem becom obtain follow corollari adaboost run sequenc label exampl drawn exu d use wla learner fl defin theorem hypothesi h adaboost gener margin least fl everi exampl s proof bound caus greater log consequ upper bound theorem less jsj next subsect use corollari theori larg margin classif establish bound gener error h term sampl size m larg margin gener let f collect realvalu function set x finit set fx said shatter f real number r b function f b f fatshatt dimens f scale denot fat f size largest set shatter f finit infin otherwis fatshatt dimens use us follow theorem theorem let f collect realvalu function x let distribut x theta fgamma g let sequenc label exampl drawn d probabl least gamma ffi choic s classifi hx j signfx f f margin least everi exampl s pr log em note section final hypothesi h adaboost output must form x invoc wla gener hypothesi form x kv k q impli vector must satisfi kvk q consid class function ae x oe bound fat f given sampl size m theorem immedi yield correspond bound pr xd hx signu delta x halfspac learn prob lem follow theorem prove appendix c give desir bound fat f theorem let x bound region n let f class function x defin abov fat f log n combin theorem corollari theorem follow algorithm use sampl size m probabl least gamma ffi hypothesi h gener satisfi pr xd log n log m log thu establish follow where onot hide log theorem algorithm obtain appli adaboost wla use paramet set describ corollari strong learn algorithm u x sampl complexit sampl complex boostingbas pnorm pac learn algorithm remark similar pac transform onlin pnorm algorithm section log factor set bound depend linearli ffl gamma quadrat kuk q kxk p ffi compar bound detail see onlin variant describ part a theorem extra factor bound present sampl complex algo rithm variant a offer advantag though user need know valu quantiti kxk p kuk q advanc order run algorithm turn part b theorem see paramet set appropri onlin algorithm onlin bound differ pac algorithm bound extra factor z again ignor log factor part c theorem show even z chosen also note omegagamma n gentil littleston given altern express onlin pnorm bound term kxk use entir similar analysi bound algorithm analog rephras case well sinc case onlin pnorm algorithm precis perceptron algorithm case algorithm view natur pacmodel analogu onlin perceptron algorithm note upper bound given lemma appendix c strengthen delta kxk see lemma theorem proof mean fatshatt dimens upper bound theorem improv to remov log factor bound theorem howev bound still contain variou log factor log term theorem algorithm extrem defin natur algorithm consid vector z w comput weak learn algorithm wla let r number coordin z z jz lim wk q ae signz r jz henc natur consid version wla denot wla vector w defin take wise analysi continu hold with minor modif describ appendix d obtain strong learn algorithm theorem hold place wla close relationship work jackson craven learn spars perceptron note one coordin z jz wla hypothesi kxk sign variabl strongli correl distribut valu signu delta similar weak learn algorithm use jackson craven take singl bestcorrel liter hypothesi break ing tie arbitrarili proof bestsingleliter algorithm use weak learn algorithm due goldmann hastad razborov howev proof assum exampl space x f g n target vector u integ coeffici thu note jackson craven algorithm learn spars perceptron appli learn problem defin discret input domain contrast algorithm appli continu input domain restrict exampl space x target vector u satisfi also observ theorem establish tighter sampl complex bound strong learn algorithm given see thi let suppos target vector coeffici algorithm appli learn problem ffi ux omegagamma let theorem impli learn algorithm sampl complex roughli ffl ig nore log factor substanti improv roughli ffl sampl complex bound given gener sampl complex bound given learn sspars kperceptron roughli ks ffl analysi paper easili extend establish sampl complex bound roughli ks ffl learn s spars kperceptron open question result give evid broad util boost algorithm adaboost natur question much util extend simpl boostingbas pac version standard learn algorithm note context kearn mansour shown variou heurist algorithm topdown decis tree induct view instanti boost anoth goal construct power boostingbas pac algorithm linear threshold function algorithm discuss paper invers quadrat depend separ paramet ffi linearprogram base algorithm learn linear threshold function see eg depend natur boostingbas pac algorithm linear threshold function perform bound similar linearprogram base algorithm acknowledg warmli thank le valiant help comment suggest r machin learn probabilist method proc th symp found comp sci advanc kernel method support vector learn perceptron algorithm fast nonma liciou distribut proc th symp found comp sci learnabl vapnikchervonenki dimens proc th symp found comp sci boost weak learn algorithm ma joriti fifth ann work comp learn theori proc ninth ann conf comp learn theori decisiontheoret gener onlin learn applic boost proc eleventh ann conf comp learn theori proc th ann conf comp learn theori proc th ann conf comp learn theori major gate vs gener weight threshold gate space effici learn algorithm probabl inequ sum bound random variabl advanc neural inform process system combin weak classifi proc fourth int workshop machin learn proc th symp theor comp st acm symp theor comp proc eighth ann conf comp learn theori learn quickli irrelev attribut abound new linearthreshold algorithm mistak bound logarithm linearthreshold learn algorithm proc fourth ann conf comp learn theori halfspac learn comput learn theori natur learn system volum i constraint prospect strength weak learnabl proc twelfth ann conf comp learn theori proc twelfth ann conf comp learn theori boost margin new explan effect vote method proc eleventh ann conf comp learn theori criteria lower bound perceptronlik learn rule advanc calculu tr