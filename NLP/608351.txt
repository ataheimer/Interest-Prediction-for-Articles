t rel loss bound temporaldiffer learn a foster vovk prove rel loss bound linear regress total loss onlin algorithm minu total loss best linear predictor chosen hindsight grow logarithm number trial give similar bound temporaldiffer learn learn take place sequenc trial learner tri predict discount sum futur reinforc signal qualiti predict measur squar loss bound total loss onlin algorithm minu total loss best linear predictor whole sequenc trial differ loss logarithm number trial bound hold arbitrari worstcas sequenc exampl also give bound expect differ case instanc chosen unknown distribut linear regress correspond lower bound show expect bound cannot improv substanti b introduct consid follow model temporaldiffer learn learn proce sequenc trial trial t gamma learner receiv instanc vector x r n gamma learner make predict gamma learner receiv reinforc signal r r pair call exampl learner tri predict outcom r fix discount rate paramet fl discount sum st futur reinforc signal exampl r profit compani month t interpret approxim compani worth time t discount sum take account profit distant futur less import short term profit note outcom defin welldefin reinforc signal bound episod set one also defin outcom finit discount sum discuss briefli section strategi choos predict learner call onlin learn algorithm qualiti predict measur squar loss loss learner trial y loss learner trial want compar loss learner loss linear function linear function repres weight vector loss w trial y ideal want bound addit loss learner loss best linear predictor arbitrari sequenc exampl ie want bound arbitrari arbitrari sequenc exampl first sum total loss learner trial argument infimum total loss linear function altern could defin gamma fl st rs make convex combin reinforc signal r r t altern definit amount simpl rescal outcom predict rel loss bound temporaldiffer learn w trial thu addit total loss learner total loss best linear function follow vovk also examin gener problem bound fix constant akwk measur complex w ie infimum includ charg complex linear function larger valu obvious easier show bound bound hold arbitrari sequenc exampl call rel loss bound rel loss bound temporaldiffer learn set first shown schapir warmuth overview result given section also show algorithm minim use valu function approxim markov process import problem reinforc learn also call polici evalu markov process continu mani state repres real vector one want predict state valu instanc x correspond state environ action valu predict instanc correspond state environ action agent introduct reinforc learn see sutton barto paper organ follow discuss previous known rel loss bound linear regress temporaldiffer learn section section propos new second order learn algorithm temporaldiffer learn the tl algorithm prove rel loss bound algorithm section section adapt tl algorithm episod case trial divid episod outcom discount sum futur reinforc signal discuss previou second order algorithm temporaldiffer learn section give lower bound rel loss section notat preliminari set ndimension real vector mn n r mthetan set real matric row n column paper vector x r n column vector x denot transpos j forster m k warmuth x scalar product two vector wx r n w euclidean norm vector x r n recal basic fact posit semidefinit matric nthetan call posit definit x hold vector x r n n fg nthetan call posit semidefinit gamma sum two posit semidefinit matric posit semidefinit sum semidefinit matrix posit definit matrix posit definit everi posit definit matrix invert gamma matric a b r nthetan write b b gamma posit semidefinit case x ax x bx vector x r n gamma shermanmorrison formula see press flanneri teukolski hold everi posit definit matrix r nthetan everi vector exampl unit matrix r nthetan posit definit everi vector x r n matrix xx r nthetan posit semidefinit find vector w r n minim term appear rel loss defin ks convex w minim gradient zero ie invert uniqu vector minim rel loss bound temporaldiffer learn might invert equat might uniqu solut solut smallest euclidean norm pseudoinvers definit pseudoinvers matrix see eg rektori also shown pseudoinvers matrix comput singular valu decomposit give number properti pseudoinvers appendix appli shermanmorrison formula show known rel loss bound linear regress first note linear regress special case setup sinc t standard algorithm linear regress ridg regress algorithm predict x trial t rel loss bound algorithm similar bound given two theorem proven foster vovk azouri warmuth bound obtain ridg regress weaker one proven new algorithm develop vovk give simpl motiv algorithm discuss rel loss bound proven it seen section best linear function trial ie linear function minim would make predict b x trial t note via b predict depend exampl howev exampl instanc x known learner make predict trial t set unknown outcom zero get predict b predict introduc vovk use differ motiv motiv follow azouri warmuth forster give altern game theoret motiv vovk prove follow bound predict algorithm theorem consid linear regress sequenc exampl r n predict j forster m k warmuth x ti ith compon vector x vovk version theorem term x n replac larger bound supremum norm instanc last inequ theorem follow z first inequ hold geometr mean alway smaller arithmet mean azouri warmuth forster give follow refin bound vovk linear regress algorithm case consid show case theorem i follow case let go zero theorem consid linear regress sequenc exampl r n theta r i predict ii vector x rel loss bound temporaldiffer learn proof show equal i also hold case show side equal continu lemma a check tg term continu x x tgamma follow lemma a otherwis x lemma a express zero show converg zero rewrit appli b factor b converg lemma a lemma a term x goe zero togeth show inde goe zero learn algorithm theorem theorem second order algorithm use second deriv simpler first order algorithm call widrowhoff least mean squar algorithm widrow stearn algorithm maintain weight vector w r n predict weight vector updat gradient descent is w learn rate method set learn rate purpos obtain good rel loss bound given cesabianchi long warmuth kivinen warmuth method learner need know upper bound x euclidean norm instanc need know paramet w k vector w r n norm kwk w loss vector w bound hold note vovk bound incompar bound theorem see also next section bound also hold ridg regress algorithm hassibi kivinen war muth paramet set depend w k believ proper tune bound also hold vovk j forster m k warmuth known rel loss bound temporaldiffer learn case discount rate paramet fl assum zero schapir warmuth given number differ rel loss bound learn algorithm td first order algorithm td essenti gener widrowhoff algorithm slight modif learn algorithm td propos sutton schapir warmuth show loss td specif set learn rate where c loss algorithm td specif set learn rate everi vector w r n kwk w set learn rate depend upper bound x euclidean norm instanc w k learner need know paramet advanc loss best linear function often grow linearli eg exampl corrupt gaussian nois case rel loss bound grow like second order learn algorithm propos temporaldiffer learn advantag rel loss bound prove grow logarithm also algorithm need know paramet like k w howev need know upper bound absolut valu outcom td sensit choic anoth advantag algorithm choos paramet like td algorithm new second order algorithm temporaldiffer learn section propos new algorithm temporaldiffer learn set call algorithm tempor least squar algorithm shorter tl algorithm rel loss bound temporaldiffer learn assum absolut valu outcom bound constant ie assum bound discount rate paramet fl paramet known learner know learner clip real number r use function motiv tl algorithm new secondord algorithm temporaldiffer learn given tabl i call algorithm tempor least squar tl algorithm motiv tl algorithm motiv azouri warmuth gave vovk predict linear regress section use equal ks hold t best linear function trial minim would make predict ks trial t set unknown outcom zero get predict e ks tl algorithm predict clip function assur predict lie bound rang follow show rel loss tl j forster m k warmuth tabl i tempor least squar tl algorithm trial t learner know ffl paramet fl a ffl instanc ffl reinforc signal tl predict ks rk cy given clip predict interv gammay invert invers gamma must replac pseudoinvers use result get worst averag case rel loss bound easier interpret implement tl algorithm case straightforward implement tl algorithm would need on arithmet oper trial comput invers matrix tabl ii give implement need on arithmet oper per trial achiev comput invers iter use shermanmorrison formula implement make correct predict end forloop follow shermanmorrison formula equal rel loss bound temporaldiffer learn tabl ii implement tl inv r nthetan z receiv instanc vector x r n inv inv gamma inv x predict receiv reinforc signal r r z rel loss bound tl algorithm temporaldiffer learn set know outcom need predict trial t cannot run vovk linear regress algorithm use outcom pre diction tl algorithm approxim vovk predict set futur reinforc signal zero it also clip predict rang gammay show loss tl algorithm much wors loss vovk algorithm good rel loss bound known start show two lemma first technic lemma use prove second lemma bound absolut valu differ vovk predict b unclip predict e tl algorithm lemma vector x proof follow pseudoinvers invers shermanmorrison formula show j forster m k warmuth thu gamma x prove lemma case use lemma a let go lemma proof note thu lemma yt z z show main result theorem consid temporaldiffer learn let sequenc exampl r n theta r outcom lie real interv gammay predict tl algorithm rel loss bound temporaldiffer learn proof let know y ie rel loss bound theorem i also hold clip predict c p thu suffic show hold lemma in next two subsect appli theorem show rel loss bound worst case averag case worst case rel loss bound corollari consid temporaldiffer learn let sequenc exampl r n theta r outcom lie real interv gammay predict tl algorithm j forster m k warmuth proof first inequ follow theorem theorem ii second follow theorem averag case rel loss bound assum outcom lie gammay instanc iid unknown distribut r n show upper bound expect rel loss trial depend n y fl particular need term akwk measur complex vector w rel loss need assum instanc bound show result use theorem bound sum term x follow theorem tra trace squar matrix dimx dimension vector space x theorem vector x linear span proof first look case choos orthonorm basi e em x also written mean interpret matrix nthetan linear function r n r n ident function x assert case follow ijm ijm ijm rel loss bound temporaldiffer learn jm jm jm jm prove theorem case choos arbitrari orthonorm basi e appli result vector x gamman first note sgamman x x case sgamman x sinc vector fx gamman rank n equal follow sgamman x first inequ case follow atra gamma corollari consid temporaldiffer learn set assum instanc x iid unknown distribut r n outcom given lie real interv gammay predict tl algorithm expect proof theorem expect rel loss iid theorem prove first inequ corollari second follow j forster m k warmuth tabl iii tempor least squar tl algorithm episod learn trial t tl predict rk cy given clip predict interv gammay startk first trial episod trial k belong invert invers gamma must replac pseudoinvers episod learn studi set outcom st discount sum futur reinforc signal use algorithm polici evalu reinforc learn correspond look continu task see sutton barto episod task trial partit episod finit length outcom depend reinforc signal belong episod let trial first trial episod trial denot startt last endt notat discount rate paramet fl outcom episod set defin st replac definit given continu set definit rel loss remain un chang note continu set essenti episod set one episod infinit length motiv section get tl algorithm episod learn present tabl iii implement algorithm given tabl iv check correct implement verifi rel loss bound temporaldiffer learn tabl iv implement tl episod learn inv r nthetan new episod start trial t set z r n receiv instanc vector x r n inv inv gamma inv x predict receiv reinforc signal r r z hold everi iter forloop follow equal note practition small matrix might invert algorithm use pseudoinvers practic suggest use tune paramet alway invert calcul pseudoinvers avoid also conjectur clip need practic data show rel loss bound episod learn assum bound outcom known learner advanc proof follow theorem similar proof theorem theorem consid temporaldiffer learn episod length let let sequenc exampl r n theta r outcom given lie real interv gammay predict tl algorithm tabl iii j forster m k warmuth bound instanc x iid unknown distribut r n expect lower bound correspond shown theorem note theorem exploit fact differ episod vari length relat theorem depend actual length episod easili develop second order algorithm second order algorithm temporaldiffer learn propos bradtk barto boyan compar algorithm episod set algorithm maintain weight vector w predict trial t bradtk barto leastsquar td shorter lstd algorithm use weight vector r x boyan lstd algorithm he consid case paramet like td algorithm use weight vector z contrast tl algorithm use weight vector rel loss bound temporaldiffer learn predict tl algorithm trial formula invers must replac pseudoinvers matrix invert note tl algorithm paramet like td lstd case algorithm lstd ident import differ tl algorithm td lstd use differ definit covari matrix bradtk barto boyan experiment compar algorithm td compar strong assumpt bradtk barto also show w algorithm converg asymptot tl algorithm propos paper design minim rel loss rel loss bound show tl well know whether similar rel loss bound hold lstd lstd algorithm experiment comparison would use lower bound section give lower bound linear regress episod temporaldiffer learn first consid case linear regress ie case outcom equal reinforc signal outcom lie gammay corollari give upper bound expect rel loss exampl iid respect arbitrari distribut next theorem show bound cannot improv substanti proof similar proof theorem vovk howev dimens n instanc greater one exampl proof gener stochast strategi iid theorem consid linear regress constant c distribut set exampl r n theta gammay everi learn algorithm expect rel loss exampl iid distribut d j forster m k warmuth proof fix paramet ff gener distribut exampl follow stochast strategi vector chosen prior distribut betaff ff n ie compon iid distribut betaff ff distribut exampl n exampl n e unit vector r n trial exampl gener iid calcul bay optim learn algorithm expect loss trial minim expect rel loss algorithm give lower bound theorem detail proof given appendix consid set discuss section trial partit episod outcom given st follow lower bound proven reduct previou lower bound linear regress theorem consid episod temporaldiffer learn episod fix length let gammay rang outcom everi constant c stochast strategi gener instanc x x reinforc signal r outcom lie gammay divis everi learn algorithm expect over stochast choic exampl rel loss proof modifi stochast strategi use proof theorem strategi gener instanc x outcom trial t gener whole episod trial instanc reinforc signal outcom episod fl consid qth trial episod fore q trial learner essenti process scale exampl lower bound theorem appli factor fl gammaq choic q lead factor lower bound rel loss bound temporaldiffer learn conclus open problem propos new algorithm temporaldiffer learn tl algorithm contrari previou second order algorithm new algorithm use differ definit covari matrix see discuss section main question whether differ realli help prove worst averag case rel loss bound tl algorithm would interest know tight bound practic data bound class linear function serv comparison class use second order algorithm addit loss loss best compar logarithm number trial conjectur even linear regress first order algorithm adapt learn rate addit loss logarithm number trial algorithm analyz appli case instanc expand featur vector dot product two featur vector given kernel function see saunder gam merman vovk also fourier wavelet transform use extract inform instanc see walker grap linear transform one reduc dimension comparison class lead smaller rel loss bound far compar total loss onlin algorithm total loss best linear predictor whole sequenc exampl suppos compar produc partit data sequenc k segment pick best linear predictor segment aim bound total loss onlin algorithm minu total loss best compar form bound obtain herbster warmuth case linear regress use firstord algorithm would like know whether simpl secondord algorithm linear regress requir on updat time per trial addit loss grow sum log section length paper focus continu learn outcom infinit discount sum futur reinforc signal section discuss tl algorithm adapt set outcom depend reinforc signal episod st applic might make sens let outcom convex combin futur reinforc signal j forster m k warmuth episod defin st case outcom would averag futur reinforc signal know rel loss bound case defin technic level would like know realli necessari clip predict temporaldiffer algorithm propos proof reduct previou proof linear regress direct proof might avoid clip anoth open technic question discuss end section conjectur paramet vovk linear regress algorithm tune obtain bound form proven first order widrowhoff algorithm similarli believ paramet new second order learn algorithm paper tune obtain bound proven first order td algorithm schapir warmuth final note lower bound continu set fl possibl show lower bound expect rel loss see theorem correspond lower bound episod case acknowledg jurgen forster support daad doktorandenstipendium im rahmen de gemeinsamen hochschulsonderprogramm iii von bund und landern manfr warmuth support nsf grant ccr thank nigel duffi valuabl comment r rel loss bound onlin densiti estim exponenti famili distribut linear analysi introductori cours rel loss bound gener linear regress predict worst case introduct wavelet unpublish manuscript depart comput scienc track best regressor addit versu exponenti gradient updat linear predict numer recip pascal survey applic mathemat ridg regress learn algorithm dual variabl worstcas analysi temporaldiffer learn algorithm learn predict method tempor differ reinforc learn introduct competit onlin linear regress fast fourier transform adapt signal process tr