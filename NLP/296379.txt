t track best disjunct a littleston develop simpl determinist onlin learn algorithm learn kliter disjunct algorithm call sc winnow keep one weight variabl multipl updat weight develop random version sc winnow prove bound adapt algorithm case disjunct may chang time case possibl target it disjunct schedul tgr sequenc disjunct one per trial it shift size total number liter addedremov disjunct one progress sequencew develop algorithm predict nearli well best disjunct schedul arbitrari sequenc exampl algorithm allow us track predict best disjunct hardli complex origin version howev amort analysi need obtain worstcas mistak bound requir new techniqu case lower bound show upper bound algorithm right constant front lead term mistak bound almost right constant front second lead term comput experi support theoret find b introduct one signific success comput learn theori commun littleston formal onlin model learn develop algorithm winnow learn disjunct littleston key featur winnow learn disjunct constant size number mistak algorithm grow logarithm input dimens mani standard algorithm perceptron algorithm rosenblatt number mistak grow linearli dimens kivinen warmuth auer meantim number algorithm similar winnow develop also show logarithm growth loss bound dimens littleston warmuth vovk cesabianchi et al haussler kivinen warmuth extend abstract appear auer warmuth m k warmuth acknowledg support nsf grant ccr paper give refin analysi winnow develop random version algorithm give lower bound show determinist random version close optim adapt version use track predict best disjunct consid follow standard onlin learn model littleston vovk cesabianchi et al learn proce trial trial algorithm present instanc x in case ndimension binari vector use produc binari predict algorithm receiv binari classif instanc incur mistak goal minim number mistak algorithm arbitrari sequenc exampl hx cours hopeless scenario determinist algorithm adversari alway choos sequenc algorithm make mistak trial reason goal minim number mistak algorithm compar minimum number mistak made concept comparison class nonshift basic setup paper use monoton kliter disjunct comparison class dimens number boolean attributesliter n disjunct boolean formula form x distinct indic j lie ng number classif error disjunct respect sequenc exampl simpli total number misclassif disjunct produc sequenc goal develop algorithm whose number mistak much larger number classif error best disjunct sequenc exampl paper consid case mistak best target disjunct caus attribut error number attribut error exampl respect target disjunct u minimum number attributesbit x chang result x y number attribut error sequenc exampl respect target concept simpli total number error exampl sequenc note target u kliter monoton disjunct number attribut error k time number classif error respect u ie k time number exampl x y sequenc ux y winnow tune function k make oak lnnk mistak sequenc exampl best disjunct incur attribut error littleston give random version winnow give improv tune origin algorithm new algorithm tune base k expect mistak bound sequenc exampl monoton kliter disjunct attribut error also show origin determinist algorithm tune number mistak set sequenc lower bound show bound close optim show algorithm expect number mistak must least upper bound correct constant lead term almost optim constant second term determinist algorithm lower bound show constant lead term optim lower bound determinist random case cannot improv significantli essenti match upper bound achiev noneffici algorithm correct factor first second term algorithm use expert cesabianchi et al expert simpli comput valu particular kliter disjunct one weight kept per expert amount expand ndimension boolean input boolean input use singl liter expert littleston warmuth vovk cesabianchi et al comparison class instead k liter disjunct expect number mistak random algorithm q bound number classif error best kliter disjunct mistak bound determinist algorithm exactli twice high observ algorithm use n k weight need much time trial calcul predict updat weight thu run time exponenti k contrast algorithm use n weight hand nois upper bound effici algorithm measur attribut error rather classif error aris sinc use one weight per attribut recal classif error respect kliter disjunct equat k attribut error captur error affect k attribut effici expans expert seem unavoid nevertheless surpris version winnow abl get right factor number attribut error random version almost right factor squar root term sens winnow compress delta weight n weight point combinatori interpret weight interpret found singl liter expert case cesabianchi freund helmbold warmuth littleston littleston use amort analysi entrop potenti function obtain worstcas loss bound howev besid care tune bound take amort analysi method signific step prove mistak bound algorithm compar best shift disjunct shift disjunct assum disjunct u specifi ndimension binari vector compon valu correspond monoton liter disjunct two disjunct u u ham distanc measur mani liter shift obtain u u disjunct schedul sequenc exampl length simpli sequenc disjunct u shift size schedul zero vector origin nonshift case u equal kliter disjunct u accordingli definit shift size k trial schedul predict disjunct u defin number attribut error exampl sequenc hx i respect schedul total number attribut chang sequenc exampl make consist schedul ie chang instanc x note loss bound nonshift case written cao delta number bit take describ disjunct k liter random determinist algorithm surprisingli abl prove bound form shift disjunct case b number bit take describ best schedul number attribut error schedul z shift size schedul take log z bit describ schedul respect given sequenc exampl worstcas mistak bound similar bound obtain competit algorithm compar number mistak algorithm number attribut error best offlin algorithm given whole sequenc ahead time offlin algorithm still incur attribut error bound addit loss onlin algorithm number attribut error best schedul as oppos coarser method bound ratio onlin offlin winnow multipl updat weight whenev algorithm make mistak weight liter correspond bit current input instanc one multipli factor case winnow version winnow paper base littleston factor either ff ff ff paramet algorithm multipl weight updat might caus weight algorithm decay rather rapidli sinc liter might becom part disjunct schedul even mislead earli part sequenc exampl algorithm predict well compar best disjunct schedul must abl recov weight quickli extens winnow simpli add step origin algorithm reset weight fin whenev drop boundari similar method lower bound weight use algorithm wml littleston warmuth design predict well best shift singl liter which call expert cesabianchi et al addit gener work littleston warmuth arbitrari size disjunct abl optim constant lead term mistak bound winnow develop random version algorithm herbster warmuth work littleston warmuth gener differ direct focu predict well best shift expert well measur term loss function discret loss count mistak loss function use paper basic build block simpl onlin algorithm use multipl weight updat vovk haussler et al predict feedback trial realvalu lie interv class loss function includ natur loss function log loss squar loss helling loss loss occur larg discret unit instead loss trial arbitrarili small thu sophist method need recov small weight quickli herbster warmuth simpli lower bound weight disjunct import whenev richer class built small union larg number simpl basic concept method appli simpli expand origin input mani input basic concept sinc mistak bound depend logarithm number basic con cept even allow exponenti mani basic concept still polynomi mistak bound method previous use develop nois robust algorithm predict nearli well best discret ddimension axisparallel box maass warmuth auer well best prune decis tree helmbold schapir case multipl algorithm maintain one weight exponenti mani basic concept howev exampl multipl algorithm exponenti mani weight still simul effici now exampl method paper immedi lead effici algorithm predict well best shift ddimension box thu combin method exist al gorithm design effici learn algorithm provabl good worstcas loss bound gener shift concept disjunct besid experi practic data exemplifi merit worstcas mistak bound research also leav number theoret open prob lem winnow algorithm learn arbitrari linear threshold function method track best disjunct still need gener learn gener class concept believ techniqu develop learn predict well best shift disjunct use set develop algorithm predict nearli well best shift linear combin discret loss replac continu loss function squar loss make problem challeng relat work natur competitor winnow well known perceptron algorithm rosenblatt learn linear threshold function algorithm addit instead multipl updat classic perceptron converg theorem give mistak bound algorithm duda hart haykin bound linear number attribut kivinen et al wherea bound winnowlik algorithm logarithm number attribut proof perceptron converg theorem also seen amort analysi howev potenti function need perceptron algorithm quit differ potenti function use analysi winnow w weight vector algorithm trial u target weight vector perceptron algorithm potenti function jjjj euclidean length vector contrast potenti function use analysi winnow littleston also use paper follow gener rel entropi cover case linear regress framework develop kivinen warmuth deriv updat potenti function use amort anal ysi framework adapt deriv perceptron algorithm winnow differ potenti function algorithm lead addit multipl algorithm respect perceptron algorithm seek weight vector consist exampl otherwis minim euclidean length winnow instead minim rel entropi thu root minimum rel entropi principl kullback kapur kesavan jumari organ paper next section formal defin notat use throughout paper alreadi discuss introduct section present algorithm section give theoret result algorithm section consid practic aspect name paramet algorithm tune achiev good perform section report experiment result analysi algorithm proof section given section lower bound number mistak made algorithm shown section conclud section notat target schedul sequenc disjunct repres nari bit vector u size shift disjunct u disjunct u z j total shift size schedul z assum u get precis bound case shift target schedul distinguish shift liter ad disjunct shift liter remov disjunct thu defin z number time liter switch on number time liter switch off sequenc exampl consist attribut vector classif f g predict disjunct u attribut vector x u number attribut error trial respect target schedul minim number attribut chang result x u g total number attribut error sequenc respect schedul denot sz a n class exampl sequenc n attribut consist target schedul shift size z attribut error wish distinguish posit neg shift denot correspond class sz number liter ad remov respect target schedul k a n denot class exampl sequenc n attribut consist nonshift target schedul ie attribut error case upper bound z z known denot correspond class z a zz k loss learn algorithm l exampl sequenc number misclassif binari predict learn algorithm l trial t algorithm present algorithm swin shift winnow see tabl extens littleston winnow algorithm littleston extens incorpor random algorithm guarante lower bound weight use algorithm algorithm maintain vector n weight n attribut w denot weight end trial t tabl algorithm swin paramet algorithm use paramet ff initi set weight initi valu trial set r predict ae receiv binari classif updat pr w w w denot initi valu weight vector trial algorithm predict use weight vector w predict algorithm depend r algorithm predict probabl pr predict probabl obtain determinist algorithm one choos function p predict algorithm receiv classif ie weight vector modifi sinc f g pr occur predict determinist ie pr correct updat occur case predict wrong pr updat weight perform two step first step origin winnow updat second step guarante weight smaller fi paramet fi a similar approach taken littleston warmuth observ weight chang probabl make mistak nonzero determinist algorithm mean weight chang algorithm made mistak furthermor ith weight modifi x weight increas multipli ff decreas divid ff paramet ff fi w function pdelta set appropri good choic function pdelta follow random predict let rand determinist version algorithm let det random version one choos fi ln ff observ det obtain rand choos threshold rand correspond straightforward convers random predict algorithm determinist predict algorithm theoret good choic paramet ff fi w given next section practic issu tune paramet discuss section result section give rigor bound expect number mistak swin first gener specif choic ff fi w pdelta chosen rand det bound shown close optim adversari exampl sequenc detail see section theorem random version let ff n pdelta rand fi n e bound hold theorem determinist version let ff n pdelta det fi n e bound hold theorem nonshift case let ff swin use function pdelta given rand swin use function pdelta given det e bound hold remark usual convers bound random algorithm bound determinist algorithm would give determinist bound observ determinist bound time random bound sinc time disjunct cannot contain n liter z give follow corollari w pdelta rand z a n j j pdelta det z a n ffn j j first give result number mistak swin inform besid n total number attribut given theorem let n pdelta rand z a n n pdelta det n pdelta rand bound hold n n pdelta det k a n bound hold n n section show bound optim constant z known advanc paramet algorithm tune obtain even better result exampl nonshift case number k attribut target concept known get theorem let n pdelta rand e bound hold n k n e set e get emswin e pdelta det k a n e bound hold n k n e set e get mswin e particular interest case domin term ie ae k ln n theorem let k ln n n pdelta rand k a n r e bound hold n k n e n e e emswin e k pdelta det k a n r e bound hold n k n e e e mswin e shift case get domin ae z ln n theorem let zminfnzg z zsuch ffl n pdelta rand z a n r z zminfnzg z n pdelta det z a n r z section show theorem constant optim furthermor show random algorithm also magnitud second order term theorem optim practic tune algorithm section give thought paramet ff fi w swin chosen particular target schedul sequenc exampl recommend base mistak bound hold target schedul sequenc exampl appropri bound number shift attribut error thu mention that sinc mani target schedul mani exampl sequenc worst case bound usual overestim number mistak made swin therefor paramet set differ recommend might result smaller number error specif target schedul exampl sequenc hand swin quit insensit small chang paramet see section effect chang benign littl known target schedul exampl sequenc paramet set theorem advis sinc balanc well effect target shift attribut error good estim number target shift number attribut error known good paramet calcul numer minim bound theorem corollari respect averag rate target shift attribut error known z r z r r z r larg error rate r mswinst corollari approxim upper bound r random predict r ffn determinist predict again optim choic ff fi obtain numer minim experiment result experi report section meant give rigor empir evalu algorithm swin instead intend illustr typic behavior swin compar theoret bound also version winnow modifi adapt shift target schedul experi use attribut target schedul length start activ liter trial one liter switch anoth trial anoth liter switch on switch switch liter continu depict figur thu initi activ liter exampl sequenc hx i chosen half exampl half valu attribut appear target schedul chosen random x probabl exampl exactli one activ attribut chosen random set number trial number activ liter figur shift target schedul use experi exampl attribut error relev attribut either set for case set for case attribut error occur trial trial figur show perform swin compar theoret bound paramet set numer minim bound corollari describ previou section yield theoret bound trial calcul actual number shift attribut error trial thu increas bound due shift target schedul attribut error trial figur reason increas indic z liter switch on z gamma liter switch off attribut error figur show theoret bound accur depict behavior swin although overestim actual number mistak amount seen switch liter caus far less mistak switch liter predict bound also relat attribut error mistak seen perform swin whole sequenc exampl shown figur compar perform version winnow modifi target shift seen swin adapt quickli shift target schedul hand unmodifi version winnow make mistak shift unmodifi version winnow use swin thu weight lower bound becom arbitrarili small caus larg number mistak correspond liter becom activ use z a z number trial number mistak theoret bound perform swin figur comparison swin theoret bound particular target schedul sequenc exampl shift attribut error indic z number trial number mistak theoret bound perform swin perform winnow figur version winnow lower bound weight make mani mistak swin ff unmodifi version set w optim initi part target schedul therefor unmodifi winnow adapt quickli initi part make increas number mistak shift target schedul shift number mistak made approxim doubl last plot figur compar perform swin tune paramet perform swin gener paramet set given theorem although tune paramet perform better differ number trial number mistak theoret bound perform swin figur tune paramet swin versu gener paramet rel small overal conclus experi that first theoret bound captur actual perform algorithm quit well second mechan lower bound weight winnow necessari make algorithm adapt target shift third moder chang paramet chang qualit behavior algorithm amort analysi section first prove gener bound given theorem random determinist version swin bound calcul bound given theorem specif choic paramet analysi algorithm proce show distanc weight vector algorithm w vector u repres disjunct trial t decreas algorithm make mistak potentialdist function use previou analysi winnow littleston follow gener rel entropi arbitrari nonneg weight vector thi distanc function also use analysi egu regress algorithm kivinen warmuth show winnow relat algorithm take deriv easi see distanc minim equal w convent assumpt u f g n distanc function simplifi start analysi random algorithm shift target di junction case deriv easili analysi first calcul much distanc du w chang trial observ term might nonzero trial term nonzero weight updat trial t fl ffi lower bound term weight updat trial t term bound third equal hold sinc x ti f g rememb x obtain x remov attribut error x last inequ follow fact last observ w ti w case w ffn get term sum trial consid trial weight updat distinguish trial denot trial bound term theta r want lower bound sum expect or total number mistak algorithm choos appropri function pdelta w denot p probabl algorithm make mistak trial t expect number mistak observ sinc case thu suffici find function pdelta constant c function pdelta satisfi get assum sz upper bound expect number mistak henc want choos pdelta c c big possibl fix pdelta let r valu pr becom sinc left hand side equat continu get r combin two ff achiev choos pdelta rand satisfi cours choos fi ln ff put everyth togeth follow lemma assum weight use algorithm swin swin use function pdelta given rand determinist variant algorithm function pdelta take valu f g thu get rgammafigammaff c rgammaffln ff c yield get choos pdelta det satisfi assum weight use algorithm swin swin use function pdelta given det go calcul bound fl ffi get bound lower upper bound w ti obvious w ti fi i upper bound w ti deriv observ w ti w tgammai pdelta rand det r w tgammai x ti find w ti ff thu ln efi lemma fi weight w ti algorithm swin function pdelta rand det satisfi proof theorem lemma proof theorem nonshift case u number attribut target disjunct u thu nonshift case term upper bound lemma replac k give theorem proof specif choic paramet proof theorem theorem corollari simpl calcu lation proof theorem get theorem second inequ use ffl substitut valu c ffl give statement theorem proof theorem get corollari j j j j ffl j j c c r z give bound theorem lower bound start prove lower bound shift case show learn algorithm l exampl sequenc learn algorithm make mani mistak although express explicitli follow theorem show sequenc gener target schedul disjunct u consist exactli one liter ie jth unit vector first lower bound show determinist algorithm adversari exampl sequenc sz a n make least mani mistak relat upper bound given theorem theorem determinist learn algorithm l n z exampl sequenc sz a n proof notat conveni assum r construct exampl sequenc depend predict learn algorithm learn algorithm make mistak trial partit trial r round first r gamma round length last round length error occur within last trial choos target schedul round target disjunct chang equal e j begin round disjunct consist exampl round l trial round still gammal consist disjunct construct attribut vector set half attribut correspond consist disjunct attribut furthermor set predict algorithm attribut vector obvious half disjunct consist exampl thu number consist disjunct divid trial thu first r gamma round disjunct consist exampl round last round two disjunct consist exampl round remain a trial fix attribut vector two disjunct predict differ set thu one disjunct disagre time classif disagr seen caus attribut error disjunct consist exampl last round attribut error remark observ lower bound determinist algorithm like impli follow lower bound random algorithm follow fact random learn algorithm turn determinist learn algorithm make twice mani mistak random algorithm make averag mean theorem impli random algorithm l sequenc sz a n remark open problem remain show lower bound form upper bound theorem squar root term turn nonshift case alreadi lower bound known lemma littleston warmuth determinist learn algorithm l n exampl sequenc a n slight modif result cesabianchi et al give lemma cesabianchi et al function nj an j j random learn algorithm l n nj an j exampl sequenc a n ln n extend result obtain follow theorem correspond upper bound given theorem theorem determinist learn algorithm l k n k exampl sequenc k a n theorem function nj an j j random learn algorithm l k n knj kan j exampl sequenc k a n r proof theorem proof reduct case n attribut divid k group g group consist attribut furthermor choos number xi a group g choos sequenc accordingli lemma respect learn algorithm sequenc extend sequenc n attribut set attribut group concaten expand sequenc get sequenc s easi see sk a n hand learn algorithm sequenc n attribut transform learn algorithm sequenc smaller number attribut set miss attribut thu subsequ learn algorithm l make least mani mistak given respect henc r r function an j chosen appropri last theorem show random algorithm constant theorem optim best constant squar root term conclus develop algorithm swin variant winnow onlin learn disjunct subject target shift prove worst case mistak bound swin hold sequenc exampl kind target drift where amount error exampl amount shift bound determinist random version swin analysi random version involv interest right lower bound show worst case mistak bound close optim case comput experi highlight explicit mechan necessari make algorithm adapt target shift acknowledg would like thank mark herbster nick littleston valuabl discuss also thank anonym refere help comment note expand dimens n learn nonmonoton disjunct reduc monoton case essenti one describ shift occur liter shift obvious necess shift current disjunct correct current exampl thu trial current disjunct would make mistak disjunct shift sinc target schedul might make mistak due attribut error z shift get z trial candid shift choos z choos one liter shift give z possibl potenti function weight must posit neg weight handl via reduct littleston haussler et al worst case random algorithm make mistak probabl trial determinist algorithm alway break tie wrong way make mistak trial thu number mistak determinist algorithm twice expect number mistak random algorithm formal let r sequenc hr j exist pdelta equal everywher valu r function pdelta satisfi condit algorithm swin forc make unbound number mistak even absenc attribut error r track best disjunct use expert advic pattern classif scene analysi tight worstcas loss bound predict expert advic tech predict nearli well best prune decis tree track best expert entropi optim principl applic addit versu exponenti gradient updat linear predict perceptron algorithm vs linear vs logarithm mistak bound input variabl relev mistak bound logarithm linearthreshold learn algorithm redund noisi attribut learn theori pp weight major algorithm inform comput effici learn virtual threshold gate learn theori pp tr ctr mark herbster manfr k warmuth track best expert machin learn v n p aug d p helmbold s panizza m k warmuth direct indirect algorithm onlin learn disjunct theoret comput scienc v n p juli chri mesterharm track linearthreshold concept winnow journal machin learn research mark herbster manfr k warmuth track best regressor proceed eleventh annual confer comput learn theori p juli madison wisconsin unit state manfr k warmuth winnow subspac proceed th intern confer machin learn p june corvali oregon gentil nick littleston robust olivi bousquet manfr k warmuth track small set expert mix past posterior journal machin learn research claudio gentil robust pnorm algorithm machin learn v n p decemb giovanni cavallanti nicol cesabianchi claudio gentil track best hyperplan simpl budget perceptron machin learn v n p decemb mark herbster manfr k warmuth track best linear predictor journal machin learn research p s b kotsianti i d zaharaki p e pintela machin learn review classif combin techniqu artifici intellig review v n p novemb peter auer use confid bound exploitationexplor tradeoff journal machin learn research