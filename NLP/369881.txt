t cascad gener a use multipl classifi increas learn accuraci activ research area paper present two relat method merg classifi first method cascad gener coupl classifi loos belong famili stack algorithm basic idea cascad gener use sequenti set classifi step perform extens origin data insert new attribut new attribut deriv probabl class distribut given base classifi construct step extend represent languag high level classifi relax bia second method exploit tight coupl classifi appli cascad gener local iter divid conquer algorithm reconstruct instanc space occur addit new attribut new attribut repres probabl exampl belong class given base classifi implement three local gener algorithm first merg linear discrimin decis tree second merg naiv bay decis tree third merg linear discrimin naiv bay decis tree algorithm show increas perform compar correspond singl model cascad also outperform method combin classifi like stack gener compet well boost statist signific confid level b introduct abil chosen classif algorithm induc good gener depend appropri represent languag express gener exampl given task represent languag standard decis tree dnf formal split instanc space axisparallel hyperplan represent languag linear discrimin function set linear function split instanc space obliqu hyper plane sinc differ learn algorithm employ differ knowledg represent search heurist differ search space explor divers result obtain statist heneri refer rescal method use class overpredict lead bia rescal consist appli algorithm sequenc output algorithm use input anoth algorithm aim would use estim probabl deriv learn algorithm input second learn algorithm purpos produc unbias estim qc jw condit probabl class problem find appropri bia given task activ research area consid two main line research one hand method tri select appropri algorithm given task instanc schaf fer select cross valid schaffer hand method combin predict differ algorithm instanc stack gener wolpert work present near follow second line research instead look method fit data use singl represent languag present famili algorithm gener name cascad gener whose search space contain model use differ represent languag cascad gener perform iter composit classifi iter classifi gener input space extend addit new attribut form probabl class distribut obtain exampl gener classifi languag final classifi languag use high level gener languag use term express languag low level cla sifier sens cascad gener gener unifi theori base theori gener earlier use form cascad gener perform loos coupl classi fier method appli local iter divideandconqu algorithm gener tight coupl classifi method refer local cascad gener implement gener decis tree interest relat multivari tree brodley utgoff neural network name cascad correl architectur fahlman gener local cascad gener describ analyz paper experiment studi show methodolog usual improv accuraci decreas theori size statist signific level next section review previou work area multipl model section present framework cascad gener section discuss strength weak propos method comparison approach multipl model section perform empir evalu cascad gener use uci data set section defin new famili multistrategi algorithm appli cascad gener local section empir evalu local cascad gener use uci data set section examin behavior cascad gener provid insight work last section summar main point work discuss futur research direct relat work combin classifi vote common method use combin classifi point ali pazzani strategi motiv bayesian learn theori stipul order maxim predict accuraci instead use singl learn model one ideal use model hypothesi space vote hypothesi weight posterior probabl hypothesi given train data sever variant vote method found machin learn literatur uniform vote opinion base classifi contribut final classif strength weight vote base classifi weight associ could chang time strengthen classif given classifi anoth approach combin classifi consist gener multipl model sever method appear literatur paper analyz biasvari analysi kohavi wolpert method mainli reduc varianc bag boost method mainli reduc bia stack gener metalearn varianc reduct method breiman propos bag produc replic train set sampl replac replic train set size origin data exampl appear other may appear onc replic train set classifi gener classifi use classifi exampl test set usual use uniform vote scheme boost algorithm freund schapir maintain weight exampl train set reflect import adjust weight caus learner focu differ exampl lead differ classifi boost iter algorithm iter weight adjust order reflect perform correspond classifi weight misclassifi exampl increas final classifi aggreg learn classifi iter weight vote weight classifi function accuraci bia reduct method wolpert propos stack gener techniqu use learn two level learn algorithm use determin output base classifi combin origin data set constitut level zero data base classifi run level level one data output base classifi anoth learn process occur use input level one data output final classif sophist techniqu cross valid could reduc error due bia chan stolfo present two scheme classifi combin arbit combin scheme base meta learn metaclassifi gener meta data built base predict base classifi arbit also classifi use arbitr among predict gener differ base classifi train set arbit select avail data use select rule exampl select rule select exampl whose classif base classifi cannot predict consist arbit togeth arbitr rule decid final classif base base predict exampl arbitr rule use predict arbit base classifi cannot obtain major later chan stolfo a framework extend use arbiterscombin hierarch fashion gener arbitercombin binari tree skalak present dissert discuss method combin classifi present sever algorithm base stack gener abl improv perform nearest neighbor classifi brodley present mc hybrid algorithm combin singl tree node univari test multivari test gener linear machin instanc base learner node mc use set ifthen rule perform hillclimb search best hypothesi space search bia given partit dataset set rule incorpor knowledg expert mc use dynam search control strategi perform automat model select mc build tree appli differ model differ region instanc space discuss result boost bag quit impress use iter ie gener classifi quinlan report reduct error rate quinlan argu techniqu mainli applic unstabl classifi techniqu requir learn system stabl obtain differ classifi small chang train set analysi biasvari decomposit error kohavi wolpert reduct error observ boost bag mainli due reduct varianc breiman reveal boost bag improv predict accuraci learn algorithm unstabl mention kohavi bauer main problem boost seem robust nois expect noisi exampl tend mi classifi weight increas exampl present sever case perform boost algorithm degrad compar origin algorithm also point bag improv dataset use experiment evalu conclud although boost averag better bag uniformli better bag higher accuraci boost bag mani domain due reduct bia boost also found frequent higher varianc bag boost bag requir consider number member model reli vari data distribut get divers set model singl learn algorithm wolpert say success implement stack gener classif task black art condit stack work still unknown exampl current hard fast rule say level gener use level gener one use k number use form level input space etc recent ting witten shown success stack gener requir use output class distribut rather class predict experi mlr algorithm a linear discrimin suitabl cascad gener consid learn set multidimension input vector yn output variabl sinc focu paper classif problem yn take valu set predefin valu yn fcl cl c g c number class classifi function appli train set construct model d gener model map input space x discret output variabl use predictor repres x d assign valu exampl x tradit framework classif task framework requir predictor x d output vector repres condit probabl distribut p pc p repres probabl exampl x belong class i ie p jx class assign exampl x one maxim last express commonli use classifi naiv bay discrimin classifi exampl way classifi eg c quinlan differ strategi classifi exampl requir chang obtain probabl class distribut defin construct oper x m repres model d train data d x repres exampl exampl x oper concaten input vector x output probabl class di tribut oper appli exampl dataset obtain new dataset cardin equal cardin ie number exampl exampl x equival exampl augment c new attribut c repres number class new attribut element vector class probabl distribut obtain appli classifi d exampl x repres formal follow ad repres applic model d data set repres effect dataset dataset contain exampl appear extend probabl class distribut gener model d cascad gener sequenti composit classifi gener level appli phi oper given train set l test set t two classifi gener proce follow use gener level data level level learn level train data classifi level test data step perform basic sequenc cascad gener classifi classifi repres basic sequenc symbol r previou composit could repres succinctli by which appli equat equival to simplest formul cascad gener possibl extens includ composit n classifi parallel composit classifi composit n classifi repres by case cascad gener gener n level data final model one given n classifi model could contain term form condit base attribut build previou built classifi variant cascad gener includ sever algorithm parallel could repres formal by run parallel oper return new data set l contain number exampl l exampl l contain n gamma theta cl new attribut cl number class algorithm set contribut cl new attribut illustr exampl exampl consid uci blake keogh merz data set monk monk data set describ artifici robot domain quit well known machin learn commun robot describ six differ attribut classifi one two class chosen monk problem known difficult task system learn decis tree attributevalu formal decis rule problem is the robot ok exactli two six attribut first valu problem similar pariti problem combin differ attribut way make complic describ dnf cnf use given attribut onli exampl origin train data present head bodi smile hold color tie class round round ye sword red ye ok round round no balloon blue no ok use tenfold cross valid error rate c naiv bay composit model c naiv bay crnaivebay oper follow level data gener use naiv bay classifi naiv bay build model origin train set model use comput probabl class distribut exampl train test set level obtain extend train test set probabl class distribut given naiv bay exampl shown earlier take form of head bodi smile hold color tie pok pnot ok class round round ye sword red ye ok round round no balloon blue no ok new attribut pok pnot ok probabl exampl belong class oknot ok c train level train data classifi level test data composit crnaivebay obtain error rate substanti lower error rate c naiv bay none algorithm isol captur underli structur data case cascad abl achiev notabl increas perform figur present one tree gener crnaivebay tree contain mixtur origin attribut smile tie new attribut construct naiv bay pok pnot ok root tree appear attribut pok attribut repres particular class probabl class ok calcul naiv bay decis tree gener c use construct attribut given naiv bay redefin differ threshold two class problem bay rule use p ok threshold decis tree set threshold decis node kind function given bay strategi exam ple attribut pok seen function comput pclass okjx use bay theorem branch decis tree perform one test class probabl certain sens decis tree combin two represent languag naiv bay languag decis tree construct step perform cascad insert new attribut incorpor new knowledg provid naiv bay new knowledg allow signific increas perform verifi decis tree despit tie smile smile ok tie smile ok pnot ok pnot ok pnot ok ok pnot ok pnot ok ok ok ok ok ok figur tree gener crbay fact naiv bay cannot fit well complex space cascad framework lower level learner delay decis high level learner kind collabor classifi cascad gener explor discuss cascad gener belong famili stack algorithm wolpert defin stack gener gener framework combin cla sifier involv take predict sever classifi use predict basi next stage classif cascad gener may regard special case stack gener mainli due layer learn structur aspect make cascad gener novel are ffl new attribut continu take form probabl class distribut combin classifi mean categor class loos strength classifi predict use probabl class distribut allow us explor inform ffl classifi access origin attribut new attribut built lower layer consid exactli way origin attribut ffl cascad gener use intern cross valid aspect affect comput effici cascad mani idea discuss literatur ting use probabl class distribut level attribut use origin attribut possibl use origin attribut class predict level attribut point wolpert origin paper stack gener aliz skalak refer schaffer use origin attribut class predict level attribut disappoint result view could explain fact combin three algorithm similar behavior biasvari analysi decis tree rule neuralnetwork see section detail point chan stolfo a use origin attribut class predict scheme denot classattribut combin mix result exploit aspect make cascad gener succeed more over particular combin impli conceptu differ ffl stack parallel natur cascad sequenti effect intermedi classifi access origin attribut plu predict low level classifi interest possibl explor paper provid classifi n origin attribut plu predict provid classifi ffl ultim goal stack gener combin predict goal cascad gener obtain model use term represent languag lower level classifi ffl cascad gener provid rule choos low level classifi high level classifi aspect develop follow section empir evalu algorithm ali pazzani tumer gosh present empir analyt result show the combin error rate depend error rate individu classifi correl among them suggest use radic differ type classifi reduc correl error criterion select algorithm experiment work use three classifi differ behavior naiv bay linear discrimin decis tree naiv bay bay theorem optim predict class unseen exampl given train set chosen class one maxim pc px attribut independ pxjci decompos product px show procedur surprisingli good perform wide varieti domain includ mani clear depend attribut implement algorithm requir probabl estim train set case nomin attribut use count continu attribut discret equal size interv found produc better result assum gaussian distribut domingo pazzani j dougherti r kohavi m sahami number bin use function number differ valu observ train set lognr differ valu heurist use dougherti et al good overal result miss valu treat anoth possibl valu attribut order classifi queri point naiv bay classifi use avail attribut langley state naiv bay reli import assumpt variabl dataset summar singl probabilist descript suffici distinguish class analysi biasvari impli naiv bay use reduc set model fit data result low varianc data cannot adequ repres set model obtain larg bia linear discrimin linear discrimin function linear composit attribut maxim ratio betweengroup varianc withingroup varianc assum attribut vector exampl class c independ follow certain probabl distribut probabl densiti function f new point attribut vector x assign class probabl densiti function f x maxim mean point class distribut cluster center boundari separ two class hyperplan michi spiegelhalt taylor two class uniqu hyperplan need separ class gener case q class need separ them appli linear discrimin procedur describ below get hyperplan equat hyperplan given by use singular valu decomposit svd comput gamma svd numer stabl tool detect sourc collinear last aspect use method reduc featur linear combin linear discrimin use all almost all avail attribut classifi queri point breiman state analysi biasvari linear discrimin stabl classifi achiev stabil limit set model fit data result low varianc data cannot adequ repres set model obtain larg bia decis tree dtree version univari decis tree use standard algorithm build decis tree split criterion gain ratio stop criterion similar c prune mechan similar pessimist error c dtree use kind smooth process usual improv perform tree base classifi classifi new exampl exampl travers tree root leaf dtree exampl classifi take account class distribut leaf also class distribut node path is node path contribut final classif instead comput class distribut path tree classif time done buntin dtree comput class distribut node grow tree done recurs take account class distribut current node predecessor current node use recurs bayesian updat formula pearl p e n probabl one exampl fall node n seen shorthand p e en e repres given exampl en set exampl node n similarli p e n je n probabl one exampl fall node n goe node n p e n je probabl one exampl class c goe node n node n recurs formul allow dtree comput effici requir class distribut smooth class distribut influenc prune mechan treatment miss valu relev differ c decis tree use subset avail attribut classifi queri point breiman among research note decis tree unstabl classifi small variat train set caus larg chang result predictor high varianc fit kind data bia decis tree low experiment methodolog chosen data set uci repositori previous use compar studi estim error rate algorithm given dataset use fold stratifi cross valid minim influenc variabl train set repeat process ten time time use differ permut dataset final estim mean error rate obtain run cross valid iter cv algorithm train train partit data classifi also evalu test partit data algorithm use default set comparison algorithm perform use pair ttest signific level set dataset use wilcoxon matchedpair signedrank test compar result algorithm across dataset goal empir evalu show cascad gener plausibl algorithm compet quit well well establish techniqu stronger statement done extens empir evalu tabl data characterist result base classifi dataset class exampl dtree bay discrim c c australian sigma sigma sigma sigma sigma balanc sigma band sigma sigma sigma sigma sigma diabet sigma german sigma glass sigma ionospher iri sigma sigma letter satimag sigma segment vehicl vote tabl present error rate standard deviat base classifi rel algorithm gamma sign first column mean error rate algorithm significantli better wors dtree error rate c present refer result provid evid more singl algorithm better overal evalu cascad gener tabl present result pairwis combin three base classifi promis combin three model column correspond cascad gener combin combin conduct pair ttest composit model compar compon use pair ttest signific level set gamma sign indic combin eg crbay significantli better compon algorithm ie c bay result summar tabl first line show arithmet mean across dataset show promis combin crdiscrim crnaiv bay crdiscrimrna bay crnaiv tabl result cascad gener composit model compar compon bayrbay bayrdi bayrc disrdi disrbay disrc australian sigma sigma balanc sigma band sigma sigma sigma sigma sigma breast credit sigma sigma sigma sigma sigma sigma diabet german glass heart ionospher sigma sigma sigma sigma iri letter segment sonar sigma sigma sigma sigma vehicl vote sigma bayesrdiscrim confirm second line show geometr mean third line show averag rank base cascad algorithm comput dataset assign rank accur algorithm rank second best on remain line compar cascad algorithm toplevel algorithm fourth line show number dataset toplevel algorithm accur correspond cascad algorithm versu number less fifth line consid dataset error rate differ signific level use pair ttest last line show pvalu obtain appli wilcoxon matchedpair signedrank test statist show promis combin use decis tree highlevel classifi naiv bay discrim lowlevel classifi new attribut built discrim naiv bay express relat attribut outsid scope dnf algorithm like c new attribut systemat appear root composit model one main problem combin classifi is algorithm combin empir evalu suggest ffl combin classifi differ behavior biasvari analysi tabl result cascad gener composit model compar compon dataset crc crdi crbay crdiscrbay crbayrdisc stack gen australian sigma sigma sigma sigma sigma sigma balanc band sigma sigma sigma sigma sigma sigma breast credit sigma sigma sigma sigma sigma diabet german glass sigma sigma sigma sigma sigma sigma hepat ionospher sigma iri letter segment sigma sonar sigma sigma sigma sigma sigma sigma vote tabl summari result cascad gener measur bay bayrbay bayrdi bayrc disc discrdisc discrbay discrc arithmet mean geometr mean averag rank nr win gamma test ffl low level use algorithm low varianc ffl high level use algorithm low bia cascad framework lower level learner delay final decis high level learner select learner low bia high level abl fit complex decis surfac take account stabl surfac drawn low level learner tabl summari result cascad gener measur c crc crbay crdi crdisrbay crbayrdi arithmet mean geometr mean averag rank nr win gamma test tabl summari comparison stack gener crdiscrbay vs stackg crbayrdisc vs stackg number win signific win test given equal perform would prefer fewer compon classifi sinc train ing applic time lower smaller number compon larger number compon also advers affect comprehens studi version three compon seem perform better version two compon research need establish limit extend scenario comparison stack gener compar variou version cascad gener stack gener aliz defin ting reimplement stack gener level classifi c bay level classifi discrim attribut level data probabl class distribut obtain level classifi use fold stratifi crossvalid tabl show last column result stack gener stack gener compar use pair ttest crdiscrimrna bay crnaiv bayesrdiscrim order gamma sign indic dataset cascad model perform significantli better wors tabl present summari result provid evid gener abil cascad gener model competit stack gener comput level attribut use intern crossvalid use intern crossvalid affect cours learn time cascad model least three time faster stack gener cascad gener exhibit good gener abil comput effici aspect lead hypothesi improv cascad gener appli iter divideandconqu algorithm hypothesi examin next section local cascad gener mani classif algorithm use divid conquer strategi resolv given complex problem divid simpler problem appli recurs strategi subproblem solut subproblem combin yield solut origin complex problem basic idea behind well known decis tree base algorithm id quinlan cart breiman et al assist kononenko et al c quin lan power approach deriv abil split hyper space subspac fit subspac differ function section explor cascad gener problem subproblem divid conquer algorithm gener intuit behind propos method behind divid conquer strategi relat captur global level discov simpler subproblem follow section present detail appli cascad gener local develop strategi decis tree although possibl use conjunct divid conquer method like decis list rivest local cascad gener algorithm gener composit classif algorithm elabor build classifi given task iter divid conquer algorithm local cascad gener extend dataset insert new attribut new attribut propag subtask paper restrict use local cascad gener decis tree base algorithm howev possibl use divideandconqu algorithm figur present gener algorithm local cascad gener restrict decis tree method refer cgtree grow tree new attribut comput decis node appli phi oper new attribut propag tree number new attribut equal number class appear exampl node number vari differ level tree gener deeper node may contain larger number attribut parent node could disadvantag howev number new attribut gener decreas rapidli tree grow class discrimin deeper node also contain exampl decreas number class mean tree grow number new attribut decreas order appli predictor cgtree must store node model gener base classifi use exampl node classifi new exampl exampl travers tree usual way input data set d classifi output decis tree function cgtreed stop return leaf class probabl distribut els choos attribut maxim split criterion partit exampl base valu attribut gener subtre ree return tree contain decis node base attribut ai store d descend subtre ree endif end figur local cascad algorithm base decis tree decis node extend insert probabl class distribut provid base classifi predictor node framework local cascad gener develop cgltree use phid adiscrimd d oper construct step intern node cgltree construct discrimin function discrimin function use build new attribut exampl valu new attribut comput use linear discrimin function decis node number new attribut built cgltree alway equal number class taken exampl node order restrict attent well popul class use follow heurist consid class number exampl node belong class greater n time number attribut default n impli differ node differ number class consid lead addit differ number new attribut anoth restrict use construct oper ad d error rate result classifi less train data empir studi use two algorithm appli cascad gener local first one cgbtree use construct oper second one cgbltree use construct oper aspect algorithm similar cgltree one restrict applic phid induc classifi d must return correspond probabl class distribut x classifi satisfi requisit could appli possibl imagin cgtree whose intern node tree themselv exampl small modif c enabl construct cgtree whose intern node tree gener c illustr exampl bayes_ bayes_ bayes_ ok ok ok ok figur tree gener cgtree use discrimrbay construct oper figur repres tree gener cgtree monk problem construct oper use is phid discrimrbayesx d root tree naiv bay algorithm provid two new attribut bay bay linear discrimin use continu attribut onli two continu attribut built naiv bay case coeffici linear discrimin shrink zero process variabl elimin use discrimin algorithm gain ratio criterion choos bay attribut test dataset split two partit one contain exampl class ok leaf gener partit two new bay attribut built bay bay linear discrimin gener base two bay attribut built root tree attribut base linear discrimin chosen test attribut node dataset segment process tree construct proce exampl illustr two point ffl interact classifi linear discrimin contain term built naiv bay whenev new attribut built consid regular attribut attribut combin built deeper node contain term base attribut built upper node ffl reus attribut differ threshold attribut bay built root use twice tree differ threshold relat work multivari tree respect final model clear similar cgltree multivari tree brodley utgoff langley refer multivari tree topolog equival threelay infer network construct abil system similar cascad correl learn architectur fahlman lebier also final model cgbtree relat recurs naiv bay present langley interest featur unifi singl framework sever system differ research area previou work gama brazdil compar system ltree similar cgltree oc murthi et al lmdt brodley et al cart breiman et al focu paper methodolog combin classifi such compar algorithm method gener combin multipl model evalu local cascad gener section evalu three instanc local cascad algorithm cgbtree cgltree cgbltree compar local version correspond global model two standard method combin classifi boost stack gener implement local cascad gener algorithm base dtree use exactli split criteria stop criteria prune mechan moreov share mani minor heurist individu small mention collect make differ decis node cgltree appli linear discrimin describ abov cg btree appli naiv bay algorithm cgbltree appli linear discrimin order attribut naiv bay categor attribut order prevent overfit construct new attribut constrain depth addit level prune greater level prune dtree tabl present result local cascad gener column correspond local cascad gener algorithm algorithm compar similar cascad model use pair ttest exampl cgltree compar crdiscrim gamma sign mean error rate composit model is statist signific level lower higher correspond model tabl present compar summari result local cascad gener correspond global model illustr benefit appli cascad gener local tabl result aloc cascad gener bboost stack c boost cascad algorithm second row indic model use comparison dataset cgbtree cgltree cgbltree cboost stack cbrbay vs correspond cascad model vs cgbltree vs cboost adult sigma sigma sigma gamma sigma sigma sigma australian balanc sigma band sigma sigma sigma credit sigma sigma sigma sigma sigma sigma diabet german glass ionospher sigma sigma sigma iri sigma sigma letter mushroom sonar sigma vehicl vote sigma sigma system cgbltree compar cboost varianc reduct method stack gener bia reduct method tabl b present result cboost default paramet aggreg tree stack gener defin ting describ earlier section boost stack compar cgbltree use pair ttest signific level set gamma sign mean boost stack perform significantli better wors cgbltree studi cgbltree perform significantli better stack dataset wors dataset step ahead compar cboost cgbltree significantli improv dataset lose dataset interest note dataset signific differ evid boost cascad differ behavior improv observ boost appli decis tabl summari result local cascad gener cgbtree cgltree cgbltree cboost stack g cbrbay arithmet mean geometr mean averag rank crbay crdi crbayrdi cgbltree cgbltree vs vs vs vs vs cgbtree cgltree cgbltree cboost stack g number win signific win test tree mainli due reduct varianc compon error rate while cascad algorithm improv mainli due reduct bia compon tabl c present result boost cascad algo rithm case use global combin c boostrnaiv bay improv cboost dataset lose summari result present tabl evid promis result intend near futur boost cgbltree number leav anoth dimens comparison involv measur number leav correspond number differ region instanc space partit algorithm consequ seen indic model complex almost dataset cascad tree split instanc space half region need dtree c clear indic cascad model captur better underli structur data learn time learn time dimens compar classifi comparison less clear result may strongli depend implement detail well underli hardwar howev least order magnitud time complex use indic c cboost run sparc machin algorithm run pentium mhz mb machin linux tabl present averag time need algorithm run dataset take time naiv bay refer result demonstr cgtree faster cboost cboost slower gener tree increas complex also cgtree faster stack gener due intern cross valid use stack gener tabl rel learn time base composit model bay discrim c bayrdi disrdi c dtree bayrbay disrbay bayrc disrc crdi crc crbay cgbtree crdisrbay cgltree cgbltree cboost stack cascad gener improv perform cascad gener local cascad gener transform instanc space new highdimension space principl could turn given learn problem difficult one phenomenon known curs dimension section analyz behavior cascad gener three dimens error correl biasvari analysi mahalanobi distanc correl ali pazzani shown desir properti ensembl classifi divers use concept error correl metric measur degre divers ensembl definit error correl two classifi defin probabl make error definit satisfi properti correl object prefer defin error correl two classifi condit probabl two classifi make error given one make error definit error correl lie interv correl one classifi formula use provid higher valu one use ali pazzani expect lowest degre correl decis tree bay decis tree discrim use differ represent languag error correl bay discrim littl higher despit similar two algorithm use differ search strategi result provid evid decis tree discrimin function make uncorrel error classifi make error differ region instanc space desir properti combin classifi tabl error correl base classifi c vs bay c vsdiscrim bay vs discrim averag biasvari decomposit biasvari decomposit error tool statist theori analyz error supervis learn algorithm basic idea consist decompos expect error three compon x comput term bia varianc zeroon loss function use decomposit propos kohavi wolpert bia measur close averag guess learn algorithm match target comput as x varianc measur much learn algorithm guess bounc around differ set given size comput as varianc x estim bia varianc first split data train test set train set obtain ten bootstrap replic use build ten classifi ran learn algorithm train set estim term varianc equat bia equat use gener classifi point x evalu set e term estim use frequenc count base algorithm use experiment evalu differ behavior biasvari analysi decis tree known low bia high varianc naiv bay linear discrimin known low varianc high bia experiment evalu shown promis combin use decis tree high level classifi naiv bay linear discrimin low level classifi illustr result measur bia varianc c naiv bay crnaiv bay dataset studi result shown figur summari result present tabl figur biasvari decomposit error rate c bay crbay differ dataset tabl bia varianc decomposit error rate bay crbay averag varianc averag bia benefit cascad composit well illustr dataset like balancescal hepat monk waveform satimag comparison bay crbay show latter combin obtain strong reduct bia compon cost increas varianc compon crbay reduc bia varianc compar c reduct error mainli due reduct bia mahalanobi distanc consid class defin singl cluster euclidean space class i centroid correspond cluster defin vector attribut mean x comput exampl class shape cluster given covari matrix use mahalanobi metric defin two distanc withinclass distanc defin mahalanobi distanc exampl centroid cluster comput as figur averag increas betweenclass distanc x repres exampl attribut vector denot centroid cluster correspond class i covari matrix class i betweenclass distanc defin mahalanobi distanc two cluster comput as pool denot centroid cluster correspond class i pool pool covari matrix use j intuit behind withinclass distanc smaller valu lead compact cluster intuit behind betweenclass distanc larger valu would lead us believ group suffici spread term separ mean measur betweenclass distanc withinclass distanc dataset numer attribut distanc measur origin dataset dataset extend use cascad algorithm observ withinclass distanc remain almost constant between class distanc increas exampl use construct oper discrimrbay betweenclass distanc almost doubl figur show averag increas betweenclass distanc respect origin dataset extend use discrim bay discrimrbay respect conclus futur work paper provid new gener method combin learn model mean construct induct basic idea method use learn algorithm sequenc iter two step process occur first model built use base classifi second instanc space extend insert new attribut gener built model given exampl construct step gener term represent languag base classifi high level classifi choos one term represent power extend bia restrict high level classifi relax incorpor term represent languag base classifi basic idea behind cascad gener architectur examin two differ scheme combin classifi first one provid loos coupl classifi second one coupl classifi tightli loos coupl base classifi preprocess data anoth stage framework use combin exist classifi without chang rather small chang method requir origin data extend insert probabl class distribut must gener base classifi tight coupl local construct induct framework two classifi coupl local although work use local cascad gener conjunct decis tree method could easili extend divideandconqu system decis list exist method bag boost combin learn model use vote strategi determin final outcom although lead improv accuraci strong limit loss interpret model easier interpret particularli classifi loos coupl final model use represent languag high level classifi possibl enrich express represent languag low level classi fier cascad gener appli local model gener difficult interpret gener loos coupl classifi new attribut built deeper node contain term base previous built attribut allow us built complex decis surfac affect somewhat interpret final model use power represent necessarili lead better result introduc flexibl lead increas instabl varianc need control local cascad gener achiev limit depth applic construct oper requir error rate classifi use construct oper less one interest featur local cascad gener provid singl framework collect differ method method relat sever paradigm machin learn exampl similar multivari tree brodley utgoff neural network fahlman lebier recurs bay langley multipl model name stack gener wolpert previou work gama brazdil present system ltree combin decis tree discrimin function mean construct induct local cascad combin extend work ltree construct oper singl discrimin function local cascad composit restrict re lax use classifi construct oper moreov composit sever classifi like cgbltree could use unifi framework use overcom superfici distinct enabl us studi fundament one practic perspect user task simplifi aim achiev better accuraci achiev singl algorithm instead sever one done effici lead reduc learn time shown methodolog improv accuraci base classifi compet well method combin classifi preserv abil provid singl albeit structur model data limit futur work open issu could explor futur involv ffl perspect biasvari analysi main effect propos methodolog reduct bia compon possibl combin cascad architectur varianc reduct method like bag boost ffl cascad gener work classifi could use neural network nearest neighbor think methodolog present work type classifi intend verifi empir futur problem involv basic research includ ffl cascad gener improv perform experiment studi suggest combin algorithm complementari behavior point view biasvari analysi form complementar consid exampl search bia so one interest issu explor is given dataset predict algorithm complementari ffl cascad gener improv perform dataset cascad abl improv perform base classifi character dataset is predict circumst cascad gener lead improv perform ffl mani base classifi use gener prefer smaller number base classifi circumst reduc number base classifi without affect perform ffl cascad gener architectur provid method design algorithm use multipl represent multipl search strategi within induct algorithm interest line futur research explor flexibl induct strategi use sever divers represent possibl extend local cascad gener provid dynam control make step direct acknowledg gratitud express financi support given feder praxi xxi project eco plurianu support attribut liacc esprit ltr project thank also pedro domingo anonym review colleagu liacc valuabl comment note except case adult letter dataset singl fold crossvalid use also evalu stack gener use c top level version use somewhat better use c top level averag mean error rate heurist suggest breiman et al two differ method present ting gama prefer cboost instead bag avail us allow crosscheck result differ result previou publish quinlan think may due differ method use estim error rate except monk dataset dtree c produc tree one leaf run time c cboost reduc factor suggest in wwwspecorg intrins nois train dataset includ bia term analysi assum singl domin class cluster although may alway satisfi give insight behavior cascad composit r reduct learn multipl descript empir comparison vote classif algorithm bag uci repositori machin learn databas arc classifi classif regress tree wadsworth intern group recurs automat bia select classifi construct multivari decis tree multivari analysi optim simpl bayesian classifi zeroon loss supervis unsupervis discret continu featur recurr cascadecorrel architectur experi new boost algorithm combin classifi construct induct linear tree combin classif procedur induct recurs bayesian classifi element machin learn machin learn machin learn system induct obliqu decis tree journal artifici intellig research probabilist reason intellig system network plausibl infer learn decis list select classif method crossvalid prototyp select composit nearest neighbor classifi stack gener work correl error reduct ensembl classifi connect scienc stack gener tr probabilist reason intellig system network plausibl infer recurr cascadecorrel architectur stack gener c program machin learn bitechn noteib multivari decis tree element machin learn recurs automat bia select classifi construct reduct learn multipl descript prototyp select composit nearest neighbor classifi optim simpl bayesian classifi zeroon loss machin learn empir comparison vote classif algorithm learn decis list induct decis tree induct recurs bayesian classifi combin classifi construct induct ctr robert munro daren ler jon patrick metalearn orthograph contextu model languag independ name entiti recognit proceed seventh confer natur languag learn hltnaacl p may edmonton canada csar ferri peter flach jo hernndezorallo deleg classifi proceed twentyfirst intern confer machin learn p juli banff alberta canada saddi segrera mara n moreno experiment compar studi web mine method recommend system proceed th wsea intern confer distanc learn web engin p septemb lisbon portug ljupo todorovski sao deroski combin classifi meta decis tree machin learn v n p march joo gama function tree machin learn v n p june huimin zhao sudha ram entiti identif heterogen databas integr multipl classifi system approach empir evalu inform system v n p april zeng dan pan jianbin he predict mhc iibind peptid use rough setbas rule set ensembl appli intellig v n p octob s b kotsianti i d zaharaki p e pintela machin learn review classif combin techniqu artifici intellig review v n p novemb