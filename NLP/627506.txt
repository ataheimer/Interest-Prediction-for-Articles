t gener neural network a author discuss requir learn gener tradit method base gradient descent limit success stochast learn algorithm base simul anneal weight space present author verifi converg properti feasibl algorithm implement algorithm valid experi describ b introduct neural network appli wide varieti applic speech gener handwrit recognit last decad seen great advanc design neural network class problem call recognit problem design learn algorithm learn weight neural network mani recognit problem longer difficult task howev design neural network gener problem well understood domain neural network applic classifi two broad categori recognit gener class first train neural network set inputoutput pair i n n recognit problem train network test previous seen input j j n corrupt nois shown fig train network expect reproduc output j correspond j spite nois shape recognit handwrit recognit exampl recognit prob lem hand gener problem train neural network test input n distinct input i n use train network shown fig network expect correctli predict output n input n model learn train typic exampl gener problem bond rate robot neural network gener problem import sinc mani applic enorm import real world would benefit work mani applic difficult success appli either convent mathemat techniqu eg statist regress standard ai approach eg rule base system neural network gener abil use domain requir priori specif function domain model rather attempt learn underli domain model train inputoutput exampl r n e n n r n e n n no n no n figur class problem learn algorithm gener problem differ learn algorithm recognit problem recognit problem network expect reproduc one previous seen out put network may rememb output input fit curv use train ing rememb output one often use larg network mani node weight howev memor learn sampl suit gener problem sinc lead wors perform predict output unseen input furthermor gener problem allow small amount error output predict network henc fit curv need pass i pair use train network address gener problem may instead fit simpl curv eg low degre polynomi basic analyt function like logx sinex tangentx etc inputoutput pair rather fit crook curv neural network use gener problem tend simpler small number hidden node layer interconnect edg weight enabl one use comput sophist algorithm earlier work neural network relat recognit problem littl research toward develop neural network model gener problem present new learn algorithm stochast backpropag gener problem verifi converg algorithm provid theoret argument toward capabl propos algorithm discov optim weight also describ implement algorithm experi algorithm solv gener problem problem formul gener problem neural network formul three differ way a analyt construct function learn c symbol semant network analyt formal focus exist network capabl gener also provid worst case time complex discov network solv arbitrari gener problem provid way discov network construct function learn formal approach gener problem complementari fashion studi algorithm discov network solv class gener problem aim discov function f map input domain output domain set learn exampl function discov may defin boolean number real number input output assum number symbol mean function network repres symbol mean beyond numer comput third approach symbol semant network associ symbol mean network gener occur attach new node appropri parent node network inherit properti parent classif formul gener problem includ special case exampl signal detect problem consid special case task signal detect problem learn recogn paramet function f o rather learn function recogn frequenc given sinusoid function exampl signal detect problem backpropag neural network appli success problem focu construct function learn formul term learn numer function simpl neural network describ direct graph e vertex set v three kind node a input node leav b hidden node intern node c output node root edg e e associ weight w j shown fig network use comput output set input node comput function weight sum input signal g inset output function g map given input network would comput output finput invers problem discov function f ie set edg weight given set inputoutput pair refer learn problem state follow given set exampl input output pair i o i n o n find weight edg e j e neural network network map j j jn close possibl repres possibl infinit domain input repres possibl infinit rang output dimension featur space f f k describ input input j consid ktupl cartesian space f f f k given learn sampl per sampl error function e real i n n gener involv find map function f neural network exampl symbol node edg weight output input input w figur feedforward neural network minim error function e entir domain i particular f learn sampl reduc error function entir domain look small subset difficult arbitrari learn sampl arbitrari input domain gener problem often simplifi make assumpt domain learn sampl assum repres entir domain adequ valu e estim valu e function f assum smooth continu well behav domain rang function f boolean set set real number usual one function fit given set learn sampl inputoutput pair make gener problem harder exampl learn specif boolean function subset domain difficult sinc sever boolean function domain fit learn sampl littl consensu criteria prefer one candid boolean function rest break tie restrict attent function set real number draw upon notion simplic function real number choos one function among set possibl function fit learn sampl simplic intuit defin term number maxima minima function simplic reduc notion degre polynomi polynomi function stochast backpropag gener idea behind algorithm use simul anneal weight space weight space defin collect configur w connect weight neural network simul anneal procedur search weight space configur w opt minim errortofit function ew search procedur base mont carlo method given current configur w network character valu weight small randomli gener perturb appli small chang randomli chosen weight differ error de current configur w slightli perturb one neg ie perturb result lower error fit process continu new state de probabl accept new configur given exp dek b t occasion transit higher error configur help search process get local minima accept rule new configur refer metropoli criteria follow criteria probabl distribut configur approach boltzman distribut given eqa zt normal factor known partit function depend temperatur boltzman constant factor exp ek b t known boltzman factor denot control paramet call temperatur due histor reason start high valu temperatur decreas slowli execut algorithm temperatur decreas boltzman distribut concentr configur lower error final temperatur approach zero minimum error configur nonzero probabl occur way get global optim weight network minim error fit train exampl provid maximum temperatur suffici high cool carri suffici slowli algorithm descript one defin configur cost function gener mechanismor equival neighborhood structur describ algorithm assum weight take discret valu set sddddsd restrict weight discret valu limit learn abil neural network gener problem configur defin ntupl weight n number weight network configur space construct allow weight take valu y cost function defin error desir output network output learn exampl shown below j c refer jth network output input cth train exampl j c refer jth desir output cth train exampl indic c refer differ output network variou train exampl respect gener neighbor configur chang one randomli chosen weight element configur d use uniform probabl distribut chose weight chang thu probabl gener neighbor configur current configur uniformli distribut neighbor one alway choos larg valu scale input output data valu small rang achiev better accuraci procedur begin not stop_criterion system frozen repeat accept fals where w lm chang perturb els exp c accept updateconfigur j equilibrium_is_approached_sufficiently_clos c m fc fig stochast backpropag algorithm psuedopasc ______________________________________________________________________________ psuedopasc descript stochast backpropag shown fig initi routin assign default valu variabl particular current configur temperatur outer_iteration_count m loop correspond simul anneal weight space inner loop repres simul anneal fix valu control paramet t execut probabl distribut current configur possibl configur becom stabl help us achiev boltzman distribut outer loop chang control paramet slowli final valu near correspond slow cool achiev configur global minimum error step insid innermost loop combin backpropag transit simul anneal use backprop backpropag algo rithm subroutin comput error deriv e w lm respect variou weight deriv help us estim chang error function particular weight chang d perturb produc neighbor configur chang randomli chosen weight d chang error function due perturb estim use error deriv obtain backpropag algorithm new configur accept uncondit iff lower error current one otherwis new configur accept probabl final program variabl updat updat state newli chosen configur note accept criterion implement draw random number uniform distribut compar exp eit basic algorithm shown fig made effici one chang function backprop procedur notic backpropag produc partial deriv e respect weight wherea use one deriv subsequ comput better modifi backpropag algorithm comput one deriv requir comparison exist algorithm exist learn algorithm eg hopfield net work base memor learn exampl accur cannot use predict gener problem two flexibl learn method includ backpropag boltzman machin learn iter algorithm base gradient descent error surfac backpropag error fit given set weight defin eq b j c actual state unit j inputoutput train exampl c j c desir state backpropag algorithm comput gradient error respect weight hidden unit j layer j affect error via effect unit k next layer k deriv error e y j given eq b index c suppress clariti dy k weight chang direct reduc error output one may chang weight simultan avoid conflict local weight adjust boltzman machin stochast natur aim learn weight achiev probabl distribut inputoutput map boltzman machin learn base seri simul anneal state space network state space network character defin state net work state node describ output state network ntupl vector one compon node learn algorithm aim achiev certain probabl distribut state gradient descent minim error probabl distribut use simul anneal weight space quit differ boltzman machin simul anneal carri state space network learn method work costerror surfac concav sinc algorithm base simpl heurist gradient descent get stuck local minima furthermor get stuck plateau gradient small shown fig algorithm cannot guarante optim discov weight learn problem npcomplet gener remain npcomplet sever restrict surpris heurist learn algorithm like backpropag alway work cannot trust find global optim weight gener problem two way approach npcomplet problem a approxim method b stochast enumer method simul anneal sinc difficult formul gener approxim method neural network learn use simul anneal method extend backpropag algorithm stochast weight chang learn weight algorithm converg properti achiev global optim weight simpl network gener implement perform studi show algorithm perform well mani gener problem global minima figur two bad case gradient descent model analysi stochast backpropag given neighborhood structur stochast backpropag view algorithm continu attempt transform current configur one neighbor mechan mathemat describ mean markov chain sequenc trial outcom trial depend outcom previou one case stochast backpropag trial correspond transit clear outcom transit depend outcom previou one ie current confi gurat markov chain describ mean set condit probabl pair outcom probabl outcom kth trial j given outcom k th trial i let k denot probabl outcom kth trial k obtain solv recurs l kp li kk kc sum taken possibl outcom hereinaft xk denot outcom kth trial henc condit probabl depend k correspond markov chain call homogen otherwis call inhomogen case stochast backpropag condit probabl denot probabl kth transit transit configur configur j thu xk configur obtain k tran sition view thi call transit probabl r r matrix pkk transit matrix r denot size configur space transit probabl depend valu control paramet temperatur thu kept constant correspond markov chain homogen transit matrix defin l li g il t il t forallji ie transit probabl defin product follow two condit probabl gener gener configur j configur i accept probabl ij t accept configur j gener configur i correspond matric gt at call gener accept matric respect result definit eqc pt stochast matrix ie gt repres uniform distribut neighborhood sinc transit implement choos random neighbor configur j current configur i at comput metropoli criteria ie min expdek b t stochast backpropag algorithm attain global minimum possibl larg number transit say k follow relat hold r opt set global optim configur minimum error fit shown eqc hold asymptot ie lim prx k r opt certain condit matrix at l gt l satisfi certain addit condit matrix at k rate converg sequenc t k faster o log k proof carri three step a show exist stationari distribut homogen markov chain b show converg innerloop stochast backpropag stationari distribu tion c show stationari distribut final frozen system nonzero probabl optim configur onli proof last step conting cool rate provid us bound rate exist stationari distribut follow theorem establish exist stationari distribut theorem feller stationari distribut q finit homogen markov chain exist markov chain irreduc aperiod furthermor vector q uniqu determin follow equat note q left eigenvector matrix p eigenvalu markov chain irreduc pair configur ij posit probabl reach j finit number transit ie markov chain aperiod configur r greatest common divisor integ n equal case stochast backpropag matrix p defin eq c sinc definit guarante foral ijc suffici irreduc check markov chain induc gt irreduc ie g lk lk t kp c establish aperiod one use fact irreduc markov chain aperiod follow condit satisfi thu aperiod suffici assum use inequ eq c fact foral prove follow l lit r l t g l t l lit jt r l t g l t l lit jt r g l t l lit r g l t l r g l thu p l lit r l t g l t c thu eq c hold ii summar follow result homogen markov chain condit probabl given eq c stationari distribut matric at gt satisfi eq c c respec tive note stochast backpropag accept probabl defin henc eq c alway satisfi set foral r opt j r opt converg stationari distribut impos condit matric at gt ensur converg qt distribut p given eq c theorem two argument function yei e opt t taken ii t for arbitrari configur depend t stationari distribut qt given ii t provid matric at g satisfi follow condit proof theorem discuss elsewher implicitli assum accept probabl depend cost valu configur configur itself henc ii t depend particular choic sinc ensur eq c follow condit suffici thu condit cc guarante converg easili check matric gt at stochast backpropag meet condit cool rate certain condit matric at gt stochast backpropag algorithm converg global minimum probabl valu l control parameterl correspond markov chain infinit length l eventu converg l ie valid follow equat shown limq r opt howev cool rate constraint sequenc l control parameterl satisfi certain properti assur converg global optim configur particular k form g one guarante converg global optim configur converg result list condit simul anneal converg global optim configur formul stochast backpropag use similar accept probabl gener probabl cool schedul simul anneal algorithm use metropoli criteria accept cri teria configur gener mechan uniform distribut neighbor gener mechan across two neighbor configur symmetr one gener arbitrari configur given configur finit number step thu satisfi condit theorem guarante converg global minima follow cool schedul n logn g satisfi condit cool rate guarante converg global minima provid g greater depth local minima implement carri complet implement stochast backpropag conduct valid stu die implement algorithm unix platform sequenti machin eg sun sinc gener often take larg amount comput plan reimplement algorithm vector processor eg cray xmp speedup current prototyp base sourc code public domain softwar implement backpropag algorithm studi softwar reus pertin modul implement stochast backpropag implement compris line c code requir approxim seven man month design code debug larg fraction effort direct toward read understand backpropag softwar reus effort reward reduc time design code debug code provid g d maxim depth local minima error surfac verifi walkthrough method extens usag prototyp use seminar cours research backpropag packag three modul user interfac learn modul test modul user interfac implement command associ command intern function via tabl command allow user examin modifi state softwar choos option specifi type speed comput display learn modul implement backpropag algorithm comput error deriv weight adjust tune weight iter train repeat weight adjust fix number time till total squar error reach valu set user learn algorithm provid option adjust weight examin pattern examin pattern test modul comput output given input neural network also comput total squar error output produc network desir output specifi input pattern augment user interfac modul ad command examin modifi paramet stochast backpropag routin process command instal tabl associ command process routin command enabl user choos altern learn algorithm also ad implement stochast backpropag c simplifi cool schedul cool schedul base expotenti cool simplic effici cool schedul use mani applic main routin learn modul name trial modifi adjust weight base weight randomli choos neighbor accept metropoli criteria test modul alter implement valid evalu stochast backpropag learn algorithm gener two type function a monoton function b non monoton function experiment setup consist four modul data set gener neural network simul data collect data analysi shown fig data set gener modul use four function linear quadrat logarithm trigonometr shown tabl neural network simul implement altern learn algorithm backpropag stochast backpropag take network configur data set modul simul learn algorithm produc output well weight data collect modul compris set routin sampl state neural network simul period say everi epoch learn sampl weight collect termin learn data analysi modul produc graph statist algorithm monitor learn phase well test phase perform algorithm learn phase measur total squar error learn set inputoutput pair perform algorithm test phase measur per pattern error new set inputoutput pair distinct learn set behavior altern algorithm learn shown figur figur show chang total squar error along learn step monoton function ie linear logarithm quadrat function stochast backpropag backpropag yield compar total squar error test train network independ set sampl stochast backpropag train network yield error per pattern backpropag train network yield error per pattern network predict output sampl within desir output figur show chang total squar error epoch learn nonmonoton function ie trigonometr function stochast backpropag network configur monoton non monoton experiment setup data set backpropag stochast backprop neural network simul output input finat weight experi per datafil plot statist plot statist etc analysi modul weight input outpput weight sampl period intermedi data collect problem fig experi design perform comparison studi tabl function control studi yield better total squar error learn well test network train backpropag network yield per pattern error predict output sampl within desir out put network train stochast backpropag yield per pattern error predict output sampl within desir output initi stochast backpropog stochast backpropog backpropogationepoch figur learn monoton function initi stochast backpropog backpropogationepoch figur learn nonmonoton function conclus stochast backpropag provid feasibl learn algorithm gener problem reduc error fit train exampl critic gener problem stochast backpropag perform well monoton function use simpl network fewer hidden node perform better backpropag algorithm nonmonoton function stochast backpropag learn algorithm theoret properti converg also provid stochast guarante find optim weight howev experi confirm it one need tune paramet implement stochast backpropag get better result acknowledg acknowledg use help cs class spring urop univers minnesota backpropag packag r handwritten numer recognit multilay neural network improv learn algorithm learn learn intern represent back propag error learn algorithm boltzman machin linear function neuron structur train art adapt pattern recognit selforgan neural network brain style comput learn gener learn recogn shape parallel network learn translat invari recognit massiv parallel network design intellig robot feder geometr mchine neural comput use artifici neural net statist discoveri observ use backpropag beyond regress new tool predict analysi behaviori scienc connectionist expert system materi handl conserv domain neural connect propag represent continu function sever variablesbi superposit continu function one variabl addit mean gener ch map abil three layer train node neural net npcomplet complex load shallow network neural network design complex learn scale gener learn algorithm gener problem memor gener ch creat artifici equat state calcul fast comput machin neural network physic system emerg collect comput abil neural comput decis optim problem parallel distribut process explor microstructur cognit constraint stisfact machin learn complex connectionist learn variou node function comput interact guid theori npcomplet optim simul anneal introduct probabl theori applic probabilist hill climb algorithm properti applic theori applic cool schedul optim anneal explor parallel distribut process handbook model john wiley tr linear function neuron structur train connectionist expert system art adapt pattern recognit selforgan neural network cool schedul optim anneal complex load shallow neural network simul anneal boltzmann machin stochast approach combinatori optim neural comput neurocomput applic explor parallel distribut process neural network design complex learn train node neural network npcomplet parallel distribut process explor microstructur cognit vol creat artifici neural network gener design intellig robot feder geometr machin brain style comput comput intract learn translat invari recognit massiv parallel network complex connectionist learn variou node function ctr v kumar s shekhar m b amin scalabl parallel formul backpropag algorithm hypercub relat architectur ieee transact parallel distribut system v n p octob