t track best expert a gener recent rel loss bound onlin algorithm addit loss algorithm whole sequenc exampl loss best expert bound gener allow sequenc partit segment goal bound addit loss algorithm sum loss best expert segment model situat exampl chang differ expert best certain segment sequenc exampl singl segment case addit loss proport log n n number expert constant proportion depend loss function algorithm produc best partit howev loss bound show predict close best partit number segment k sequenc length ell bound addit loss algorithm best partit ok log nk logellk case loss per trial bound one obtain algorithm whose addit loss loss best partit independ length sequenc addit loss becom oklog n k loglk l loss best partitionwith k segment algorithm track predict best expert aresimpl adapt vovk origin algorithm singl best expert case origin algorithm keep one weight per expert spend o time per weight trial b introduct consid follow onlin learn model learn occur seri trial label trial goal predict outcom receiv end trial begin trial t algorithm receiv ntupl x element x ti ntupl x repres predict expert e valu outcom trial t algorithm produc predict base current expert predict tupl x past predict outcom end trial algorithm receiv outcom algorithm incur loss measur discrep predict outcom similarli expert incur loss well possibl goal minim total loss algorithm trial arbitrari sequenc instanc outcom pair such pair call exampl sinc make assumpt relationship predict expert outcom y alway sequenc author support nsf grant ccr extend abstract appear herbster warmuth m herbster m k warmuth far away predict particular algorithm thu minim total loss arbitrari sequenc exampl unreason goal refin relativ goal minim addit loss algorithm loss best expert whole sequenc expert larg loss goal might actual easi achiev sinc algorithm addit loss loss best expert may small howev least one expert predict well algorithm must learn quickli produc predict close predict best expert sens addit loss algorithm loss best expert bound expert framework might use variou set exampl expert might predict chanc rain likelihood stock market rise fall anoth set expert might variou subalgorithm recogn particular pattern master algorithm combin expert predict need know particular problem domain simpli keep one weight per expert repres belief expert predict decreas weight function loss expert previou work vovk vovk other littleston warmuth haussler kivinen warmuth produc algorithm upper bound addit loss algorithm loss best expert algorithm compar loss best expert call staticexpert algorithm paper addit loss bound algorithm form c ln n larg class loss function c constant depend loss function l n number expert class loss function contain essenti common loss function except absolut loss discret loss count predict mistak treat special case littleston warmuth vovk cesabianchi freund haussler helmbold schapir warmuth exampl loss function squar rel entropi loss respect see section definit loss function paper consid modif goal introduc littleston warmuth littleston warmuth sequenc exampl subdivid k segment arbitrari length distribut segment associ expert sequenc segment associ sequenc expert call partit loss partit sum total loss expert associ segment best partit size k partit k segment smallest loss modifi goal perform well rel best partit size k goal model real life situat natur exampl might chang differ expert produc better predict exampl pattern might chang differ subalgorithm may predict better differ segment onlin sequenc pattern seek design master algorithm track perform best sequenc expert sens incur small addit loss best partit size k whole sequenc exampl given ahead time one could comput best partit certain size associ expert use dynam program algorithm get exampl onlin never produc best partit even so abl bound addit loss best offlin partit arbitrari sequenc exampl trial k expert distinct partit immedi get good bound problem expand set n expert expert partitionexpert repres singl partit trial sequenc predict trial expert associ segment contain current trial thu use staticexpert algorithm obtain bound c ln addit loss algorithm loss best partit two problem first algorithm ineffici sinc number partitionexpert exponenti number partit second bound addit loss grow sequenc length abl overcom problem instead keep one weight exponenti mani partit get away keep one weight per expert done staticexpert algorithm track predict best partit essenti free n subalgorithm expert whose predict want combin staticexpert algorithm new master algorithm take on addit time per trial time requir simul n subalgorithm develop two main algorithm fixedshar algorithm variabl share algorithm base staticexpert algorithm maintain weight form e gammajt expert cf littleston warmuth vovk total past loss expert past trial trial master algorithm combin expert predict use current weight expert outcom trial receiv multipli weight everi expert e gammajl l loss expert current trial call updat weight loss updat modifi staticexpert algorithm ad addit updat obtain algorithm sinc model best expert may shift seri trial cannot simpli use weight form e gammajt expert optim segment loss prior segment may arbitrarili larg thu weight may becom arbitrarili small need modifi staticexpert algorithm small weight recov quickli reason expert share portion weight expert loss updat call share updat fixedshar variableshar algorithm first loss updat follow share updat differ algorithm share updat fraction expert weight ad weight expert fixedshar algorithm expert share fix fraction weight other guarante ratio weight expert total weight expert may bound below differ form lower bound weight use wml algorithm companion paper learn shift disjunct auer warmuth appear journal issu latter two method appli learn problem loss discret m herbster m k warmuth loss ie count mistak contrast method work gener class continu loss function staticexpert algorithm handl vovk haussler et al class includ common loss function squar loss rel entropi loss helling loss class tight bound addit loss haussler et al algorithm loss best expert ie nonshift case fix share algorithm obtain addit loss ock log nk log k k essenti sketch algorithm use staticexpert algorithm exponenti mani partitionexpert salient featur fixedshar algorithm still use o time per expert per trial howev algorithm addit loss still depend length sequenc lower bound give partial evid seem unavoid loss function loss singl trial unbound such rel entropi loss case loss particular trial one such squar loss develop second algorithm call variabl share algorithm algorithm obtain bound addit loss independ length sequenc also share weight loss updat howev amount expert share commensur loss expert current trial particular expert loss share weight version share updat trivial implement cost constant amount time n weight although algorithm easi describ prove addit loss bound take care believ techniqu constitut practic method track predict best expert provabl worstcas addit loss bound essenti ingredi success nonstationari set seem algorithm stationari set multipl weight updat whose loss bound grow logarithm dimens problem besid vovk aggreg algorithm vovk weight major algorithm littleston warmuth use loss updat basi work number algorithm develop exampl algorithm learn linear threshold function littleston littleston algorithm whose addit loss bound loss best linear combin expert sigmoid linear combin expert bound kivinen warmuth helmbold kivinen warmuth signific progress recent achiev nonstationari set build techniqu develop paper see discuss conclus section paper outlin follow preliminari section present algorithm section give basic proof techniqu section section contain detail proof fixedshar variabl share algorithm respect absolut loss treat special case section section discuss subtl power gener variabl share algorithm call proximityvariableshar algorithm gener lead improv bound case best expert next segment alway like close previou expert preliminari lower bound given section simul result artifici data exemplifi method given section final section conclud discuss recent work casual reader might interest detail proof recommend read section contain preliminari section algorithm section simul section preliminari let denot number trial n denot number expert label conveni simpli refer expert index thu expert i refer expert e predict n expert trial refer predict tupl x predict expert trial denot x ti expert may view oracl extern algorithm thu may repres predict neural net decis tree physic sensor perhap even human expert outcom trial predict algorithm trial instanceoutcom pair call tth exampl paper outcom expert predict predict algorithm throughout paper alway denot arbitrari sequenc exampl ie sequenc element n theta length loss function lp q function consid four loss function paper squar rel entropi helling absolut loss ent p hel p trial loss algorithm ly similarli loss expert trial ly x ti call subsequ contigu trial segment notat nonneg integ denot segment start trial number end trial round paren use end trial includ segment current sequenc abbrevi loss expert segment tt ltt loss algorithm whole trial sequenc defin ls readi give main definit paper use scenario best expert chang time inform kpartit slice sequenc k segment expert associ segment formal kpartit denot p nkte s consist three posit integ n k two tupl e posit integ number length trial sequenc s n size expert pool k number target shift tupl k element refer one trial convent use tupl divid trial sequenc m herbster m k warmuth paramet initi initi weight w ti predict loss updat receiv tth outcom share updat three algorithm staticexpert ti no share updat fixedshar variableshar figur staticexpert fixedshar variableshar algorithm call ith segment th segment also refer initi segment tupl e k element element e denot expert associ ith segment t t i loss given kpartit loss function l trial sequenc algorithm four algorithm consid paper staticexpert fix share variableshar proximityvariableshar first three summar figur proximityvariableshar algorithm gener variableshar algorithm algorithm given figur discuss gener defer section algorithm learn process proce trial denot trial number algo rithm maintain one posit weight per expert weight w ti or normal version v thought measur algorithm belief qualiti ith expert predict start trial t weight expert initi n algorithm follow three paramet j c ff paramet j learn rate quantifi drastic first updat be paramet c set j loss function the absolut loss except treat separ section paramet ff quantifi rate shift expect occur fixedshar algorithm design potenti unbound loss function rel entropi loss variableshar algorithm assum loss per trial lie fixedshar al gorithm ff rate shift per trial thu five shift expect trial sequenc variableshar algorithm ff approxim rate shift per unit loss best partit is five shift expect occur partit total loss ff tune paramet j c consid greater depth section ff section final staticexpert algorithm use paramet ff sinc assum shift occur trial algorithm receiv instanc summar predict n expert x algorithm plug current instanc x normal weight v predict function predv x order produc predict simplest case algorithm predict weight mean expert predict ie predv sophist predict function introduc vovk vovk discuss section predict algorithm perform two updat step first updat loss updat second share updat loss updat weight expert multipli e gammajl l loss ith expert current trial thu updat occur l learn rate j intensifi effect updat use w ti denot weight middl two updat weight refer intermedi weight share updat staticexpert algorithm vacuou howev algorithm share updat crucial briefli argu necess share updat nonstationari set give intuit descript function move predict well best expert predict well sequenc expert loss updat longer appropri sole updat assum two expert two segment first segment expert small loss expert larg loss role revers second segment end first segment loss updat caus weight expert almost zero howev second segment predict expert import weight need recov quickli share updat make sure possibl simul section further intuit share updat need two share updat summar m herbster m k warmuth below straightforward implement cost on time per expert per trial fixedshar w ff variableshar w contrast implement figur use intermedi variabl pool cost o time per expert per trial loss updat everi expert share fraction weight equal everi expert receiv weight enabl expert recov weight quickli rel expert fixedshar updat expert share fraction ff weight trial one expert perfect long segment type share optim sinc perfect expert keep share weight possibl nonperfect expert variableshar updat sophist roughli expert share weight loss larg perfect expert share expert high loss eventu collect weight howev perfect expert start incur high loss rapidli begin share weight expert allow good expert previous small rel weight recov quickli discuss paramet ff shift rate introduct discuss algorithm use exponenti mani static expert one partit goal achiev bound close ineffici algorithm use n weight bound obtain share algorithm slightli weaker partitionexpert algorithm grace degrad neither length sequenc number shift k known advanc predict function proof techniqu consid two choic predict function simplest predict weight mean warmuth pred wmean v sophist predict function give slightli better bound introduc vovk vovk haussler et al defin l z function must monoton let l gamma z denot invers l z l z vovk predict defin two step pred vovk v loss c valu j c function pred wmean v x pred vovk v x ent p hel p q figur c crealiz c valu loss predict function pair follow definit technic condit relat predict function predv x loss function l constant c j et al vovk loss function l predict function pred c jrealiz constant c j total weight consid four loss function paper squar rel entropi helling absolut loss see section howev algorithm limit loss function techniqu vovk haussler et al warmuth determin constant c j wide class loss function algorithm also easi adapt classif use major vote littleston warmuth predict function count mistak loss practic applic worstcas loss bound may provabl given loss function howev share updat may still use interest applic predict disk idl time see work helmbold et al helmbold long sherrod squar rel entropi helling loss c jrealiz pred wmean pred vovk j c valu c and henc two predict function summar figur sinc absolut loss complex bound treat section own smaller valu c lead smaller loss bound see lemma c valu pred vovk cf column two figur optim larg class loss function haussler et al proof loss bound algorithm base follow lemma lemma embodi key featur algorithm predict done loss incur algorithm temper correspond chang total weight lemma give inequ lemma use vovk haussler et al proof essenti same sinc share updat chang total weight w ti haussler et al sequenc exampl expert i total loss master algorithm figur may m herbster m k warmuth bound loss function l predict function pred c jrealiz cf definit figur proof sinc l pred c jrealiz definit sinc share updat chang total weight ti w t impli henc sinc w far use basic techniqu littleston warmuth vovk cesabianchi et al haussler et al ie c ln w becom potenti function amort analysi static expert case when c final weight form w n thu lemma lead bound relat loss algorithm loss static expert share updat make much difficult lower bound final weight intuit suffici share weight recov quickli howev much share final weight low follow section bound final weight individu expert term loss partit loss partit lp nkte s sum sequenc loss defin sequenc expert partit expert accumul loss segment bound weight use lemma fixedshar algorithm lemma variabl share algorithm sinc partit compos distinct segment must also quantifi weight transfer expert associ segment expert associ follow segment done lemma fixedshar algorithm lemma variableshar algorithm lower bound weight combin lemma bound total loss fixedshar algorithm theorem variableshar algorithm theorem fixedshar analysi algorithm work unbound loss function total addit loss grow length sequenc lemma sequenc exampl intermedi weight expert trial least e gammajltt i time weight expert start trial t formal proof combin loss fixedshar updat equat rewritten drop addit term produc share updat appli iter trial tt sinc bound w weight trial loss updat weight trial reduc factor e gammajli x i therefor rt simpl algebra definit lab i bound lemma follow lemma sequenc exampl s weight expert start trial least ff time intermedi weight expert j trial t proof expand fixedshar updat thu w ff done bound addit loss m herbster m k warmuth theorem let sequenc exampl let l pred c j realiz kshift sequenc partit p nkte s total loss fixedshar algorithm paramet ff satisfi proof recal e k expert last segment lemma bound w ek note follow weight arbitrari partit express follow telescop product thu appli lemma final term w equal one sinc appli share updat final trial therefor definit lp nkte s e gammajlp nkte substitut bound w ek simplifi obtain bound theorem hold k tradeoff term ck ln n cjlp nkte s ie k small ck ln n term small cjlp nkte s term larg viceversa optim choic ff obtain differenti bound theorem ff follow corollari rewrit bound theorem term optim paramet choic ff corollari give interpret theorem bound term code length introduc follow notat let gammap binari entropi measur nat gammaq binari rel entropi nat corollari let sequenc exampl let l pred c j realiz kshift sequenc partit p nkte s total loss fixedshar algorithm paramet ff satisfi ck ff bound becom interpret bound ignor constant c j differ nat bit term ln n k lnn gamma account encod expert partit log n bit initi expert logn gamma bit expert thereaft final need encod k shift occur the inner boundari partit ff interpret probabl shift occur gamma trial term gamma hff dff kff correspond expect optim code length see chapter cover thoma code shift estim ff instead true probabl ff bound thu exampl close similar predict code brought mani paper eg feder merhav gutman note ff minim bound theorem depend k unknown learner practic good choic ff may determin experiment howev upper bound lower bound k may tune ff term bound corollari let sequenc exampl k posit integ set loss fix share algorithm bound p nkte s partit k k proof recal loss bound given theorem set separ term appli inequ last inequ follow condit obtain bound corollari replac equat upper bound k m herbster m k warmuth variableshar analysi variableshar algorithm assum loss expert per trial lie henc variableshar algorithm work combin squar helling absolut loss function rel entropi loss function variableshar algorithm upper bound addit loss algorithm independ length trial sequenc abbrevi w ti w ti sinc section need refer weight expert middl trial first give two technic lemma follow convex r fi r c appli first inequ lemma rh c db b gammac thu lemma begin trial may lower bound weight expert either express a express b j expert differ i ae w ti e gammajli x ti proof expand loss updat variableshar updat trial cf express a obtain drop summat term express b drop one summand second term w appli lemma obtain b lemma weight expert start trial start trial reduc factor e gammaj theta proof lemma a trial weight expert reduc follow w ti appli iter rt theta lemma b lower bound weight transfer expert p expert q singl trial next lemma show weight transfer sequenc trial lemma distinct expert p q ltt trial may lower bound weight expert q theta e gammaj proof expert p accumul loss trial tt transfer part weight specif expert q via variableshar updat let denot weight transfer expert p expert q trial it denot total weight transfer expert p expert q trial tt transfer weight howev still reduc function loss expert q success trial lemma weight ad trial reduc factor e gammaj theta lower bound factor e gammaj e gammaj thu theta e gammaj complet proof lemma still need lower bound total transfer weight w tp ff l loss expert p trial i ie assumpt direct applic lemma b weight transfer expert p expert q first trial segment least w tp ff l e gammajl likewis appli lemma trial ti expert p appli lemma b trial i give us lower bound transfer weight total transfer weight a ff ff l m herbster m k warmuth split last sum two term ff l ff upper bound expon gamma ff one also replac sum first expon upper bound substitut lead applic lemma thu rewrit inequ ff theta ff appli lemma give us ff proof loss bound variableshar algorithm proce analog proof fixedshar algorithm loss bound case follow weight sequenc expert along sequenc segment within segment bound weight reduct expert lemma fixedshar analysi lemma variableshar analysi pass one segment next bound weight expert correspond new segment weight expert former segment lemma respect former lemma use fix share algorithm simpl sinc trial expert alway share fix fraction weight howev sinc weight share everi trial produc bound depend sequenc length variableshar algorithm produc bound independ length accomplish expert share weight accord loss howev expert accumul signific loss cannot use lemma bound weight follow expert term previou expert nevertheless former expert make signific loss current segment impli may bound current segment former expert collaps segment togeth word collaps two consecut segment t creat singl segment t associ expert first segment origin two consecut segment segment thu determin bound term relat collaps partit whose loss much wors lemma partit p nkte s exist collaps partit p nk segment except initi segment expert associ prior segment incur least one unit loss loss whole sequenc collaps partit exce loss origin partit follow properti hold proof recal e expert associ ith segment compris trial t t i segment i loss expert e associ prior segment i gamma less one merg segment segment i combin segment new partit associ expert e igamma formal iter decrement k one delet e tupl e t continu hold bound loss collaps partit p nk note loss new expert subsum segment one thu per applic transform loss increas one thu sinc applic done theorem let sequenc exampl let l pred c jrealiz let l rang partit p nkte s total loss variableshar algorithm paramet ff satisfi proof lemma let p nkte s arbitrari partit proof need properti loss segment except initi segment regard expert associ prior segment least one cf properti hold use lemma replac p nkte s collaps partit p nk properti hold properti hold alreadi p nkte s notat conveni refer p nkte s p nk recal loss exce loss p nkte s sinc hold exist trial q ith segment for lt express w e telescop product appli lemma ii ff m herbster m k warmuth simplifi follow bound ff last inequ follow thu substitut bound simplifi obtain bound theorem cannot optim upper bound function ff sinc k lp nkte s known learn algorithm tune ff base upper bound lp nkte s approach use corollari corollari let sequenc exampl l k posit real set l loss variableshar algorithm bound follow ck p nkte s partit lp nkte s l addit l partit p nkte s lp nkte s l obtain upper bound ck proof proceed upper bound three term contain ff bound theorem we use rewrit as appli ident ln x bound lp nkte s l give follow upper bound previou express l l therefor upper bound use express upper bound equat obtain equat l upper bound equat first term bound k second term k k ln region thu upper bound use express upper bound equat give us equat done absolut loss analysi absolut loss function l ab p jrealiz predict function pred vovk pred wmean howev cj thu tune complex sake simplic use weight mean predict littleston warmuth section theorem littleston warmuth absolut loss function l ab p jrealiz predict function pred wmean v x obtain slightli tighter bound could also use vee algorithm absolut loss jrealiz haussler et al algorithm take on log n time produc predict weight mean vee predict allow outcom lie binari outcom absolut loss on time predict function exist realiz criterion vee predict vovk cesabianchi et al unlik c crealiz loss function discuss earlier cf figur absolut valu loss constant paramet thu must tune practic tune j may produc numer minim upper bound howev use tune j produc freund schapir theorem lemma p q q m herbster m k warmuth use tune bound variableshar algorithm theorem theorem let loss function absolut loss let sequenc exampl l k posit real k k lp nkte s l k l set two paramet variableshar algorithm ff j k respect k k loss algorithm weight mean predict bound follow altern let l k posit real k k lp nkte s l k l set two paramet variableshar algorithm ff j k respect k k loss algorithm weight mean predict bound follow proximityvariableshar analysi section discuss proximityvariableshar algorithm see figur recal variableshar algorithm expert share fraction weight depend loss trial fraction share uniformli among remain expert proximityvariableshar algorithm enabl expert share nonuniformli expert proximityvariableshar updat cost on per expert per trial instead o see figur algorithm allow us model situat prior knowledg like pair consecut expert let us consid paramet algorithm ntupl contain initi weight algorithm ie w paramet initi initi weight w n ti predict loss updat receiv tth outcom proximityvariableshar updat figur proximityvariableshar algorithm second addit paramet besid j c complet direct graph size n without loop edg weight jk fraction weight share expert j expert k natur vertex outgo edg must nonneg sum one probabl distribut prior initi expert probabl distribut prior expert follow expert j upper bound proximityvariableshar algorithm fix share algorithm could gener similarli take proxim account theorem let sequenc exampl let l pred c jrealiz let l rang partit p nkte s total loss proximityvariableshar algorithm paramet ff satisfi proof omit proof bound sinc similar correspond proof theorem variableshar algorithm chang fraction replac correspond paramet note set give previou bound variableshar algorithm theorem case last sum ok ln n account code length name best expert except first one use proximityvariableshar algorithm get last sum ok case m herbster m k warmuth simpl exampl assum processor circular list two processor distanc processor i iid mod d next best expert alway constant away previou one last sum becom ok cours notion close choic paramet might suitabl note price decreas last sum updat time on per trial howev expert arrow end label valu share updat proximityvariableshar algorithm still on lower bound upper bound fixedshar algorithm grow length sequenc addit loss algorithm loss best kpartit approxim hold unbound loss function rel entropi loss restrict loss lie variableshar algorithm give addit loss bound approxim loss best kpartit k l one natur question whether similar reduct possibl unbound loss function word whether unbound loss function bound form possibl replac minf lg give evid contrari give adversari argument forc algorithm make loss best onepartit for adversari set section limit give construct easili extend adversari forc lnnln gamma log n addit loss best onepartit n expert iter adversari may forc addit loss best kpartit here assum log n gamma posit integ theorem rel entropi loss exist exampl sequenc length two expert lp te partit singl shift loss furthermor algorithm a proof adversari strategi describ figur use denot predict arbitrari learn algorithm l loss trial t conveni number trial two expert one alway predict alway predict adversari return sequenc outcom follow sequenc outcom neither sequenc empti thu singl shift best partit partit loss trial otherwis assum without loss gener thu new trial gammat els go step go step let remain trial exit figur adversari strategi prove ls thu prove lemma clearli loss gener assum note threshold gammat furthermor l ent l ent t thu condit i i follow condit ii hold simpl induct shift occur condit ii hold sinc condit ii gammat therefor add l least ln gamma t condit i obtain condit ii done step never execut shift occur last trial step skip thu step never execut trial condit ii bound lemma first reason lower bound tight show upper bound algorithm discuss paper close lower bound number partit thu may expand set expert partitionexpert discuss introduct use staticexpert algorithm weight mean predict give upper bound total loss algorithm loss best partit zero match lower bound second bound fixedshar algorithm cf corollari larger lower bound gamma addit term may upper bound m herbster m k warmuth total loss algorithm trial loss variabl share algorithm loss static algorithm vovk loss fix share algorithm loss typic expert loss best partit k variabl share loss bound fix share loss bound figur loss variableshar algorithm vs staticexpert algorithm scale weight figur rel weight variableshar algorithm simul result section discuss simul artifici data simul mainli meant provid visual algorithm track predict best expert seen empir evid practic use algorithm believ merit algorithm clearli reflect strong upper bound prove theorem scale weight trial vovk rel weight figur rel weight staticexpert algorithm earlier section simul show loss algorithm typic sequenc exampl bound paper worstcas bound hold even adversariallygener sequenc exampl surprisingli loss algorithm simul random sequenc close correspond worstcas bound proven paper thu simul show loss bound tight sequenc compar perform staticexpert algorithm two share algorithm follow set chose use squar loss loss function widespread use task tune learn rate loss function simpl use vovk predict function cf equat chose accord figur consid sequenc trial four distinct segment begin trial trial outcom y predict tupl contain predict expert gener predict expert chose differ expert best one segment best expert alway expect loss per trial expert expect loss per trial end segment new best expert chosen sinc outcom alway gener expect loss sampl predict uniform random distribut typic best expert respect thu expect loss best partit denot segment boundari abov varianc oe actual loss best partit particular simul use plot fixedshar algorithm tune f base valu use ff f m herbster m k warmuth tune suggest corollari variableshar algorithm tune ff v base valu use ff v tune suggest corollari use theorem calcul worst case upper bound loss fixedshar algorithm variableshar algorithm respect see theta mark figur simul artifici data show worstcas bound rather tight even simpl artifici data mani heurist find suitabl tune use tune prescrib theorem notic type simul result rel insensit tune ff exampl calcul ff v variableshar algorithm overestim standard deviat loss bound algorithm increas actual loss algorithm simul increas figur plot loss staticexpert algorithm versu loss two share algorithm examin figur show first segment staticexpert algorithm perform compar share algo rithm howev remain three segment staticexpert algorithm perform poorli loss essenti bad loss typic expert the slope total loss typic expert staticexpert algorithm essenti later segment share algorithm perform poorli begin new segment howev quickli learn new best expert current segment share algorithm loss plateau almost slope slope total loss best expert two share algorithm qualit behavior even though fixedshar algorithm incur approxim addit loss variableshar algorithm simul tri learn rate j slightli smaller two verifi even choic learn rate total loss staticexpert algorithm improv significantli figur plot weight normal weight vector w maintain variableshar algorithm staticexpert algorithm trial sequenc figur see variableshar algorithm shift rel weight rapidli latter part segment rel weight best expert almost one the correspond plot fixedshar algorithm similar hand see figur staticexpert algorithm also learn best expert segment howev staticexpert algorithm unabl shift rel weight suffici quickli ie take length second segment partial unlearn best expert first segment rel weight best expert segment one two essenti perform random walk third segment final segment rel weight best expert segment three also perform random walk summari see simul evid fixedshar variableshar updat necessari track shift expert conclus paper essenti gave reduct multipl updat algorithm work well compar best expert arbitrari segment exampl algorithm work well compar best partit ie concaten segment two type share updat analyz fix share algorithm work well loss function unbound variableshar algorithm suitabl case rang loss lie first method essenti one use wml algorithm littleston warmuth recent altern develop auer warmuth learn shift disjunct loss discret loss as classif problem method simpl effect algorithm updat mistak occur ie conserv updat second method variableshar updat sophist particular one expert predict perfectli while collect weight howev expert start incur larg loss share weight expert help next best expert recov weight zero method present littleston warmuth inspir number recent paper auer warmuth adapt winnow algorithm learn shift disjunct compar best shift disjunct complic compar best expert howev sinc classif problem simpl share updat similar fixedshar updat suffici focu paper track predict best expert class loss function origin staticexpert algorithm vovk develop vovk haussler et al share updat appli experiment predict disk idl time helmbold et al onlin manag invest portfolio singer addit reduct shown expert metric task system algorithm blum burch share updat use success new domain metric task system natur probabilist interpret share algorithm recent given vovk particular applic share algorithm necessari consid choos paramet ff theoret techniqu exist fixedshar algorithm elimin need choos valu ff ahead time one method tune paramet among thing specialist framework freund schapir singer warmuth even though bound produc way alway optim anoth method incorpor prior distribut possibl valu ff sake simplic discuss method herbster vovk singer paper m herbster m k warmuth acknowledg would like thank peter auer phillip long robert schapir volodya vovk valuabl discuss also thank anonym refere help comment note discret loss defin ae note lent p q use dpkq notat customari inform theori replac assumpt k k k obtain bound final term c k replac c k vovk recent prove sharper bound algorithm vovk ff unlik corollari need lower bound k call partit describ segment boundari best partit respect tradeoff k lp nkte s express implicitli theorem r track best disjunct use expert advic element inform theori univers predict individu sequenc ieee transact inform theori decisiontheoret gener onlin learn applic boost use combin predictor special sequenti predict individu sequenc gener loss function dynam disk spindown techniqu mobil comput track best expert ii addit versu exponenti gradient updat linear predict learn irrelev attribut abound new linearthreshold algorithm mistak bound logarithm linearthreshold learn algorithm phd thesi weight major algorithm toward realist competit portfolio select algorithm game predict expert advic derandom stochast predict strategi predict dotproduct expert framework tr ctr atsuyoshi nakamura learn specialist decis list proceed twelfth annual confer comput learn theori p juli santa cruz california unit state jeremi z kolter marcu a maloof use addit expert ensembl cope concept drift proceed nd intern confer machin learn p august bonn germani v vovk probabl theori brier game theoret comput scienc v n p peter auer manfr k warmuth track best disjunct machin learn v n p aug olivi bousquet manfr k warmuth track small set expert mix past posterior journal machin learn research avrim blum carl burch onlin learn metric task system problem machin learn v n p april chri mesterharm track linearthreshold concept winnow journal machin learn research peter auer use confid bound exploitationexplor tradeoff journal machin learn research giovanni cavallanti nicol cesabianchi claudio gentil track best hyperplan simpl budget perceptron machin learn v n p decemb marco barreno blain nelson russel sear anthoni d joseph j d tygar machin learn secur proceed acm symposium inform comput commun secur march taipei taiwan wei yan christoph d clack divers committe vote depend profit proceed th annual confer genet evolutionari comput juli london england mark herbster manfr k warmuth track best regressor proceed eleventh annual confer comput learn theori p juli madison wisconsin unit state claudio gentil robust pnorm algorithm machin learn v n p decemb mark herbster manfr k warmuth track best linear predictor journal machin learn research p amol deshpand zachari ive vijayshankar raman adapt queri process foundat trend databas v n p januari