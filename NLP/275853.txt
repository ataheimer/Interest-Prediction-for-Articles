t schedul blockcycl array redistribut a abstractthi articl devot runtim redistribut onedimension array distribut blockcycl fashion processor grid previou studi concentr effici gener commun messag exchang processor involv redistribut focu schedul messag organ messag exchang structur commun step minim content build upon result walker otto solv particular instanc problem deriv optim schedul gener case name move cyclicr distribut pprocessor grid cyclic distribut qprocessor grid arbitrari valu redistribut paramet p q r s b introduct runtim redistribut array distribut blockcycl fashion multidimension processor grid difficult problem recent receiv consider attent interest motiv larg hpf program style scientif applic decompos phase phase optim distribut data array onto processor grid typic array distribut accord cyclicr pattern along one sever dimens grid best valu distribut paramet r depend characterist algorithm kernel well communicationtocomput ratio target machin optim valu r chang phase phase one machin anoth think heterogen environ runtim redistribut turn critic oper state among other basic decompos redistribut problem follow two subproblem messag gener array redistribut effici scan process order build messag exchang processor commun schedul messag must effici schedul minim commun overhead given processor typic sever messag send processor subset these term mpi collect oper must schedul someth similar mpi alltoal commun except processor may send messag particular subset receiv the subset depend sender previou work concentr mainli first subproblem messag gener messag gener make possibl build differ messag pair processor must commun therebi guarante volumeminim commun phase each processor send receiv data need howev question effici schedul messag receiv littl attent one except interest paper walker otto schedul messag order chang array distribut cyclicr p processor linear grid cyclickr grid aim extend walker otto work order solv gener redistribut problem is move cyclicr distribut p processor grid cyclic distribut qprocessor grid gener instanc redistribut problem turn much complic particular case consid walker otto howev provid effici algorithm heurist optim schedul commun induc redistribut oper main result follow valu redistribut paramet p q r s construct optim schedul is schedul whose number commun step minim commun step defin processor sendsrec one messag therebi optim amount buffer minim content commun port construct optim schedul reli graphtheoret techniqu edg color number bipartit graph delay precis mathemat formul result section need sever definit beforehand without loss gener focu onedimension redistribut problem articl although usual deal multidimension array highperform comput problem reduc tensor product individu dimens hpf allow one loop variabl align direct therefor multidimension assign redistribut treat sever independ onedimension problem instanc rest articl organ follow section provid exampl redistribut oper expos difficulti schedul commun section briefli survey literatur redistribut problem particular emphasi given walker otto paper section present main result section report mpi experi demonstr use result final section state conclus futur work direct motiv exampl consid array xm gamma size distribut accord block cyclic distribut cyclicr onto linear grid p processor number goal redistribut x use cyclic distribut q processor number simplic assum size x multipl qs least common multipl p r qs redistribut pattern repeat slice l element therefor assum even number slice x enabl us without loss gener avoid discuss side effect let l number slice exampl consid first exampl note new grid q processor ident to disjoint of origin grid p processor actual total number processor use unknown valu commun summar tabl refer commun grid note view sourc target processor grid disjoint tabl even may actual case see sourc processor messag processor receiv messag too henc need use full alltoal commun scheme would requir step total messag sent per processor or precis messag local copi rather tri schedul commun effici ideal could think organ redistribut step commun phase step messag would exchang involv disjoint pair processor would perfect oneport commun machin processor send andor receiv one messag time note may ask someth more tri organ step way step involv pair processor exchang messag length approach interest cost step like dictat length longest messag exchang step note messag length may may vari significantli number tabl vari singl slice vector vector x length length vari time number byte need repres one datatyp element schedul meet requir name step disjoint processor pair exchang messag length provid section report solut schedul tabl entri posit p q tabl denot step number g clariti processor p send messag processor q tabl comput cost commun step be proport to length longest messag involv step total cost redistribut sum cost step elabor model commun cost section tabl commun grid messag length indic vector x size commun grid msg nbr msg exampl second exampl show use effici schedul even processor commun everi processor illustr tabl messag length vari ratio need organ alltoal exchang step way messag length commun step again abl achiev goal see section solut schedul given tabl where step number p cost given tabl we check step compos messag length exampl third motiv exampl shown tabl commun scheme sever unbalanc processor may differ number messag send andor receiv techniqu abl handl complic situat provid section schedul compos step longer possibl messag length step for instanc processor messag length send processor messag length achiev redistribut commun step processor sendsrec one messag per step number commun step tabl clearli optim processor send cost schedul given tabl tabl commun step commun step b f g e c tabl commun cost commun cost step b c e f g total cost exampl final exampl p q show size two processor grid need same see tabl commun grid unbalanc solut schedul see section compos commun step number optim sinc processor messag receiv note total cost equal sum messag length processor must receiv henc optim literatur overview briefli survey literatur redistribut problem particular emphasi given work walker otto tabl commun grid indic vector x size msg nbr msg messag gener sever paper dealt problem effici code gener hpf array assign statement like array b distribut blockcycl fashion linear processor grid research see stichnoth et al van reeuwijk et al wakatani wolf dealt princip array distribut use either pure scatter cyclic distribut cyclic hpf full block distribut cyclicd n array size p number processor recent howev sever algorithm publish handl gener blockcycl distribut sophist techniqu involv finitest machin see chatterje et al settheoret method see gupta et al diophantin equat see kennedi et al hermit form lattic see thirumalai ramanujam linear program see ancourt et al compar survey algorithm found wang et al report power algorithm handl blockcycl distribut effici simpler case pure cyclic fullblock map end messag gener phase processor comput sever differ messag usual store temporari buffer messag must sent set receiv processor exampl section illustr symmetr processor comput number length messag receiv therefor alloc correspond memori space summar messag gener phase complet processor tabl commun step commun step g tabl commun cost commun cost step b c e f cost prepar messag processor must send data processor possess inform regard messag receiv number length origin commun schedul littl attent paid schedul commun induc redistribut oper simpl strategi advoc instanc kaln ni view commun total exchang processor specifi oper compar survey wang et al use follow templat execut array assign statement gener messag tabl post receiv advanc minim oper system overhead pack commun buffer carri barrier synchron tabl commun grid messag length indic vector x size nbr msg nbr msg send buffer wait messag arriv unpack buffer although commun phase describ precis note explicit schedul messag sent simultan use asynchron commun pro tocol approach induc tremend requir term buffer space deadlock may well happen redistribut larg array scalapack librari provid set routin perform array redistribut describ prylli tourancheau total exchang organ processor arrang virtual caterpillar total exchang implement success step step processor arrang pair perform sendrec oper caterpillar shift new exchang pair form again even though special care taken implement total exchang attempt made exploit fact processor pair may need commun first paper devot schedul commun induc redistribut walker otto review two main possibl implement commun induc redistribut oper wildcard nonblock receiv similar strategi wang et al describ abov asynchron strategi simpl implement requir buffer messag receiv henc total amount buffer high total volum data redistribut tabl commun step commun step tabl commun cost commun cost step b c e f g h j total cost synchron schedul synchron algorithm involv commun phase step step particip processor post receiv send data wait complet receiv sever factor lead perform degrad instanc processor may wait other receiv data hot spot aris sever processor attempt send messag processor step avoid drawback walker otto propos schedul messag that step processor send one messag receiv one messag strategi lead synchron algorithm effici asynchron version demonstr experi written mpi ibm sp intel paragon requir much less buffer space walker otto provid synchron schedul special instanc redistribut problem name chang array distribut cyclicr p processor linear grid cyclickr grid size main result provid schedul compos k step step processor send receiv exactli one messag k smaller p size grid dramat improv tradit alltoal implement tabl commun grid messag length indic vector x size msg nbr msg aim articl extend walker otto work order solv gener redistribut problem is move cyclicr distribut p processor grid cyclic distribut qprocessor grid retain origin idea schedul commun step step particip processor neither send receiv one messag avoid hot spot resourc content explain strategi well suit current parallel architectur section give precis framework model cost redistribut main result problem formul consid array xm gamma size distribut accord blockcycl distribut cyclicr onto linear grid p processor number goal redistribut x use cyclic distribut q processor number equival perform hpf assign cyclicr processor grid cyclic qprocessor grid blockcycl data distribut map global index vector x ie element xi onto processor index p block index l item index x local block with indic start map gamma p l x may written birc deriv relat gener assign a dealt similarli tabl commun step commun step tabl commun cost commun cost step b c total cost similarli sinc distribut cyclic qprocessor grid global index j map y get redistribut equat qs least common multipl p r qs element li x initi distribut onto processor becaus l multipl p r henc r divid l p divid l xi r similar reason two element redistribut onto processor word redistribut pattern repeat slice l element therefor restrict discuss vector x length l follow let rq bound equat becom s given distribut paramet r s grid paramet p q redistribut problem determin messag exchang is find valu p q redistribut equat solut unknown l m x y subject bound equat comput number solut given processor pair p q give length messag start simpl lemma lead handi simplif lemma assum r rel prime is proof redistribut equat express equat express solut given processor pair p q delta divid z z deltaz deduc solut redistribut problem r p q let us illustr simplif one motiv exampl back exampl note need scale messag length move redistribut oper r rel prime one not let us return exampl assum know build commun grid tabl deduc commun grid say keep messag scale length process make sens new size vector slice deltal rather l see tabl result commun grid cours schedul commun remain cost tabl multipli delta commun pattern consid redistribut paramet r s p q assum qs commun pattern induc redistribut oper complet alltoal oper proof rewrit equat ps gamma p rl gamma qsm arbitrari multipl g sinc z lie interv gamma whose length r guarante multipl g found within interv convers assum g r s exhibit processor pair p q exchang messag inde desir processor pair see thi note pr gamma becaus g divid p r henc multipl g ad pr gamma qs lie interv gamma therefor messag sent p q redistribut follow aim character pair processor need commun redistribut oper in case s consid follow function anoth proof see petitet tabl commun indic vector x size nbr msg nbr msg function f map processor pair p q onto congruenc class pr gamma qs modulo g accord proof lemma p send messag q fp modg let us illustr process use one motiv exampl back exampl exampl as proof lemma receiv messag p receiv messag see tabl check thi character class introduc integ u v r theta the extend euclid algorithm provid number rel prime r s follow result proposit assum r u mod g proof first see pq inde integ note sinc g divid p r qs divid pq two differ class disjoint by definit turn class number element see thi note k integ sinc g class deduc number element class pq next see p q s mod becaus final p p r qs divid divid rs deduc pq divid henc processor pair p distinct thu enumer class definit consid redistribut paramet r s p q assum let lengthp q length messag sent processor p processor q redistribut singl slice vector x size said earlier commun pattern repeat slice valu report commun grid tabl section singl slice is equal lengthp q interest repres homogen commun processor pair given class exchang messag length proposit assum qs length vector x redistribut let volk piecewis function given figur k gamma recal p q classk send messag q volk volr figur piecewis linear function vol proof simpli count number solut redistribut equat pr easili deriv piecewis linear vol function repres figur know build commun tabl section still deriv schedul is way organ commun effici possibl commun schedul commun model accord previou discuss concentr schedul compos sever success step step sender send one messag symmetr receiv receiv one messag give formal definit schedul follow definit consid redistribut paramet r s p q ffl commun grid p theta q tabl nonzero entri lengthp q posit p q p send messag q ffl commun step collect pair t lengthp t commun step complet sender receiv activ incomplet otherwis cost commun step maximum valu entri word maxflengthp ffl schedul success commun step nonzero entri commun grid appear one one step cost schedul may evalu two way number step ns simpli number commun step schedul total cost tc sum cost commun step as defin abov commun grid illustr tabl section summar length requir commun singl slice vector is vector size qs motiv evalu schedul via number step via total cost follow ffl number step ns number synchron requir implement sched ule roughli estim commun step involv processor a permut measur unit number step good evalu cost redistribut ffl may tri precis step sever messag differ length exchang durat step like relat longest length messag simpl model would state cost step ff ff startup time invers bandwidth physic commun link although express take hot spot link content account proven use varieti machin cost redistribut accord formula affin express ff theta ns motiv interest number step total cost simpl case simpl character processor pair class special case r q well p rel prime proposit assum respect denot invers r modulo g proof rel prime qs henc g therefor invers r modulo g well defin and comput use extend euclid algorithm appli r g similarli invers modulo g well defin too condit easili translat condit proposit simpl case nice solut schedul problem assum first simpli schedul commun class class class compos pq processor pair equal distribut row column commun grid class exactli q send processor per row p receiv processor per column direct consequ proposit note g divid p q hypothesi gcdr schedul class want processor g send messag processor or equival look receiv side word processor posit p within block g element must send messag processor posit q within block g element done maxpq complet step messag instanc five block sender three block receiv block sender send messag block receiv use algorithm gener block permut order commun block irrelev alltoal commun scheme illustr exampl schedul class lead algorithm messag length given step case simpli regroup class equival modulo g proceed befor summar discuss follow result proposit assum schedul class success lead optim commun scheme term number step total cost proof assum without loss gener p q accord previou discuss the number class time p the number step class commun step step schedul messag class k henc length volk time p commun step compos messag length name process given class k remark walker otto deal redistribut shown go r kr simplifi go techniqu describ section enabl us retriev result gener case gcd p entri commun grid may evenli distribut row sender similarli entri commun grid may evenli distribut column receiv back exampl see tabl row commun grid nonzero entri messag row similarli henc r column commun grid nonzero entri column first goal determin maximum number nonzero entri row column commun grid start analyz distribut class class classk k processor pair distribut follow ffl p entri per column q column grid none remain column ffl q entri per row p row grid none remain row proof first let us check sinc r rel prime q by definit r s becaus pq element per class sinc class obtain translat class restrict discuss distribut element class formula lemma state r mod s mod p take valu multipl r mod q take valu multipl r henc result check total number element note let us illustr lemma one motiv exampl back exampl element class locat p column processor grid let us check class instanc inde follow lemma show cannot use schedul base class consid class separ would lead incomplet commun step rather build commun step mix element sever class order use avail processor maximum number element row column commun grid obviou lower bound number step schedul processor cannot send or receiv one messag commun step proposit assum otherwis commun grid full use notat lemma maximum number mr element row commun grid maximum number mc element column commun grid e proof accord lemma two element classk classk row commun grid s interv pq necessarili divid p rel prime u fortiori rel prime u therefor divid share row processor grid congruent modulo induc partit class sinc exactli q element per row class sinc number class congruent valu modulo either b rsgamma c rsgamma e deduc valu mr valu mc obtain similarli turn lower bound number step given lemma inde achiev theorem assum otherwis commun grid full use notat lemma lemma optim number step ns opt schedul proof alreadi know number step ns schedul greater equal g give construct proof bound tight deriv schedul whose number step maxfmr mc g so borrow materi graph theori view commun grid graph set send processor set receiv processor entri p q commun grid nonzero g bipartit graph all edg link vertex p vertex q degre g defin maximum degre vertic g accord konig edg color theorem edg color number bipartit graph equal degre see vol p berg p mean edg bipartit graph partit g disjoint edg match construct proof follow repeatedli extract e maximum match satur maximum degre node iter exist maximum match guarante see berg p defin schedul simpli let match iter repres commun step remark proof theorem give bound complex determin optim number step best known maximum match algorithm bipartit graph due hopcroft karp cost ojv j maxp q iter construct schedul procedur ojp j construct schedul whose number step minim schedul implement goal twofold design schedul ffl minim number step schedul ffl minim total cost schedul alreadi explain view commun grid bipartit graph e accur view edgeweight bipartit graph edg edg p q length lengthp q messag sent processor p processor q adopt follow two strategi stepwis specifi number step choos iter maximum match satur node maximum degre sinc free select match natur idea select among match one maximum weight the weight match defin sum weight edg greedi specifi total cost adopt greedi heurist select maximum weight match step might end schedul ns opt step whose total cost less implement approach reli linear program framework see chapter let jv j theta jej incid matrix g ae edg j incid vertex sinc g bipartit total unimodular each squar submatrix determin gamma match polytop g set vector x q jej ae intuit select match polyhedron determin equat integr rewrit set vector x q jej find maximum weight match look x c n jej weight vector choos greedi strategi simpli repeat search maximum weight match commun done choos stepwis strategi ensur that iter vertic maximum degre satur task difficult vertex v maximum degre posit i replac constraint ax translat number maximum degre vertic f g jv j whose entri posit iff ith vertex maximum degre note either case polynomi method match polyhedron integr solv ration linear problem guarante find integ solut see fact greedi strategi better stepwis strategi term total cost consid follow exampl exampl consid redistribut problem commun grid given tabl stepwis strategi illustr tabl number step equal optim total cost see tabl greedi strategi requir step name see tabl total cost see tabl tabl commun grid messag length indic vector x size msg nbr msg comparison walker otto strategi walker otto deal redistribut know go r kr simplifi go appli result section see remark gener case s evenli distribut among column commun grid becaus r necessarili among row howev row total number nonzero element divid word bipartit graph regular sinc maximum match perfect match messag length lengthp p q commun grid consequ stepwis strategi lead optim schedul term number step total cost note ns opt k hypothes walker otto use notat lemma note result appli graph regular entri commun grid equal follow theorem extend walker otto main result tabl commun step stepwis strategi stepwis strategi tabl commun cost stepwis strategi stepwis strategi step b c e f g h j total cost proposit consid redistribut problem arbitrari p q s schedul gener stepwis strategi optim term number step total cost strategi present articl make possibl directli handl redistribut arbitrari cyclicr arbitrari cyclic contrast strategi advoc walker otto requir two redistribut one cyclicr cycliclcmr second one cycliclcmr cyclic mpi experi section present result run intel paragon redistribut algorithm describ section tabl commun step greedi strategi greedi strategi tabl commun cost greedi strategi greedi strategi step b c e f g h j k l total cost descript experi execut intel paragon xp comput c program call routin mpi librari mpi chosen portabl reusabl reason schedul compos step step gener one send andor one receiv per processor henc use onetoon commun primit mpi main object comparison new schedul strategi current redistribut algorithm scalapack name caterpillar algorithm briefli summar section run schedul algorithm proceed follow comput schedul step use result section pack commun buffer carri barrier synchron start timer execut commun use redistribut algorithm resp caterpillar algorithm stop timer unpack buffer maximum timer taken processor emphas take cost messag gener account compar commun cost onli instead caterpillar algorithm could use mpi alltoallv commun primit turn caterpillar algorithm lead better perform mpi alltoallv experi the differ roughli short vector long vector use physic processor input output processor grid result sensit grid disjoint grid sender receiv result three experi present below first two experi use schedul present section optim term number step ns total cost tc third experi use schedul present section optim term ns back exampl first experi correspond exampl redistribut schedul requir step see tabl sinc messag length theoret improv caterpillar algorithm step figur show signific differ two execut time theoret ratio obtain small vector eg size doubleprecis real result surpris startup time domin cost small vector larger vector ratio vari due content problem schedul need step step gener commun wherea step caterpillar algorithm gener fewer commun between per step therebi gener less content back exampl second experi correspond exampl redistribut schedul requir step total cost caterpillar algorithm requir step too step least one processor send messag length proport to henc total cost theoret gain expect long vector becaus startup time obtain anyth better content experi ibm sp network workstat would like lead favor ratio back exampl third experi correspond exampl experi similar first one redistribut schedul requir much fewer step caterpillar two differ howev p q algorithm guarante optim term total cost instead obtain theoret ratio obtain result close explain thi need take closer look caterpillar algorithm shown tabl step caterpillar algorithm inde empti step theoret ratio rather global size redistribut vector bit doubl precis microsecond caterpillar optim schedul figur compar redistribut time intel paragon tabl commun cost caterpillar schedul caterpillar step b c e f g h j k l total cost conclus articl extend walker otto work order solv gener redistribut problem is move cyclicr distribut p processor grid cyclic distribut qprocessor grid valu redistribut paramet p q r s construct schedul whose number step optim schedul shown optim term total cost particular instanc redistribut problem that includ walker otto work futur work devot find schedul optim term number step total cost arbitrari valu redistribut problem sinc problem seem difficult it may prove npcomplet anoth perspect explor use heurist like greedi algorithm introduc assess perform run experi gener optimist result one next releas scalapack librari may well includ redistribut algorithm present articl global size redistribut vector bit doubl precisionmicrosecond caterpillar optim schedul figur time measur caterpillar greedi schedul differ vector size redistribut r linear algebra framework static hpf code distribut graph et hypergraph gener local address commun set dataparallel program portabl linear algebra librari distribut memori comput design issu perform softwar librari linear algebra comput high perform comput matrix comput handbook combinator compil array express effici execut distributedmemori machin processor map techniqu toward effici data redistribut effici address gener blockcycl distribut lineartim algorithm comput memori access sequenc dataparallel program steel jr algorithm redistribut method block cyclic decomposit effici blockcycl data redistribut mpi complet refer gener commun array state ment design fast address sequenc gener dataparallel program use integ lattic implement framework hpf distribut array messagepass parallel comput system redistribut blockcycl data distribut use mpi redistribut blockcycl data distribut use mpi runtim perform parallel array assign empir studi tr ctr prashanth b bhat viktor k prasanna c s raghavendra blockcycl redistribut heterogen network cluster comput v n p stavro souravla mano roumelioti pipelin techniqu dynam data transfer multiprocessor grid intern journal parallel program v n p octob chinghsien hsu shihchang chen chaoyang lan schedul contentionfre irregular redistribut parallel compil journal supercomput v n p june hyungyoo yook myongsoon park schedul gen_block array redistribut journal supercomput v n p juli chinghsien hsu spars matrix blockcycl realign distribut memori machin journal supercomput v n p septemb minyi guo yi pan improv commun schedul array redistribut journal parallel distribut comput v n p may minyi guo ikuo nakata framework effici data redistribut distribut memori multicomput journal supercomput v n p novemb neungsoo park viktor k prasanna cauligi s raghavendra effici algorithm blockcycl array redistribut processor set ieee transact parallel distribut system v n p decemb chinghsien hsu yehch chung donlin yang chyiren dow gener processor map techniqu array redistribut ieee transact parallel distribut system v n p juli chinghsien hsu yehch chung chyiren dow effici method multidimension array redistribut journal supercomput v n p aug saeri lee hyungyoo yook misoo koo myongsoon park processor reorder algorithm toward effici gen_block redistribut proceed acm symposium appli comput p march la vega nevada unit state chinghsien hsu kunm yu compress diagon remap techniqu dynam data redistribut band spars matrix journal supercomput v n p august emmanuel jeannot frdric wagner schedul messag data redistribut experiment studi intern journal high perform comput applic v n p novemb peizong lee wenyao chen gener commun set array assign statement blockcycl distribut distribut memori parallel comput parallel comput v n p septemb antoin p petitet jack j dongarra algorithm redistribut method blockcycl decomposit ieee transact parallel distribut system v n p decemb jihwoei huang chihp chu effici commun schedul method processor map techniqu appli data redistribut journal supercomput v n p septemb