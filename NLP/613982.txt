t assess perform new ibm sp commun subsystem a ibm recent launch upgrad commun subsystem sp parallel comput chang affect hardwar softwar element highperform switch messag interfac adapt new implement mpi messagepass librari character extent chang affect execut time parallel applic author run collect benchmark sp old commun subsystem machin upgrad benchmark includ pointtopoint collect commun test well set complet parallel applic perform indic latenc throughput exhibit basic commun test execut time case real applic result indic certain circumst signific perform increas result b introduct long time pass sinc highperform comput commun realiz highest comput speed reason cost reach via massiv parallel eighti earli nineti sort build euphoria led work done r beivid ja gregorio depart electr comput engin univers california irvin visit research design use massiv parallel processor mpp thousand even ten thousand process element howev nowaday commun seem think hundr processor repres upper limit feasibl parallel comput system addit mainli due lack stabil supercomput sector constructor make rel conserv decis regard hardwar softwar element mpp way guarante system surviv market reason period ibm sp parallel comput built around workstationbas hardwar softwar repres one success approach mpp seem capabl surviv difficult time hp current mpp includ sp shown suitabl run coars grain parallel applic interchang larg data structur mani case system use simpli increas throughput sequenti job multipl user share machin howev big challeng sp similar machin distribut memori parallel comput effici run communicationdemand medium fine grain parallel applic reduc execut time nowaday follow principl allow product portabl softwar mpp program use convent imper languag enhanc commun librari pvm mpi implement messag pass synchron among process gei mpi environ effici parallel applic maxim workload evenli distribut among processor overhead introduc parallel process minim cost commun synchron oper must kept low possibl order achiev thi interconnect subsystem use support interchang messag must fast enough avoid becom bottleneck tradit latenc throughput two paramet use indic perform interconnect network mpp throughput increas one gener mpp next signific reduct latenc seem tougher problem design industri unfortun perform parallel applic sensit latenc and increas throughput help process long messag help significantli interchang reasonablys messag ten thousand bytesi sever order magnitud smaller requir reach maximum achiev throughput current mpp current attempt measur character differ compon latenc modern mpp lead conclus interconnect network router wire overdimens compar node abil send receiv messag word largest compon latenc network network interfac commun softwar caus messag pass overhead two element io hardwar softwar must greatli improv reduc messag latenc mpp and thu achiev better perform run parallel applic them import issu highlight sever research develop project includ cul myr recent ibm develop new version highperform switch constitut interconnect network sp upgrad affect switch itself also io adapt connect processor switch that ibm upgrad oper system aix aix latter includ new version mpi special tailor sp aix mpi avail addit layer softwar mpl nativ ibm messag pass librari chang aim increas parallel applic perform paper report experiment data obtain run collect benchmark applic sp first old new commun subsystem inform use make perform comparison two sp comput whose differ commun subsystem tri report exhaust evalu new subsystem provid exact character latenc componentsthat done futur paper howev import allow commun know preliminari experiment data especi search machin run parallel applic design new mpp or redesign exist one paper structur follow next section brief introduct ibm sp parallel comput made includ limit inform avail recent chang commun subsystem describ collect program use test machin section result run program analyz section paper end conclus section acknowledg section reader find refer access path parallel code use elabor paper overview ibm sp sp distribut memori parallel comput processor node interconnect commun subsystem ibm offer sever altern subsystem possibl provid commun among processor use standard network technolog ethernet fddi atm howev highend sp system ibm offer highperform switch provid better characterist parallel comput figur show highlevel system structur sp age micro channel control switch adapt system io bu memori processor highperform switch adapt detail node node figur sp system ibm also offer sever altern node access sp c see acknowledg section compos node power mhz processor attach mb memori mean bit wide bu node kb instruct cach kb data cach mb l cach micro channel control govern io bu connect processormemori subset devic disk ethernet network highperform switch mean appropri adapt sp commun subsystem compos highperform switch plu adapt connect node switch element need care design allow cooper without introduc bottleneck adapt contain onboard microprocessor offload work pass messag node node memori provid buffer space dma engin use move inform node adapt memori switch link highperform switch describ ibm an anytoani packetswitch multistag indirect network similar omega network advantag network bisect bandwidth increas linearli size system in contrast direct network ring mesh tori guarante system scalabl core network crossbar chip offer bidirect port use build small sp system larger system board compos two stage chip for total bidirect port use system alway least one stage necessari order provid redund path at least pair node reader find age stu sketch configur system node frame build block sp system frame contain switch board node sp avail c two frame node each one frame use run sequenti batch job second one avail run parallel program one use experi report paper origin system commun subsystem follow characterist adapt built around intel processor mb ram theoret peak transfer abil adapt mb although achiev maximum mb due overhead associ access manag micro channel link highperform switch provid mb peak bandwidth direct mb bidirect nodetonod latenc s system node recent system upgrad inform new system avail write report limit abl confirm new adapt built around powerpc processor allow doubl peak transfer bandwidthreach mb new switch offer mb peak bidirect bandwidth latenc less s system node gar regard softwar environ sp node run full version aix ibm version unix includ unix featur plu specif tool librari program execut parallel program first test cs sp carri version aix includ mpl ibmdesign librari parallel program use messagepass paradigm sni mpl actual interfac implement sever commun subsystem particular run softwar layer design make best use highperform switch altern mpl might also run ip and thu almost commun media includ switch environ mpi librari mpich see bri also avail addit softwar layer mpl introduct aix version ibm decid adopt mpi nativ languag program sp system replac nonstandard mpl way applic program mpi could run less overhead compar previou version introduct new switch accompani new version mpi implement although aix remain without signific chang order take advantag characterist new hardwar mpl still avail eas migrat ensur usabl old code mpi mpl use fortran c program paper report result obtain run experi bechmark describ section follow three configur cs sp tabl commun subsystem softwar old switch adapt aix v mpich implement mpi on top mpl old switch adapt aix v nativ version mpi nv new switch adapt aix v nativ version mpi tabl sp configur test experi benchmark order make fastbut faircomparison two version sp select ran small collect test program alreadi use research other prepar us select program aim perform progress evalu machin start simpl point point commun go collect commun numer kernel finegrain nonnumer applic measur obtain experi provid inform latenc throughput commun channel complet network inform use firstord indic predict chang commun subsystem affect execut time parallel applic next subsect select benchmark briefli describ point point commun first group test base code provid dongarra character point point commun use mpi dd two processor engag sort ping pong processor charg measur processor read valu walltim clock invok mpi_send oper block mpi_recv meanwhil processor perform symmetr oper latter oper finish processor clock read again thu delay twomessag interchang one direct measur latenc comput one half time achiev throughput also comput consid latenc messag size oper done time avoid warmup effect anoth time averag result messag size provid input paramet program measur minimum maximum averag latenc throughput consid averag valu repres perform user obtain machin author xh consid minimum valu latenc suppos free influenc oper system user case minimum averag valu close case collect commun sever mpi collect oper object detail measur similar taken point point commun pay special attent two them broadcast reduct experi perform other exhibit similar behavior addit random traffic test perform experi sever messag might compet network resourc well access common destin next describ detail test broadcast test involv processor perform broadcast processor the root oper includ itself test perform iter averag valu iter processor perform broadcast mpi_bcast processor design root barriersynchron mpi_barri comput broadcast delay root measur time moment broadcast start time barrier finish subtract precomput barrier delay obtain throughput figur indic amount inform receiv particip reduct test like previou one use mpi_reduc instead mpi_bcast case throughput figur consid amount inform sent particip mpi offer collect predefin oper mpi_max mpi_min mpi_sum etc perform reduct also allow user defin new oper test userdefin null oper perform last test group random set separ avail processor two group even rank odd rank even processor send data anyon odd processor randomli chosen respond immedi delay twoway interchang achiev throughput comput like point point case parallel applic order assess behavior sp run complet applic select four benchmark mg lu sp bt na parallel benchmark npb version bai shallow water model code swm parkbench suit benchmark wf parallel simul ps develop group mig next briefli describ program version npb obtain sourc code form in contrast version written fortran plu mpi program compil execut without chang sourc code benchmark oper sever input problem size number grid point npb specifi three class a b c depend problem size experi use class b ie mediums problem mg use multigrid method comput solut threedimension scalar poisson equat lu simul comput fluid dynam applic use symmetr success overrelax solv block lower triangularblock upper triangular system equat result unfactor implicit finitediffer discret navierstok equat three dimens sp bt simul comput fluid dynam applic solv system equat result approxim factor implicit finitediffer discret navierstok equat bt solv blocktriangular system x block sp solv scalar pentadiagon system result full diagon approxim factor scheme bai swm parallel algorithm testb solv nonlinear shallow water equat rotat sphere use spectral transform method program use messagepass paradigm fortran plu mpi form part parkbench benchmark suit develop patrick h worley oak ridg nation laboratori ian t foster argonn nation laboratori benchmark includ sever input file select problem size algorithm use use mediums problem requir approxim gb workspac run bit precis gflop default parallel algorithm distribut fourier transform distribut legendr transform ps parallel discreteev simul develop group evalu characterist messag pass network d toru topolog cutthrough flow control paramet simul basic problem size in term number switch element load network in term percentag maximum theoret bandwidth network bisect number time step appropri makefil distribut code simul result report paper toru x router simul cycl load vari mean number messag need perform simul vari million nearli million process organ logic x toru process alway commun four logic neighbor commun follow particular tempor pattern messag short byte code written c plu mpi perform evalu latenc throughput basic paramet character applic view perform commun subsystem conjunct determin adequaci given system execut given type parallel applic first messag latenc impos restrict granular applic and second throughput impos limit maximum amount inform process interchang per unit time reason follow section analyz result describ benchmark term two paramet order show upgrad sp commun subsystem affect them preliminari attempt evalu perform improv achiev new sp commun subsystem intend perform exhaust analysi everi mpi function result discuss minimum kernel aim offer first overview potenti perform chang applic might experi point point commun first approach assess commun perform parallel comput measur minimum time requir send messag two process locat differ node reason mani perform evalu studi concentr point point commun order establish initi comparison point among differ platform cul dd hoc xh tabl figur show result obtain run point point test describ previou section observ tabl introduct new version mpi impli reduct softwar overhead consequ startup time lower latenc notic reduc messag short howev messag kb new version perform wors the explan phenomenon found fra deal later section maximum achiev throughput remain old new mpi implement introduct new switch clearli bring reduct latenc length rang latenc reduct special signific medium long messag thu allow achiev higher throughput latenc s throughput mb byte ov ov nv ov ov nv tabl averag valu latenc throughput point point commun l messag length configur ov ov nv describ tabl latenc s messag thov thov thnv throughput mb messag length figur latenc throughput point point commun data tabl preliminari analysi result easili establish character latenc gener latenc messag length l decompos two compon startup time h time use messag header reach destin spool time time requir transmit remaind messag path origin destin alreadi establish spool time usual model linear function l thu tl express as repres time requir transmit byte path establish paramet interest throughput defin as gener asymptot behavior function l follow l thmax maximum asymptot throughput achiev commun subsystem equat see thmax calcul t b latenc tabl fit equat obtain maximum throughput for unidirect point point commun are mb system behavior short messag long messag see clearli detail set experi carri new switch result shown figur case minimum latenc valu repres averag valu noisi produc clear graph curv fit done use least squar latenc s messag length kb throughput mb messag length kb figur minimum latenc maximum throughput new switch messag smaller kb detail analysi data indic exist three differ region defin messag length see figur one region fit equat differ paramet latenc s messag length kb region latenc s messag length kb region latenc s messag length kb region figur messag latenc three differ messag length region interest observ h smaller region correspond smaller messag b smaller region correspond longer messag mean system optim minim latenc short messag maxim throughput long messag mention refer fra explain chang region region perfectli short messag eager protocol use ie messag sent destin immedi longer messag rendezv protocol use ie messag sent receiv node agre receiv it switch rendezv protocol incur higher startup cost reduc number time inform copi thu reduc cost per byte chang protocol come ibm nativ version mpi discuss also valid experi ov third region behavior latenc linear region figur show clear sawedg effect jump everi kb moment without inform manufactur unabl offer reason hypothesi explain behavior set equat easi see maximum throughput achiev region long messag comput paramet b equat minimum latenc obtain region short messag domin paramet h equat go back comparison among sever sp configur consid exampl data tabl byte quit short messag seen latenc basic old new switch contrast long messag eg latenc notic reduc throughput increas short effect commun subsystem upgrad been hn ho thmax n thmax o therefor upgrad basic effect spool time reduc less one half origin howev startup time use messag header reach destin almost same summar channel throughput doubl startup time remain same result first approach tell us coars grain parallel applic requir interchang larg data structur experi signific perform improv improv also notic oper system level use system increas throughput batch taskswhen node run decoupl fashion contrast applic requir frequent interchang short messag experi signific reduct execut time upgrad fact happen messag size requir obtain reason perform increas chang hockney defin l length messag allow util one half maximum channel bandwidth hoc old subsystem l aprox byte see tabl increas reach around kb see figur therefor minimum messag size requir effici run parallel applic increas order magnitud goal increas perform sp run applic requir frequent interchang short messag imper reduc startup time proport as even than maximum throughput increas tabl summar paramet h thmax l sever wellknown mpp includ three consid sp configur machin h s thmax mb convex spp pvm convex spp pvm cray td pvm intel paragon meiko cs spov tabl latenc asymptot throughput parallel comput data convex cray paragon ksr meiko taken dd collect commun measur characterist repres commun pattern involv two processor sever mpi collect oper random traffic two set regard broadcast collect oper experi offer us inform summar figur seen behavior much like point point case chang softwar mean chang behavior latenc curv use nativ mpi alway reduc latenc regard hardwar upgrad graph show signific improvementalthough signific messag longer kb even larg enough messag latenc reduct complet broadcast oper spectacular point point case and therefor throughput increas impress perform remain collect oper includ reduct describ previou section measur appli strategi use broadcast show similar behavior therefor comment applicableso repeat latenc s messag thov thov thnv throughput mb messag length figur latenc throughput broadcast oper result obtain run experi random commun two set processor plot figur abovement broadcast case signific latenc reduct achiev messag size kb latenc s messag thov thov thnv throughput mb messag length figur latenc throughput random set test parallel applic comment section execut time benchmark applic measur order get insight chang commun subsystem may affect real parallel applic combin comput commun data analyz previou section reader infer reduct execut time mark case parallel applic requir process interchang long messagesmg lu sp bt swm timeold timenew execut time benchmark mg lu sp bt swm mflopsold mflopsnew mflop benchmark figur result npb swm benchmark graph figur show execut time in second perform in mflop bt lu mg sp swm mention nativ version mpi use test therefor basic chang hardwar affect perform result expect improv quit modest case thing better parallel simul perform increas measur figur show execut time vari load simul model which indic number messag applic manag refer point comparison execut time experi node intel paragon time achiev sp old execut time load figur execut time parallel simul summar section state chang commun subsystem effect run parallel applic design abl achiev import reduct startup time ie latenc short medium messag conclus aim write paper offer preliminari evalu effect upgrad introduc ibm commun subsystem sp perform parallel applic clear pretend analysi upgrad system comprehensiveit mainli offer snapshot behavior so collect experi perform use sp frame run benchmark measur perform specif commun oper plu other repres typic parallel applic purpos experi analyz progress effect upgrad overal system perform point point commun collect oper kernel numer applic na parallel benchmark swm nonnumer applic fine grain characterist analyz obtain measur conclud import improv achiev bandwidth commun channel allow applic reach throughput valu much higher achiev old networkalthough possibl applic interchang long enough messag kb mb absolut term point point commun asymptot throughput new commun subsystem doubl previou one contrast messag short improv bandwidth translat better perform startup time reduc word latenc short medium messag chang significantli old new system consequ applic requir frequent interchang massiv amount inform experi clear reduct execut time result shown report confirm conclus research studi real bottleneck mpp commun lie messagepass softwar messag interfac attach node interconnect network therefor point mpp design focu interest opinion overhead messag pass softwar minim even mean introduc chang architectur node processor messag interfac locat close processor possibl connect system bu even bu offchip cach processor acknowledg want express grate acknowledg follow institut c centr de computaci comunicacion de catalunya provid access machin test technic support cicyt research done support comisin interministeri de ciencia tecnologa spain contract tic dgicyt direccin gener de investigacin cientfica tcnica which grant allow ja gregorio stay uci visit associ research depart electr comput engin univers california irvin provid support equip access r sp system architectur na parallel benchmark user guid mpich assess fast network interfac mpi program environ ibm spsp ibm power parallel divis pvm user guid tutori network parallel comput commun challeng mpp intel paragon meiko cs comput architectur empir evalu techniqu parallel simul messag pass network messag pass interfac forum myricom myrinet inform commun softwar parallel environ ibm sp sp highperform switch pstswm v model commun overhead mpi mpl perform ibm sp tr ctr sangman moh chansu yu ben lee hee young youn dongsoo han dongman lee fourari treebas barrier synchron mesh without nonmemb involv ieee transact comput v n p august jess labarta sensit perform predict messag pass program journal supercomput v n p nov zoltan johasz analyt method predict perform parallel imag process oper journal supercomput v n p janfeb j a gregorio r beivid f vallejo model interconnect subsystem massiv parallel comput perform evalu v n p februari manuel prieto ignacio m llorent francisco tirado data local exploit decomposit regular domain problem ieee transact parallel distribut system v n p novemb