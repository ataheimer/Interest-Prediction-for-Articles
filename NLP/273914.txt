t theori neuromata a finit automatonth socal neuromaton realiz finit discret recurr neural network work parallel comput mode consid size neuromata ie number neuron descript complex ie number bit neuromaton represent studi prove constraint time delay neuromaton output play role within polynomi descript complex shown regular languag given regular express length n recogn neuromaton thgrn neuron further prove network size is worst case optim hand gener equival polynomi length regular express given neuromaton then two special construct neural acceptor optim descript complex thgr b introduct neural network model comput motiv idea brain function comput power effici tradit investig within framework comput scienc one less commonli studi task address comparison comput power neural network tradit finit model comput recogn regular languag appear finit research support ga cr grant no discret recurr neural network use languag recognit parallel mode time step one bit input string present network via input neuron output neuron signal possibl constant time delay whether input string read far belong relev languag way languag recogn neural acceptor briefli neuromaton clear neuromata recogn regular languag similar definit neural acceptor appear problem languag recognit neural network explor context finit automata shown everi mstate determinist finit automaton realiz discret neural net om neuron least neuron necessari construct upper lower bound improv show thetam neuron suffic that worst case network size cannot decreas assum either ologmtim simul delay polynomi weight moreov sever experi train secondord recurr neural network exampl behav like determinist finit automata either practic exploit rule extract also done use standard neural learn heurist backpropag present paper relat size neural acceptor length regular express one hand known possess express power finit automata hand repres tool whose descript effici exceed determinist finit automata first section respect introduc basic formal deal regular languag neuromata prove constant time delay neuromaton output play role within linear neuromata size therefor restrict neuromata respond one comput time step input string read then section prove regular languag describ regular express length n recogn neuromaton consist on neuron subsequ section show that gener result cannot improv regular languag given regular express length n requir neural acceptor n therefor respect neuromaton construct regular express sizeoptim use exampl construct neural learn learn algorithm regular express exampl string first employ hand section construct proven effici revers exist neuromaton everi equival regular express exponenti length next section present two special construct neural acceptor singl nbit string recognit requir on neuron either on connect constant weight on weight size o number bit requir entir string acceptor descript case proport length string mean automata construct optim descript complex point view exploit part complex neural network design exam ple construct cyclic neural network o n neuron edg comput boolean function section introduc concept hopfield languag languag recogn socal hopfield acceptor hopfield neuromata base symmetr neural network hopfield network hopfield network studi wide outsid framework formal languag converg properti formal languag theoret point view prove interest fact name class hopfield languag strictli contain class regular languag henc repres natur proper subclass regular languag furthermor formul necessari suffici socal hopfield condit state regular languag hopfield languag section show construct hopfield neuromaton on neuron regular languag satisfi hopfield condit thu obtain complet character class hopfield languag far as closur properti hopfield languag concern show class hopfield languag close union intersect concaten complement close iter final section investig complex empti problem regular languag given neuromata hopfield acceptor prove problem pspacecomplet somewhat surpris ident problem regular express determinist nondeterminist finit automata known nlcomplet confirm fact section neuromata stronger regular express descript complex point view next consequ obtain equival problem neuromata pspacecomplet well previou result jointli point fact neuromata present quit effici tool recognit regular languag subclass respect also descript addit abovement construct gener analog neural network preliminari version paper concern gener neuromata hopfield languag respect appear regular languag recal basic notion languag theori introduc definit regular express determin regular languag concept determinist finit automaton defin kleen theorem correspond finit automata regular languag mention well alphabet finit set symbol string alphabet sigma finitelength sequenc symbol sigma empti string denot e string symbol x string concaten x string xy string xx ntime abbrevi x n length string x denot jxj total number symbol x languag alphabet sigma set string sigma let l l two languag languag l delta l call concaten l l g let l languag defin n iter l denot l languag l n l n similarli posit iter definit set regular express alphabet defin minim languag alphabet f follow condit ff fi also ff write regular express omit mani parenthes assum higher preced concaten latter higher preced exampl may written abbrevi ntime ff n jff n j remain n delta jffj definit set set regular languag ff denot regular express ff follow ff fi ff also use regular express ff correspond posit iter ff definit determinist finit automaton tupl finit set automaton state sigma input alphabet in case gamma q transit function q q initi state automaton f q set accept state definit gener transit function automaton defin follow way fg languag recogn finit automaton a theorem kleen languag l regular f recogn finit automaton ie neuromata section formal concept neural acceptor socal neu romaton discret recurr neural network or neural network short exploit languag recognit follow way network com putat input string present bit bit network mean singl predetermin input neuron neuron network work paral lel follow thi possibl constant time delay output neuron show whether input string alreadi read relev languag similar definit appear definit neural acceptor briefli neuromaton tupl out set n neuron includ input neuron inp output neuron v set edg set integ weight function we use abbrevi z threshold function the abbrevi initi state network graph v e call architectur neural network n size neuromaton number bit need whole neuromaton represent especi weight threshold function call descript complex neuromaton formal due notat consist arbitrari xml f g l input neuroma ton further assum orient path inp architectur v e length least k state neural network discret time map g begin neural network comput state set time step network comput new state old state tgamma follow otherwis neural acceptor n input x f g denot state output neuron v time g languag recogn neuromaton n time delay k first show respect languag recognit capabl constant time delay neuromaton output play role within linear neuromata size precis languag recogn time delay k neuromaton size n recogn time delay neuromaton size o k n neuromaton size n orient path inp architectur v e length least exist neuromaton n init size k n proof idea proof construct n way check ahead possibl comput n next step cancel wrong comput match actual input read time delay purpos besid input output neuron inp neuroma ton n consist k block n x forese comput n one possibl next k bit x input denot neuron block way n index relev k bit state neuron v x n equal state neuron v v n comput next bit ie first k gamma bit x f g k perform achiev follow block n yb f g new state old state block n ay f g therefor neuron n ay connect neuron n yb correspond edg label relev weight respect origin weight function w n block n yb constant input b taken account modifi threshold neuron block especi input bit c f g n indic comput n ay c valid therefor input neuron inp cancel comput set neuron state n ay zero thu neuron kgamma block n ay state previou input bit match a also enabl block n yb execut correct comput influenc one invalid block either n n connect n yb suppress due zero state neuron block n x connect x lead output neuron well label weight know half influenc moreov among remain one correspond neuron v x v v block n x state distanc inp n least k last k input bit cannot influenc output suffici multipli threshold kgamma order preserv function comput out way correct recognit accomplish lookahead formal definit neuromaton n follow ay ay init v ya kgamma vy state neuron v neuromaton n input g kgamma time step k gamma term bwhinp vi threshold definit take account weight constant input block n y f g correspond origin weight associ edg lead input inp relev neuron v neuromaton n definit w ensur set state neuron block n y f g kgamma zero iff input inp similarli definit weight w togeth term aw hinp threshold definit caus zero state neuron block n y clearli size neuromaton n follow lemma restrict neuromata respond one comput time step input string read size is constant multipl factor larg size equival neuromata constant time delay therefor rest paper assum time delay neuromaton recognit neuromaton architectur contain edg input output neuron also denot l n ln neuromaton n next prove neural acceptor view finit automaton therefor neuromata recogn exactli regular languag due theorem theorem let languag recogn neuromaton n l regular proof let neural acceptor defin determinist finit automaton sigmag q transit function defin q x sigma follow final sigmag proposit follow theorem upper bound show regular languag given regular express length n may recogn neuromaton size on idea recognit neuromaton compar input string possibl string gener regular express report via output neuron whether string match input therefor construct architectur neural network correspond structur regular express neuromaton proce orient network path correspond string gener express that time match part input string read far theorem everi regular languag l rl denot regular express exist neuromaton n size jffj l recogn n ie proof let regular languag denot regular express ff construct neuromaton n jffj first build architectur v e neural network n ff recurs respect structur regular express ff purpos defin sequenc graph correspond whole express ff recurs partit shorter regular subexpress v correspond elementari subexpress ff we say vertic type sake notat simplic identifi subexpress ff vertic graph assum alreadi construct subexpress ff differ henc besid empti languag empti string regular express fi denot union concatena tion iter subexpress fi respect relev regular oper vertex fi fraction possibl new vertic correspond subexpress fi aris graph v realli rigor first remov vertex fi add new vertic howev due notat simplic insist rigor therefor identifi one new vertic old fi write rather inexactli exampl fi form fi fl moreov ffl fi v g ffl fi e v fiigg ffl fi form fi g ffl fi form fi g ffl fi form fiig construct finish contain subexpress defin network architectur follow way defin weight function w threshold function neuron type neuron type initi state defin set v contain three special neuron inp out start well as neuron type one subexpress ff henc jv exampl neuromaton regular languag figur the type neuron depict insid circl repres neuron threshold depict weight edg constant input gamma inp start figur neuromaton prove construct v abov easi observ graph correspond structur regular express ff mean everi string orient path start lead v p contain vertic relev type hand path correspond string l neural acceptor n ff pass possibl path match network input begin noninput neuron start v activ it state send signal connect neuron subsequ becom passiv it state due domin threshold connect neuron type compar type network input becom activ match otherwis remain passiv due weight threshold valu follow neuron type becom activ iff inp v activ least one j inp hj ii e activ neuron type becom activ iff inp v passiv least one j inp hj ii e activ way relev path travers travers end neuron realiz logic disjunct activ iff prefix input string read far belong l complet proof lower bound section show lower n number neuron that worst case necessari recognit regular languag describ regular express length n consequ follow construct neuromaton section sizeoptim standard techniqu employ purpos given length regular express defin regular languag correspond set exponenti number prefix languag prove prefix must bring neuromaton exponenti number differ state order provid correct recognit impli desir lower bound definit denot ln pi k pn respect follow regular languag hi clear pn n set prefix languag ln prove sever lemma concern properti regular languag regular express defin languag ln definit fact length abbrevi repeat concaten includ determin length therefor first show regular express ff n linear length onli denot languag ln number prefix pn shown exponenti respect n proof i regular express denot languag ln definit subsequ factor n gamma time subexpress e obtain desir regular express linear length jff defin languag ii follow definit jpi k follow lemma show prefix pn complet string ln lemma proof i ii follow definit iii assum languag ln defin via iter definit henceforth write pi prove two differ prefix pn complet suffix one result string ln one not lemma proof assum x exist distinguish two case without loss gener suppos n ii lemma obtain x due ii lemma write x fe g g without loss gener ffl denot z pi i lemma z ln z ln ii lemma impli x ln close concaten contrari suppos x exist henc z ii lemma follow z contradict thu x ln readi prove follow theorem concern lower bound theorem neuromaton n recogn languag n neuron proof neuromaton n recogn languag must differ state reach take input prefix pn differ x complet lemma x ln x ln impli n needsomegagamma n binari neuron neuromata stronger although result section seem descript complex point view neuromata regular express polynomi equiva lent section show exist neuromaton everi equival regular express exponenti length mean neu romata construct regular express describ section effici revers theorem everi n exist regular languag ln recogn neuromaton nn size on descript complex on regular express ff n defin defin finit languag set binari string length m neuromaton recogn ln binari nbit counter accept input string iff length m neuron correspond ith bit counter chang state iff howev correspond boolean function cannot comput one neuron and therefor small subnetwork three neuron introduc purpos v disjunct b least one v state requir take two comput step mean counter two time slower requir count till m moreov neuron v gener binari sequenc special neuron rst introduc suppress fire output neuron v ngamma bit read formal definit counter follow clearli size nn order on descript complex on let ff n minim length regular express defin languag prove jff n m express ff contain iter defin finit languag the part contain iter would denot empti languag and thu could omit gener string express ff n read left right side without return sinc iter allow string ln length ff n must contain least symbol f g henc jff theorem show that sens big gap descript power regular express neuromata discuss issu later section neural string acceptor previou result show neuromata descript capabl regular express section studi power confin certain subclass regular express here deal simplest case consid fix binari string f g present two construct neural acceptor singl string recognit nbit string requir on neuron either on connect constant weight on weight o number bit requir entir string acceptor descript case proport length string mean construct optim descript complex point view studi elementari case use singl string recognit often part complic task techniqu develop architectur neural network design can exampl sometim improv construct neuromata section regular express consist long binari sub string exampl applic construct neural learn string view train pattern result network compos neural acceptor string work parallel theorem string f g n exist neural acceptor size on neuron on connect constant weight thu descript complex neuromaton thetan proof sake simplic suppos first posit integ p idea neural acceptor construct split string f g n p piec length p encod p substr use p for p binari weight edg lead p compar neuron c input string gradual store per p bit p buffer neuron buffer full relev compar neuron c compar assign part string correspond part input string x store buffer send result comparison next compar neuron synchron comparison perform p clock neuron neuron tick time step network comput neuron tick period p step last compar neuron c p repres output neuron report end whether input string x match a formal definit neuromaton recognit string follow see also figur defin denot fi fi put ae remain weight threshold set final initi state i i i i i i i i i i i i x i i z z z z z z z z z z z z z z z z z z z z z z z z z z l l l l l l l l l l l l l l l l l l l l z z z z z z z z z z z z z z z z z z z z z z ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae omega omega omega omega omega omega omega omega omega omega omega omega omega omega omega omega omega omega inp sp start figur architectur neural string acceptor on edg note weight w constant requir due neuron c keep state becom activ order transfer possibl posit result comparison next compar neuron therefor relev feedback must exceed sum input achiev threshold valu avoid insert auxiliari neuron rememb result preced comparison neighbor compar neuron neuron constant feedback one input previou compar neuron exceed construct neural acceptor also easili adapt p techniqu proof theorem employ recognit last r bit string a result architectur size or connect neural string acceptor identifi neuron start abovement neuron i i i i i i i i i x i i i i i i i i i i i i i z z z z z z z z z z z z z z z z z z z z z z z z z z l l l l l l l l l l l l l l l l l l l l z z z z z z z z z z z z z z z z z z z z z z ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae omega omega omega omega omega omega omega omega omega omega omega omega omega omega omega omega omega omega l l l l z z start q q q inp reset sp figur architectur neural string acceptor on theorem string f g n exist neural acceptor size neuron on weight size o thu descript complex neuromaton thetan proof design desir neural acceptor similar construct proof theorem except number compar neuron reduc two neuron c c case substr encod op each weight size o p correspond connect lead clock neuron comparison neuron content input buffer view binari number first convert integ integ compar relev encod part string compar neuron c see whether smaller equal time compar compar neuron c see whether greater equal neuron realiz logic conjunct compar output ad indic whether part input string match correspond part string a howev lead one comput step neural acceptor correct synchron achiev exploit abovement addit architectur recognit last r bit q string a detail synchron omit well as complet formal definit neural string acceptor give definit weight relev comparison pgammaj igammapj weight w defin differ clock neuron activ part input buffer architectur neural string acceptor depict figur instead abovement conjunct neuron reset ad realiz negat compar conjunct possibl termin clock piotr indyk point latter string acceptor construct theorem also exploit build cyclic neural network o n neuron edg comput boolean function case binari vector function valu encod string acceptor posit relev bit part vector includ desir output given first nbit input possibl correspond n bit substr gener present acceptor achiev relev part function valu vector relev bit extract posit determin last nbit input hopfield languag section restrict special subclass regular expr sion section concentr special type neural acceptor socal hopfield neuromata base symmetr neural network hopfield network network weight symmetr therefor architectur neuromata seen undirect graph hopfield network tradit studi use due favorit converg properti network also particular interest natur physic realiz exist eg ise spin glass optic comput use concept hopfield neuromata defin class hopfield languag recogn particular acceptor neuromaton base symmetr neural network hopfield network hi call hopfield acceptor hopfield neuromaton languag recogn hopfield neuromaton n call hopfield languag first show class hopfield languag strictli contain within class regular languag purpos formul necessari condit socal hopfield condit regular languag hopfield languag intuit hopfield languag cannot includ word potenti infinit substr allow hopfield neuromaton converg forget relev inform previou part input string recogn idea proof find necessari condit prevent hopfield neuromaton converg definit regular languag l said satisfi hopfield condit f everi theorem everi hopfield languag satisfi hopfield condit proof let hopfield languag recogn hopfield neuro ng defin integ vector size n theta let gammafinpg integ matrix size n theta n note matrix w symmetr sinc n hopfield acceptor let us present input string v x acceptor n suffici larg network comput must start cycl input network n possibl state differ state cycl x correspond input bit state follow state i index shift permut invers permut let r compos permut r further let c binari vector size n theta follow definit state cycl defin integ symbol denot transposit vector obvious p use fact matrix w symmetr obtain moreov x write know therefor c impli p e p cannot c n time case inequ c strict complementari case c simultan imposs well sinc then number s c i would greater number s c therefor conclud c consequ impli cycl length p henc everi v f g either complet proof l satisfi hopfield condit exampl follow theorem regular languag hopfield languag satisfi hopfield condit hopfield condit suffici section prove necessari hopfield condit definit state regular languag hopfield languag suffici well construct hopfield neuromaton shown regular languag satisfi hopfield condit theorem everi regular languag satisfi hopfield condit exist hopfield neuromaton n size jffj l recogn n henc l hopfield languag proof architectur hopfield neuromaton regular languag ff satisfi hopfield condit given gener construct proof theorem result obtain orient network n correspond structur regular express ff neuron n n besid special neuron inp out start associ one symbol f g ff ie type s task n check whether agre input bit transform n equival hopfield network n suppos ff contain iter binari substr two bit standard techniqu transform acycl neural network hopfield network employ idea consist adjust weight prevent propag signal backward preserv origin function neuron transform start neuron out carri opposit direct orient edg end neuron start neuron whose outgo weight alreadi adjust threshold incom weight multipli suffici larg integ exce sum absolut valu outgo weight suffici suppress influenc outgo edg neuron transform accomplish orient path lead start label decreas sequenc weight problem lie realiz gener iter use symmetr weight consid subnetwork n correspond iter substr ff let subnetwork arisen subexpress fi ff proof theorem abovement transform perform path lead incom edg outgo one label decreas sequenc weight order avoid backward signal spread signal propag output subnetwork back subnetwork input iter requir one hand integ weight associ connect small enough order suppress backward signal propag hand weight suffici larg enough influenc subnetwork input neuron clearli two requir contradictori consid simpl cycl c subnetwork consist orient path pass one backward edg lead end path ie output i begin ie input i let type neuron cycl c establish iter f g jaj moreov suppos x f g x k k hopfield condit set v postfix l associ path lead c out similarli set v prefix l associ path lead start c everi contradict hopfield condit prefix v exist becaus otherwis cycl c could realiz iter two bit therefor x k impli string contain substr form b b b henc string form either notat simplic confin former case latter remain similar furthermor consid ba minim ja j shift decreas sequenc weight c start end neuron n relev weight n modifi abovement procedur ensur consist linkag c within n exampl mean edg lead output neuron c input neuron evalu suffici larg weight realiz correspond iter problem lie signal propag neuron n b neuron n assum support small weight connect n b n new neuron id copi state input neuron inp connect neuron n via suffici larg weight strengthen small weight connect n b obvious neuron n b b new neuron id activ time enabl requir signal propag n b n togeth hand neuron n b activ neuron id passiv due fact copi input prevent neuron n becom activ time howev symbol b ff neuron n b outsid cycl c but within subnetwork i edg n lead situat correspond concaten union oper within fi case activ neuron n b id would caus neuron n fire avoid thi add anoth new neuron n behav ident neuron n symbol g thu neuron connect n link n edg origin outgo n n b correspond b reconnect lead n similar approach use opposit case abovedescrib procedur appli simpl cycl c subnetwork correspond iter fi cycl necessarili disjoint decomposit ba minim ja j ensur consist synthesi similarli whole transform process perform iter ff case iter part anoth iter magnitud weight inner iter need accommod embodi outer iter also possibl neuron id support iter point final number simpl cycl ff jffj henc size result hopfield neuromaton remain order jffj figur preced construct illustr exampl hopfield neuromaton regular languag simpl cycl consist neuron clarifi detail notic decreas sequenc weight cycl start end neuron n well as neuron id enabl signal propag n b n neuron n ident n also creat neuron origin connect n see figur start inp id id figur hopfield neuromaton corollari let l regular languag l hopfield languag f l satisfi hopfield condit final briefli investig closur properti class hopfield languag theorem class hopfield languag close union concaten intersect complement close iter proof close hopfield languag union concaten follow corollari obtain hopfield neuromaton complement negat function output neuron multipli associ weight and threshold ad threshold henc hopfield languag close intersect well final due theorem hopfield languag wherea hopfield languag satisfi hopfield condit theorem empti problem order illustr descript power neuromata investig complex empti problem regular languag given neuromata hopfield acceptor prove problem pspacecomplet definit given hopfield neuromaton n hopfield neuromaton empti problem denot nep hnep issu decid whether languag recogn hopfield neuromaton n nonempti theorem nep hnep pspacecomplet proof show nep hnep pspace input string hopfield neuromaton n guess bit bit accept check simul network comput polynomi space wit nonempti next show nep pspacehard let arbitrari languag pspace x f g will polynomi time construct correspond neuromaton n x iff n nep further let polynomi space bound ture machin recogn a first cyclic neural network n simul construct polynomi time use standard techniqu idea construct tape cell subnetwork simul tape head posit comput ie local transit rule neighbor subnetwork connect enabl head move input x encod initi state n end neural network comput one neuron n call result signal whether x a neural network n embodi neuromaton n follow input neuron inp n connect neuron output neuron n identifi neuron result n accept x iff neuron activ end simul iff ln contain word length equal length comput x iff n nep thu x iff n nep complet proof nep pspacecomplet hopfield neuromata similar simul achiev use symmetr neural network n converg comput arbitrari asymmetr neural network simul symmetr network polynomi size assum without loss gener stop everi input henc hnep pspacecomplet well theorem somewhat surpris ident problem regular express determinist nondeterminist finit automata known nlcomplet sens show big gap descript power regular express neuromata confirm result section discuss issu later section differ descript power regular express neu romata also illustr complement oper empti problem complement regular express becom pspacecomplet empti problem complex complement neuromaton chang output neuron easili negat next consequ show neuromaton equival problem pspacecomplet well given two hopfield neuromata n n hopfield neuromaton equival problem denot neqp hneqp issu decid whether languag recogn hopfield neuromata same ie whether corollari neqp hneqp pspacecomplet proof prove neqp suffici show complement pspace purpos input string neuro mata guess accept one two neuromata reject one check simul network comput polynomi space wit nonequival ln show neqp pspacehard complement nep denot conep pspacecomplet well polynomi time reduc neqp let neuromaton n instanc conep polynomi time construct correspond instanc n n neqp ln empti iff neuromaton n identifi n let n arbitrari small neuromaton recogn empti languag easi see correct reduct henc neqp pspacecomplet pspacecomplet hneqp achiev similarli conclus paper explor altern formal regular languag represent base neural network compar socal neuromata classic regular express obtain result within polynomi descript complex nondetermin captur regular express simul parallel mode neuromata determinist natur opinion descript power neuromata consist effici encod transit function transit function determinist nondeterminist finit automata usual specifi list valu old state input symbolnew state function neuromata given vector formula one neuron evalu it encod interpret like gener program point view easi observ tabl transit rule determinist finit automata special case program moreov neuromata encod nondeterminist transit function effici use parallel way behavior neuromata exponenti number possibl state describ polynomi size howev process revers neural transit program cannot gener rewritten polynomi size tabl polynomi length regular express moreov number neuromaton state although exponenti limit match lower bound neuromaton size achiev use standard techniqu exist regular languag requir exponenti number neuromaton state recogn also complex neural transit rule specif make empti problem neuromata harder classic formal finit automata regular express hand complement oper nondeterminist case formal caus exponenti growth descript complex complement neuromata easili achiev also investig hopfield neuromata studi wide due converg properti shown hopfield neuromata determin proper subclass regular languag socal hopfield languag via socal hopfield condit complet character class hopfield languag conclud neuromata present quit effici tool recognit regular languag subclass respect also descript acknowledg grate marku holzer piotr indyk petr savicki stimul discuss relat topic paper thank tereza bedanova realiz pictur latex environ r design analysi comput algorithm effici simul finit automata neural net learn regular express pattern match complex issu discret hopfield net work learn extract finit state automata secondord recurr neural network bound complex recurr neural network implement finit state machin optim simul automata neural net person commun note space complex decis problem finit automata represent event nerv net finit au tomata comput complex neural network survey circuit complex neural network discret neural comput theoret foundat complex issu discret neurocomput learn finit state machin selfclust recurr network tr effici simul finit automata neural net neurocomput note space complex decis problem finit automata learn extract finit state automata secondord recurr neural network circuit complex neural network learn finit machin selfclust recurr network discret neural comput learn extract initi meali automata modular neural network model bound complex recurr neural network implement finit state machin design analysi comput algorithm comput complex neural network hopfield languag learn regular express pattern match