t optim thread mpi execut smp cluster a previou work shown use thread execut mpi program yield great perform gain multiprogram sharedmemori machin paper investig design implement threadbas mpi system smp cluster studi indic proper design thread mpi execut pointtopoint collect commun perform improv substanti compar processbas mpi implement cluster environ contribut includ hierarchyawar adapt commun scheme thread mpi execut threadsaf network devic abstract use eventdriven synchron provid separ collect pointtopoint commun channel paper describ implement design illustr perform advantag linux smp cluster b introduct commerci success smp architectur smp cluster commod compon wide deploy high perform comput due great econom advantag cluster mpi messagepass standard wide use highperform parallel applic implement larg array comput system paper studi fast execut mpi program dedic smp cluster mpi paradigm mpi node execut piec program separ address space mpi node uniqu rank allow mpi node identifi commun peer result global variabl declar mpi program privat mpi node natur map mpi node pro cess howev commun process go oper system kernel could costli previou studi show processbas implement suer larg perform loss multiprogram sharedmemori machin smm map mpi node thread open possibl fast synchron address space share approach requir compil transform mpi program threadsaf form demonstr previou tmpi work approach deliv signific perform gain larg class mpi c program multiprogram smm extend thread mpi implement singl smm support smp cluster straightforward smp cluster environ process thread within machin commun share memori commun process thread dierent machin go network normal sever order magnitud slower sharedmemori access thu addit map mpi node thread within singl machin import ecient mpi implement take advantag twolevel commun channel hierarchi common intuit cluster environ internod messag delay domin perform com munic thu advantag execut mpi node thread diminish shown later experi counter intuit multithread yield great perform gain mpi commun smp cluster environ use thread speed synchron thread singl smp node also greatli reduc buer orchestr overhead commun among thread dierent node paper present design implement threadbas mpi system linux smp cluster examin benefit multithread platform key optim propos hierarchyawar adapt commun scheme thread mpi execut threadsaf network devic abstract use eventdriven synchron provid separ collect pointtopoint commun channel eventdriven synchron among mpi node take advantag lightweight thread elimin spin overhead caus busi poll channel separ allow flexibl ecient design collect commun prim itiv experi conduct dedic smp cluster expect greater perform gain demonstr nonded cluster futur work rest paper organ follow section introduc background overview threadbas mpi system tmpi cluster section discuss design tmpi system section report perform studi section conclud paper background overview design base mpich known portabl mpi implement achiev good perform across wide rang architectur scheme target mpi program execut use thread thu first give type mpi program address paper briefli give overview mpich system then give highlevel overview threadbas mpi system tmpi smp cluster use thread execut mpi program use thread execut mpi program improv perform portabl mpi program minim impact multiprogram due fast context switch ecient synchron thread shown experi sgi origin indic thread mpi execut outperform sgi nativ mpi implement order magnitud multiprogram smm mention befor need compiletim transform mpi program emerg process model use mpi paradigm major task procedur call variabl privat basic provid perthread copi global variabl insert statement origin program fetch thread privat copi variabl insid function global variabl referenc algorithm base gener mechan avail thread librari call threadspecif data tsd extend tsd make feasibl run multithread mpi program note tsdbase transform algorithm abl support multithread within singl mpi node howev current tmpi runtim system allow thread within singl mpi node call mpi function simultan mpi_thread_seri detail inform pleas refer everi mpi program transform map mpi node thread one major restrict aect applic thread execut mpi program cannot call lowlevel librari function threadsaf eg signal sinc scientif program involv type function particularli mpi specif discourag use signal techniqu applic larg class scientif applic two minor factor need mention aect much applic techniqu total amount memori use mpi node run one smp node fit singl virtual address space problem consid bit os becom popular fix perprocess file descriptor tabl size unix system sinc unix network librari use file descriptor repres stream connec tion applic might fail total number file open mpi node singl smp rel larg applic regular read computewrit pattern modifi separ file io program circumv problem mpich smp cluster mpich follow sophist layer design welldocu discuss literatur design borrow idea mpich briefli summar architectur design mpich section goal mpich layer design make easi fast port system new architectur yet allow room tuneup replac rel small part softwar shown figur mpich commun implement consist four layer bottom top are devic layer includ variou oper system facil softwar driver kind commun devic adi layer layer encapsul dierenc variou commun devic provid uniform interfac upper layer adi layer export pointtopoint commun interfac mpi pointtopoint primit built directli upon adi layer also manag highlevel mpi commun semant context commun mpi collect primit built upon point topoint primit layer messag share channel pointtopoint commun collect commun mpich use special tag distinguish messag belong user pointtopoint commun intern messag collect oper mpi collect mpi pointtopoint adi chameleon interfac td sgi other shmem mpi collect mpi point point abstract devic interfac devic figur mpich commun architectur port mpich dierent platform necessari wrap commun devic target system provid adi interfac design mainli target larg parallel system network cluster map mpi node individu process easi modifi current mpich system map mpi node lightweight thread lowlevel layer mpich threadsaf even though latest mpich releas support mpi standard mt level actual mpi_thread_singl cluster node mpi node process mpich daemon process interprocess pipe share memori connect ws ws ws ws figur mpich use combin tcp share memori shown figur current support smp cluster mpich basic combin two devic sharedmemori devic network devic tcpip figur show sampl setup configur exampl mpi node evenli scatter cluster node also mpi daemon process one cluster node fulli connect other daemon process necessari drain messag tcp connect deliv messag across clusternod boundari mpi node commun daemon process standard interprocess commun mechan domain socket mpi node cluster node also commun share memori ws ws ws ws cluster node mpi node process mpich daemon process interprocess pipe connect figur mpich use tcp onli one also configur mpich compil time make complet oblivi sharedmemori architectur insid cluster node use loopback devic commun among mpi node run cluster node case setup look like figur exampl show number mpi node node distribut previou configur dierent previou one daemon process one mpi node send messag mpi node cluster node go path send messag mpi node dierent cluster node possibl faster thread mpi execut smp cluster cluster node mpi node thread connect direct mem access thread sync ws ws ws ws figur commun structur thread mpi execut section provid overview thread mpi execut smp cluster describ potenti advantag tmpi facilit understand take sampl program use figur illustr setup commun channel tmpi or threadbas mpi system figur see map mpi node cluster node thread insid process add addit daemon thread process despit appar similar figur number dierenc design mpich tmpi commun mpi node cluster node use direct memori access instead sharedmemori facil provid oper system tmpi commun daemon mpi node also use direct memori access instead domain socket unlik processbas design remot messag send receiv deleg daemon process threadbas mpi implemen tation everi mpi node send receiv remot mpi node directli shown later section dierenc impact softwar design provid potenti perform gain tmpi threadbas mpi system addit tmpi give us follow immedi advantag processbas mpi system compar mpich system use mix tcp share memori depict figur mpich usual share memori limit system resourc osimpos limit maximum size singl piec share memori also systemwid bound total size share memori fact one test benchmark come mpich fail limit sharedmemori resourc threadbas system suffer problem thread process access whole address space easi way mpich aggreg dier ent type devic result mpi node use nonblock poll check pend messag either devic could wast cpu cycl sender readi yet synchron among thread flexibl lightweight combin eventdriven style daemon mpi node freeli choos busi spin block wait messag share memori use mpich persist systemwid resourc automat way perform resourc clean program clean execut oper system could run sharedmemori descriptor buggi user program exit without call proper resourc cleanup function leav garbag share memori os thu reliabl user program run mpich base cluster sensit type error threadbas system tmpi problem share address space access complet userlevel oper compar threadbas mpi system pure tcpip base mpich implement depict figur follow disadvantag two data copi send messag two mpi node cluster node mpich synchron two mpi node also becom complic threadbas system tmpi prolifer daemon process network connect situat get even wors fatter cluster node node processor run mpi node relat work mpi network cluster also studi number project lammpi mpi system base upon multicomput manag environ call trolliu dierent mpich sens design specif network cluster lower level commun provid standalon servic uniqu request progress interfac address issu optim mpi perform cluster smp sun mpi implement discuss optim mpi collect commun primit larg scale smp cluster focu work version redhat linux instal kernel version number mb optim collect oper singl fat smp node mpistart made coupl optim smp cluster modifi mpich adi layer propos twolevel broadcast scheme take advantag hierarch commun channel collect commun design extend idea highli optim thread mpi execut magpi optim mpi collect commun primit cluster connect widearea network mpifm mpiam attempt optim perform lower level commun devic mpi techniqu appli tmpi system mpilit lpvm tpvm studi problem run messag pass program use thread singl sharedmemori machin knowledg research eort toward run mpi node use thread smp cluster research complement work focus take advantag execut mpi node use thread system design implementa tion section detail design implement threadbas mpi system tmpi system architectur mpi inter intra other mpi commun inter intramachin commun abstract network thread sync interfac os facil thread pthread thread impl figur tmpi commun architectur system architectur mpi commun primit shown figur four layer bottom top are oper system facil tcp socket interfac pthread network thread synchron abstract layer potenti layer allow portabl tmpi system perform tuneup provid dierent implement network commun thread synchron thread synchron abstract almost direct map pthread api except thread launch batch style creat thread entiti start singl function call time function call return thread finish execut network devic abstract tailor thread mpi execut model dierent either tradit socket interfac mpich adi interfac talk detail section section synchron primit shown figur compareandswap low level abstract commun manag thread intra dierent in cluster node intraclusternod commun manag commun among thread singl cluster node interclusternod commun layer wrap network abstract interfac manag logic commun multithread level specif thread local rank uniqu among thread cluster node global rank uniqu among thread cluster node given global rank need find cluster node thread resid local rank cluster node well revers lookup also need anoth function interclusternod commun modul resourc discov alloc api allow flexibl placement mpi node eg mani cluster node use rank mpi node place cluster node decis made anywher complet automat fulli control user suppli paramet mpi commun primit implement includ notion commun messag tag context implement upon three build block intra interclusternod commun thread interfac particularli collect primit implement awar twolevel commun hierarchi despit similar mpich design shown figur coupl notabl dierenc two system top layer tmpi awar dierent mechan commun thread dierent cluster node implement organ commun take advantag twolevel hierarch commun channel discuss section section tmpi collect commun primit built upon pointtopoint commun prim itiv instead implement independ top layer network devic abstract provid pointtopoint collect commun among process dierent cluster node tmpi network devic abstract network devic abstract tmpi call netd abstract network applic group relev process dierent machin uniqu rank provid basic commun function applic thin layer contain core function group three categori connect manag includ creation process number cluster node setuptear commun channel relev process fulli connect queri id total number process collect commun provid collect commun among relev process creat netd current implement use adapt algorithm choos among three dierent span tree base number process involv size small simpl scatter tree use hypercub use size larg balanc binari tree chosen size fall middl pointtopoint commun netd uniqu pointtopoint commun api messag contain messag header part option payload part content messag header interpret netd receiv messag caller must provid netd callback function call messag handl netd buer messag header invok handl sender id messag messag header messag handl respons examin header perform necessari action receiv payload part one interfac necessari ecient support mpi commun primit tmpi mpi node receiv messag unknown sourc mpi_any_sourc situat complic tmpi normal network devic provid atom receiv oper thu multipl thread wait messag port singl logic messag could fragment receiv dierent thread get around prob lem tmpi use daemon thread receiv incom messag invok messag handl context daemon thread messag handl respons decod actual sourc destin buer incom data notifi destin thread separ pointtopoint collect commun channel netd weve mention figur everi thread process access commun port origin process featur inspir us idea separ collect pointtopoint commun channel allow tmpi take full advantag synchron natur collect commun elimin intermedi daemon overhead reason figur thick line actual repres two tcp connec tion one dedic collect oper pointtopoint commun daemon thread respons serial incom messag pointtopoint commun separ pointtopoint commun channel collect commun channel base care observ mpi semant mpi receiv oper much complic collect oper besid wildcard receiv oper discuss befor outoford notion messag tag asynchron notion nonblock receiv thu daemon thread requir serial also buer purpos due limit buer network devic daemon thread activ drain incom messag deadlock situat permit mpi standard could happen figur show exampl node node would block mpi_bsend statement without presenc daemon thread even enough buer space avail collect commun mpi oper never out oford alway synchron sens collect oper complet mpi node involv reach rendezv point more structur span tree determin runtim oper mpich separ collect pointpoint commun channel highlevel mpi collect commun primit implement top point topoint commun layer result collect oper go pointtopoint commun daemon caus unnecessari overhead separ pointto point collect commun channel could benefit processbas mpi implement well howev may eectiv tmpi two process cluster node cannot directli share network commun port eg socket hierarchyawar collect commun design collect commun tmpi implement seri intra interclusternod collect communi cation exampl mpi_bcast interbcast follow intrabcast mpi_allreduc intra reduc follow interreduc interbcast finish intrabcast intraclusternod collect commun take advantag address space share implement base ecient lockfre fifo queue algorithm way collect commun take full advantag twolevel commun hierarchi conceptu twophas collect commun build span tree two step idea first mention mpistart project essenti design root mpi node cluster node form span tree connect cluster node mpi node connect local root cluster node form whole span tree span tree exactli n edg cross clusternod boundari call network edg n number cluster node involv commun note mpich use share memori set actual take twostep approach build span tree directli given number mpi node without know distribut mpi node cluster node result span tree collect commun mpich may network edg figur compar span tree sampl mpi program size run cluster node see tmpi the left part result network edg mpich the right part network edg possibl employ techniqu pipelin asynchron oper optim collect oper howev optim requir mpi standard eectiv emin real applic base experi span tree mpi program node three cluster node three cluster node contain mpi node respect thick edg network edg tmpi mpich figur collect commun span tree tmpi mpich adapt buffer manag pointto point commun pointtopoint commun tmpi bear lot similar mpich design conceptu mpi node receiv queue unexpectedmessag queue sender come correspond receiv send request handl deposit receiv unexpect messag queue similarli receiv come correspond sender receiv request handl store receiv queue pair sender receiv dierent cluster node daemon thread receiv side act behalf remot sender one dicult problem face design temporari buer messag correspond receiv readi yet intraclusternod pointtopoint com munic alway block sender till receiv come intern buer space avail how ever sender send messag remot receiv know whether sucient buer space hold messag messag size mpi could arbitrarili larg tradit conserv solut threephas asynchron messagepass protocol tmpi address space share fast synchron among thread lead us ecient adapt buer solut combin optimist eager push protocol threephas protocol basic sender need make guess base messag size whether transfer actual messag data send request metadata the eagerpush protocol send metadata first send data later receiv readi the threephas protocol remot daemon receiv side acknowledg whether sender made correct guess not figur show three simplifi case tmpi inter clusternod pointtopoint commun protocol provid detail descript follow note figur accommod case synchron readi send oper figur a sender send request actual data receiv side either receiv alreadi arriv still intern buer space avail daemon accept data store send request inform actual data proper queue notifi sender data accept wake receiv necessari sendersid dae mpi_bsendbuf big_siz type mpi_recvbuf big_siz type mpi_bsendbuf big_siz type mpi_recvbuf big_siz type figur possibl deadlock without daemon thread a success eagerpush reqdat r got dat b fail eagerpush degrad threephas protocol reqdat got req receiv readi dat r got dat r c threephas protocol got req receiv readi dat r got dat r sender daemon q msg queue r receiv network transact msg queue op receiv arrivalwakeup network boundari figur pointtopoint commun node dierent cluster node mon receiv confirm free data behalf sender figur b sender still send data request time receiversid daemon cannot accept incom data daemon receiv discard data store request metadata unexpectedmessag queue later on receiv arriv discov partial complet send request ask associ data sender sendersid daemon send data receiv side receiversid daemon receiv data save data receiversuppli user buer tell sender data receiv subsequ sendersid daemon dealloc buer upon receiv acknowledg note actual data transfer twice case figur c sender decid send request part data part separ whole flow essenti figur except actual data transfer onc design allow optim perform sender make correct guess function correctli degrad perform guess wrong remain issu decid sender switch eagerpush threephas protocol decis complic fact intern buer space tmpi share mpi node cluster node aggreg among seri request so common intuit choos eagerpush protocol often possibl could still prefer buer request even though buer space avail abl amount buer space might use hold data multipl subsequ request like tradit nonpreempt schedul problem favor short request algorithm could suboptim due lack futur knowledg current implement tmpi sender send data request messag size static defin threshold receiv side receiv daemon greedili buer incom data whenev possibl current threshold set kbyte base empir studi final allow receiv daemon send messag could result deadlock condit design care tmpi netd layer support block nonblock send nonblock send mere put messag queue send daemon respons send messag and dealloc memori necessari receiv daemon alway use nonblock send discuss benefit address space share tmpi one benefit threadbas mpi implement potenti save data copi addressspac shar ing intraclusternod pointtopoint commun tmpi need one intermedi data copi mpich take two intermedi data copi share memori three intermedi data copi without share memori interclusternod pointto point commun sinc daemon either sender receiversid access sender receiverbu tmpi take zero two intermedi data copi mpich alway need three intermedi data copi ing addit data need move across process boundari tmpi data need transfer across process boundari three time mpich sinc two involv process must synchron transfer data one other mpich perform sensit os schedul multiprogram environ tmpi scalabl presenc singl receiv daemon thread handl incom messag potenti bottleneck term scalabl tmpi possibl configur multipl daemon incom connect partit among multipl daemon howev current compiletim paramet plan studi adapt choos number daemon runtim fu ture also creat instanc nonblock send daemon make respons messag send oper initi mpi node could benefici sudden surg outgo data want block sender thread that current small scale set none configur nece sari point tmpi design scalabl accommod larg cluster fat smp node experi section evalu eectiv propos optim techniqu thread mpi implement versu processbas implement choos mpich refer processbas implement due wide accept emphas experi meant illustr whether flaw mpich design instead discuss previou sec tion want show potenti advantag thread mpi execut smp cluster environ fact optim techniqu tmpi possibl multithread mpi implement implement prototyp system linux smp cluster includ mpi function mpi standard list appendix a yet support heterogen architectur support userdefin data type layout mpich version use contain function mpi standard provid partial support mpi standard howev ad function rel independ task aect experiment result experi conduct cluster six quad xeon mhz smp gb main memori cluster node two fast ethernet card connect lucent canjun switch oper system redhat linux run linux kernel version channel bond enabl microbenchmarkperform point topoint commun section use microbenchmark make finegrain analysi pointtopoint commun subsystem tmpi mpich pingpong test first use pingpong benchmark access perform pointtopoint commun vari data size byte megabyt common practic choos dierent metric small larg messag messag size smaller note mpich detect whether underli system homogen startup time data convers overhead incur is channelbond allow singl tcp connect util multipl network card could improv network bandwidth help reduc network delay case achiev raw bandwidth mbp pair cluster node kilobyt report perform term roundtrip delay messag lager kilobyt report transferr defin total amount byte sentreceiv divid roundtrip time round trip time s a pingpong short messag tmpi mpich transfer rate b pingpong long messag tmpi mpich figur interclusternod pingp perform figur show pingpong perform two mpi node dierent cluster node see messag size small tmpi perform slightli better mpich except messag small case tmpi save interprocess data transfer overhead such system call context switch becom evid messag size becom larger tmpi constant mb bandwidth advantag mpich due save data copi round trip time s a pingpong short messag tmpi transfer rate b pingpong long messag tmpi figur intraclusternod pingpong perform two version mpich use mpich share memori mpich mpich without share memori access impact share memori thread processbas mpi system figur show pingpong perform two mpi node cluster node compar perform three mpi system tmpi mpich share memori mpich mpich without use share memori mpich evid ignor underli sharedmemori architectur yield much wors perform mpich compar two system tmpi advantag mpich mainli come save intermedi memori copi for long messag fast synchron among thread for short messag result tmpi perform nearli doubl mpich share memori note tmpi mpich perform drop reach peak transfer rate like caus underli memori content oneway messag pipelin second benchmark simpl oneway sendrecv test sender keep send messag receiv bench mark examin impact synchron among mpi node compar averag time take sender complet send oper short messag still use transfer rate metric larg messag messag size byte singl op time a oneway sendrecv short messag tmpi mpich messag size kb transfer rate b oneway sendrecv long messag tmpi mpich figur oneway sendrecv perform figur show oneway sendrecv perform two mpi node dierent cluster node theoret speak ing thread processbas mpi implement yield similar perform short messag averag time send oper equal singl trip time divid number outstand messag fli basic limit network bandwidth howev mention befor processbas mpi implement interclusternod messag pass need travel local daemon remot daemon time data need copi daemon local buer unless sender receiv daemon precis synchron either buer could full empti caus stall pipelin even though might still bandwidth avail seen figur mpich perform unstabl messag size small due diculti synchron among process messag size becom larg fewer outstand messag perform less sensit synchron addit also see tmpi s advantag mpich short messag mb bandwidth advantag larg messag due save extra data copi need processbas mpi system comparison repeat oneway sendrecv test two mpi node cluster node result shown figur compar among three mpi system notat figur expect mpich perform almost ident tmpi small messag slightli poorer tmpi larg messag intermedi daemon process along data path either system cost extra data copi mpich amort among multipl outstand request hand mpich show irregular behavior certain rang messag size note figur a use dierent scale accommod perform curv mpich messag size byte singl send oper take ms complet messag size goe beyond op time tmpi messag size byte a oneway sendrecv short messag tmpi op time transfer rate b oneway sendrecv long messag tmpi figur intraclusternod oneway sendrecv perform two version mpich use mpich share memori mpich mpich without share memori mpich that fall normal rang s s abl identifi exact sourc problem think might someth resourc content os level regardless glitch perform mpich much wors two mpi system due ignor underli share memori microbenchmark perform collect commun compar perform collect commun primit run three microbenchmark call mpi_reduc mpi_bcast mpi_allreduc number time respect compar averag time oper data volum involv benchmark small cost mainli come synchron run benchmark three differ set mpi node cluster node node scatter dierent cluster node node scatter mpi node mpi_bcast mpi_reduc use three dierent variat regard root node dierent iter test alway stay same rotat among mpi node rotat repeat use root coupl time root shift combo number conclus drawn experi result shown figur case mpich perform worst among three mpi system except case mpich mpich perform almost same signifi import take advantag share memori mpi system smp cluster environ tmpi time faster mpi_bcast time faster mpi_reduc mpich exploit share memori within smp among factor includ address space share hierarchi awar algorithm perform gain mainli come separ collect pointtopoint commun channel theoret speak mpi_bcast mpi_reduc oper alway faster mpi_allreduc oper howev unit s mpi reduc mpi bcast mpi allreduc node distr root tmpi mpich mpich tmpi mpich mpich tmpi mpich mpich rotat rotat rotat combo figur collect commun perform number shown tabl averag time s oper run benchmark three mpi system tmpi mpich share memori mpich mpich without share memori mpich mpi reduc mpi bcast test case root alway same root rotat among mpi node rotat fix root number time rotat combo notat a b use node distribut mean use b cluster node mpi node experi show case mpich mpi_bcast mpi_reduc oper perform wors mpi_allreduc oper anomali caus mpich design collect commun top pointtopoint commun messag collect commun still go daemon get store messag queue match travers queue mani outstand request cost queue oper becom expens due content daemon could becom bottleneck mpi_allreduc test suer problem mpi node synchron one outstand request hand separ commun channel pointtopoint collect com munic tmpi show anomali tmpi same root test perform much better rotat root test case mean tmpi take better advantag messag pipelin due address space share elimin intermedi daemon figur also evid advantag hierarchi awar commun design exampl mpi_bcast test perform roughli equal summat case case sinc tmpi broadcast case broadcast follow broadcast individu cluster node hand without use twolevel span tree mpich perform wors summat case case similar conclus also hold mpi_reduc mpi_allreduc macrobenchmark perform section use two applic kernel benchmark access eectiv tmpi optim kernel benchmark use matrixmultipl mm gaussianelimin ge perform computationintens linear algebra comput mm consist mostli mpi_bsend mpi_recv ge mpi_bcast run mm mpi node ge mpi node detail node distribut shown figur mm ge figur distribut mpi node b mean use b cluster node mpi node ensur number cluster node number mpi node cluster node decreas increas total number mpi node compar tmpi mpich use share memori smp perform result depict figur see number mpi node small tmpi mpich perform similarli howev number mpi node becom larg cluster node involv tmpi show better scalabl mpich mm tmpi constant mflop advantag mpich mainli come save intermedi copi ge neither system keep linear speedup mpich perform even degrad reach peak mpi node tmpi outperform mpich cluster node involv inde verifi advantag tmpi smp cluster environ reason tmpi gain much advantag ge case compar mm case ge call mpi broadcast function mm use point topoint commun demonstr previou section tmpi outperform mpich substanti collect commun conclud remark paper present design implement threadbas mpi system smp cluster contribut includ novel network devic abstract interfac tailor thread mpi execut smp cluster hierarchyawar commun propos number optim techniqu includ separ number mpi node mflop a matrix multipl tmpi mpich number mpi node mflop b gaussian elimin tmpi mpich figur macrobenchmark perform pointtopoint collect commun channel adapt buer eventdriven synchron take advantag multithread micro macro benchmark test experi show tmpi outperform mpich substanti note tmpi optim target class c program mpich design gener mpi program experi confirm even cluster environ internod network latenc rel high exploit threadbas mpi execut smp deliv substanti perform gain global commun fast lightweight syn chroniz experi focu dedic cluster futur work studi perform multiprogram environ threadbas synchron may achiev perform gain acknowledg work support part nsf acir acir ccr would like thank lingkun chu help cluster administr would also like thank kai shen anonym refere valuabl comment r case network workstat now beowulf parallel workstat scientif comput lam open cluster environ mpi parallel comput architectur hardwaresoftwar approach tpvm distribut concurr comput lightweight process abstract devic definit support implement highlevel pointtopoint messag pass interfac deliv network perform numer applic mpi collect commun oper cluster wide area system mpifm higher perform mpi workstat cluster httpwwwlscndedulam forum yearyear httpwww mpisim use parallel simul evalu mpi program adapt twolevel thread manag fast mpi execut share memori machin optim mpi collect cluster largescal smp proceed acmiee supercomput program transform runtim support thread mpi execut share memori machin httpnowcsberkeleyedufastcommmpi lpvm step toward multithread pvm tr highperform portabl implement mpi messag pass interfac standard mpifm mpisim magpi optim mpi collect cluster largescal smp adapt twolevel thread manag fast mpi execut share memori machin program transform runtim support thread mpi execut sharedmemori machin mpistart multiprotocol activ messag cluster smp parallel comput architectur ctr jian ke martin burtscher evan speight runtim compress mpi messan improv perform scalabl parallel applic proceed acmiee confer supercomput p novemb rohit fernand keshav pingali paul stodghil mobil mpi program comput grid proceed eleventh acm sigplan symposium principl practic parallel program march new york new york usa lingkun chu hong tang tao yang kai shen optim data aggreg clusterbas internet servic acm sigplan notic v n octob weirong zhu yanwei niu guang r gao perform portabl earth case studi across sever parallel architectur cluster comput v n p june