t geometr approach leverag weak learner a adaboost popular effect leverag procedur improv hypothes gener weak learn algorithm adaboost mani leverag algorithm view perform constrain gradient descent potenti function iter distribut sampl given weak learner proport direct steepest descent introduc new leverag algorithm base natur potenti function potenti function direct steepest descent neg compon therefor provid two techniqu obtain suitabl distribut direct steepest descent result algorithm bound incompar adaboost analysi suggest algorithm like perform better adaboost noisi data weak learner return low confid hypothes modest experi confirm algorithm perform better adaboost situat b introduct algorithm like adaboost abl improv hypothes gener weak learn method great potenti practic benefit call algorithm leverag algorithm leverag weak learn method exampl leverag algorithm includ bag arcx logitboost one class leverag algorithm follow follow templat construct master hypothes given sampl leverag algorithm begin default master hypothesi h construct distribut sampl as function sampl current master hypothesi h possibl t train weak learner use distribut sampl obtain weak hypothesi h pick ff creat new master hypothesi author support nsf grant ccr essenti arc paradigm introduc breiman skeleton adaboost boostbyresampl algorithm although leverag algorithm includ arc algorithm follow templat leverag algorithm gener section introduc geolev algorithm chang exampl sampl well distribut them paper consid class classif problem g howev follow schapir singer allow weak learner hypothes confid rate map domain x real num ber sign number give predict label magnitud measur confid master hypothes produc templat interpret way although underli goal produc hypothes gener well focu quickli leverag algorithm decreas sampl error varieti result bound gener error term perform sampl given sampl margin hypothesi h instanc x hx margin h entir sampl vector hypothesi correctli label sampl margin vector whose compon posit focus margin vector provid geometr intuit leverag problem particular potenti function margin space use guid choic ff distribut direct steepest descent ff valu minim potenti h leverag algorithm view way perform feasibl direct descent potenti function amort analysi use potenti function often use bound number iter requir achiev zero sampl error potenti function give insight strength weak variou leverag algorithm boost algorithm properti convert weak pac learn algorithm strong pac learn algorithm although theori behind adaboost algorithm eleg lead somewhat intrigu result minim normal factor distribut reduc train error search better understand adaboost reduc sampl error led geometr algorithm geolev geoarc although perform bound algorithm poor show boost properti bound incompar adaboost better weak hypothes contain mostli lowconfid predict main contribut paper follow use natur potenti function deriv new algorithm leverag learner call geolev for geometr leverag algorithm highlight relationship adaboost arc feasibl direct linear program use geometr interpret prove converg bound algorithm geolev bound number iter taken geolev achiev ffl classif error train set provid gener transform geolev arc algorithm geoarc bound hold summar preliminari experi geolev geoarc motiv novel algorithm geolev consid geometri margin space sinc mani empir analyt result show good margin sampl lead small gener error natur seek master hypothesi larg margin one heurist seek margin vector uniformli larg margin ie vector parallel indic master hypothesi correct equal confid everi instanc sampl geolev algorithm exploit heurist attempt find hypothes whose margin vector close possibl direct focu singl iter leverag process drop time subscript margin vector print bold face often normal euclidean length one thu h margin vector master hypothesi h whose th compon let goal vector p m normal length one recal sampl size margin vector lie normal margin vector lie dimension unit sphere note easi rescal confid multipli predict hypothesi h constant chang direct hs margin vector therefor assum appropri normal without loss gener first decis taken leverag distribut place sampl sinc distribut compon also view nonneg vector situat marginspac start iter shown figur order decreas angl h g must move head h toward g vector angl goal vector g lie cone normal lie rim shown figur h weak hypothesiss margin vector which need unit length parallel h tangent rim addit h h decreas angl g hand line h cut cone angl goal vector g reduc ad multipl h h time angl g cannot decreas h vector lie plane p tangent cone contain vector h shown figur theta fig situat margin space start iter weak learner learn all hypothesi h better random guess learner edg e id y hx posit mean delta h posit distribut view margin vector perpendicular plane p h lie p therefor leverag abl use h reduc angl h g suggest figur appropri direct gener neither jjdjj compon posit normal yield distribut sampl weak learner howev possibl compon neg case thing complic compon neg flip sign compon sign correspond label sampl creat new direct normal distribut new sampl x s possibl new label modifi sampl distribut use gener new weak hypothesi h let h margin h modifi sampl h fact complic differenti geolev arc algorithm arc algorithm permit chang sampl way second transform avoid label flip discuss section fig direct distribut use geolev sign flip cancel second decis taken algorithm incorpor weak hypothesi h master hypothesi h weak hypothesi edg distribut describ use decreas goal find coeffici ff h jjhffhjj decreas angl much possibl take deriv show minim discuss see geolev perform kind gradient descent consid angl g current h potenti margin space direct steepest descent move direct approxim gradient take us toward goal vector sinc littl control hypothes return weak learner approxim direct best do step size chosen adapt make much use weak hypothesi possibl geolev algorithm summar figur relat previou work breiman defin arc algorithm use potenti function express componentwis function margin form breiman allow componentwis potenti f depend sum ff s arc algorithm input sampl weak learn algorithm initi master hypothesi h predict everywher m repeat add els add call weak learner distribut obtain hypothesi h fig geolev algorithm breiman show that certain condit f arc algorithm converg good hypothes limit furthermor show adaboost arc algorithm arc algorithm polynomi fx complet describ adaboost algorithm show notat perform feasibl direct gradient descent potenti function adaboost fit templat outlin introduct choos distribut z z normal factor sum master hypothesi updat ad multipl new weak hypothesi coeffici ff chosen minim exp next iter z valu unlik geoboost margin vector adaboost hypothes normal show adaboost view minim potenti approxim gradient descent direct steepest descent wrt compon margin vector proport distribut adaboost give weak learner continu analog coeffici ff given new hypothesi minim potenti x updat master hypothesi ident thu adaboost behavior approxim gradient descent function defin direct descent weak learner hypothesi furthermor bound adaboost perform proven schapir singer implicitli perform amort analysi potenti function arcx also fit templat outlin introduct keep unnorm master hypothesi notat distribut chosen trial proport algorithm also view gradient descent potenti function th iter rather comput coeffici ff function weak hypothesi arcx alway choos ff thu h weight t master hypothesi mani gradient descent method unfortun depend potenti function make difficult use amort analysi connect gradient descent hint freund note breiman other interpret gener previou work relax constraint potenti function particular show construct algorithm potenti function direct steepest descent neg compon potenti function view leverag algorithm show relationship feasibl descent linear program relationship provid insight role weak learner feasibl direct method tri move direct steepest descent howev must remain feasibl region describ constraint descent direct chosen closest neg gradient gammarf satisfi constraint exampl simplifi zoutendijk method chosen direct satisfi constraint maxim gammarf delta similarli leverag algorithm discuss constrain produc master hypothes lie span weak learner hypothesi class one view role weak learner find feasibl direct close given distribut or neg gradient fact weak learn assumpt use boost analysi geolev impli alway feasibl direct gammarf delta bound zero gradient descent framework outlin provid method deriv correspond leverag algorithm smooth potenti function margin space potenti function use adaboost arcx advantag compon gradient posit thu easi convert distribut hand method outlin previou section section use handl gradient neg compon approach use ratsch et al similarli interpret potenti function margin recent friedman et al given maximum likelihood motiv adaboost introduc anoth leverag algorithm base loglikelihood criteria indic minim squar loss potenti perform less well experi monoton potenti conjectur nonmonoton penal margin greater contribut factor method describ section may provid way amelior problem converg bound section examin number iter requir geolev achiev classif error ffl sampl key step show sine angl goal vector g master hypothesi h reduc iter upper bound result recurr give bound rapidli train error decreas begin consid singl boost iter margin space quantiti previous defin recal g h norm h not addit let h denot new master hypothesi end iter angl h g assum throughout sampl finit d delta h edg weak learner hypothesi h respect distribut given weak learner bound decreas depend h r jjhjj note r chosen maintain consist work schapir singer start iter end iter sin recal h h ffh normal sinc h alreadi unit length lemma valu co maxim and sin minim g proof lemma follow examin first second deriv co respect ff use valu ff littl algebra show although desir bound hold h find conveni first minim respect h delta h remain depend h express function r jjhjj final bound lemma equat minim h proof lemma follow examin first second deriv respect h delta h consider simplifi yield d recal d therefor bound two way use differ bound jjdjj first bound deriv note jjdjj jjdjj recal combin bound jjdjj yield sin jjhjj repeat applic bound yield follow theorem theorem r edg weak learner hypothes first iter sine angl g margin vector master hypothesi comput iter bound jjdjj anoth way obtain bound often better note jjdjj d delta substitut continu yield sin continu result follow theorem theorem let r edg weak learner hypothes angl g margin master hypothes start first iter t angl g margin master hypothesi produc iter relat result sampl error use follow lemma lemma sin angl g master hypothesi h sampl error h less ffl proof assum sin rm norm hold h posit compon therefor master hypothesi correctli classifi exampl sampl error rate r gamma m combin lemma theorem give follow corollari corollari iter sampl error rate geolev master hypothesi bound recurr theorem somewhat difficult analyz appli follow lemma abe et al lemma consid sequenc fg g nonneg number satisfi g t posit constant f c n given lower bound r r valu upper bound h jjh jj appli lemma recurr set h show sin thi previou result lead follow theorem theorem weak learner alway return hypothes edg greater r h upper bound jjh jj geolev hypothesi ffl train error iter similar bound obtain freund schapir adaboost theorem iter sampl error rate adaboost master hypothesi depend jjhjj implicit bound remov h compar corollari theorem lead follow observ first bound geolev contain squareroot differ would correspond halv number iter requir reach error rate ffl sampl effect approxim factor r term import differ factor multipli r term preced approxim geolev bound sin jjh jj aboost bound jjh jj larger factor better bound depend sin mean geolev progress taper approach zero sampl error weak hypothes equal confid exampl jjh jj time larger jjh jj differ factor simpli sin start boost process close geolev factor larger howev sin small m geolev predict perfectli sampl thu geolev seem gain much later iter difficulti prevent us show geolev boost algorithm hand consid less like situat weak hypothes produc confid predict one sampl point abstain rest jjh jj geolev bound extra factor sin geolev bound uniformli better adaboost case convers arc algorithm geolev algorithm discuss far fit templat arc algorithm modifi label sampl given weak learner must switch recurr rather recurr sin small also break boost paradigm weak learner may requir produc good hypothesi data consist concept underli concept class section describ gener convers produc arc algorithm leverag algorithm kind without place addit burden weak learner throughout section assum weak learner hypothes produc valu gamma convers introduc wrapper weak learner leverag algorithm replac signflip trick section wrapper take weight leverag algorithm creat distribut set neg compon zero renorm modifi distribut given weak learner return hypothesi h margin vector h margin vector modifi wrapper pass leverag algorithm dx neg h set gamma thu leverag algorithm see modifi margin vector h use comput ff margin new master hypothesi intuit leverag algorithm fool think weak hypothesi wrong part sampl actual correct therefor margin master hypothesi actual better track leverag algorithm furthermor appar edg weak learner increas wrap transform intuit formal follow theorem theorem edg weak learner respect distribut see r edg modifi weak hypothesi respect sign weight request leverag algorithm r r proof ensur otherwis assumpt h impli r r minim theorem compon master margin vector use wrap leverag algorithm ever greater actual margin master hypothesi proof theorem follow immedi note compon h greater correspond compon h call wrap version geolev geoarc arc algorithm instruct examin potenti function associ geoarc ip min potenti similar form follow potenti function zero entir posit orthant ip leverag framework describ togeth transform enabl analysi undifferenti potenti function full implic remain explor preliminari experi perform experi compar geolev geoarc adaboost set datasetsth class one use previou experi uci repositori experi run along line report quinlan ran cross valid dataset two class classif leverag algorithm ran iter use singl node decis tree implement mlc weak hypothes note sigma valu hypothes larg norm notic split criterion use singl node larg impact result therefor result report dataset better mutual inform ratio gain ratio report comparison adaboost geolev geoarc perform compar geolev result illustr figur figur scatter plot gener error dataset result appear indic new algorithm compar adaboost experi clearli warrant especi interest situat weak learner produc hypothes small norm conclus direct studi present geolev geoarc algorithm attempt form master hypothes correct equal confid sampl found conveni view algorithm perform feasibl direct gradient descent constrain hypothes produc weak learner potenti function use geolev monoton gradient neg compon therefor direct steepest descent cannot simpli normal creat distribut weak learner describ two way solv problem first construct modifi sampl flip label solut mildli unsatisfi strengthen requir weak learner weak learner must deal broader class possibl target therefor also present second transform increas requir weak learner fact use second transform actual improv effici geolev adaboost fig gener error geolev versu adaboost round leverag algorithm one open issu whether improv exploit improv geoarc perform bound second open issu determin effect transform appli nonmonoton potenti function consid mason et al upper bound sampl error rate master hypothes produc geolev geoarc algorithm bound incompar analog bound adaboost bound indic ge olevgeoarc may perform slightli better start leverag process weak hypothes contain mani lowconfid predict hand bound indic geolevgeoarc may exploit later iter well may less effect weak learner produc valu hypothes disadvantag make unlik geoarc algorithm boost properti one possibl explan geolevgeoarc aim cone inscrib posit orthant margin space sampl size grow dimens space increas volum cone becom diminish fraction posit orthant adaboost potenti function appear better navig corner posit orthant howev preliminari test indic iter gener error geoarcgeolev similar adaboost classif dataset uci repositori comparison use node decis tree classifi weak learn method would interest compar rel perform use weak learner produc hypothes mani lowconfid predict acknowledg would like thank manfr warmuthrobert schapir yoav freund arun jagota claudio gentil eurocolt program committe use comment preliminari version paper r polynomi learnabl probabilist concept respect kullbackleibl diverg train algorithm optim margin classifi bag predictor arc edg boost weak learn algorithm major decisiontheoret gener onlin learn applic boost addit logist re gression statist view boost data mine use mlc improv gener explicit optim margin bag boost c margin adaboost boost margin new explan effect vote method improv boost algorithm use confidencer predict estim depend base empir data tr theori learnabl size net give valid gener polynomi learnabl probabilist concept respect kullbackleibl diverg equival model polynomi learnabl train algorithm optim margin classifi design analysi effici learn algorithm learn boolean formula introduct comput learn theori boost weak learn algorithm major bag predictor exponenti gradient versu gradient descent linear predictor decisiontheoret gener onlin learn applic boost gener converg result linear discrimin updat adapt version boost major algorithm drift game addit model boost infer gener diverg boost entropi project predict game arc algorithm improv boost algorithm use confidencer predict empir comparison vote classif algorithm margin distribut bound gener