t rel loss bound onlin densiti estim exponenti famili distribut a consid onlin densiti estim parameter densiti exponenti famili onlin algorithm receiv one exampl time maintain paramet essenti averag past exampl receiv exampl algorithm incur loss neg loglikelihood exampl respect current paramet algorithm offlin algorithm choos best paramet base exampl prove bound addit total loss onlin algorithm total loss best offlin paramet rel loss bound hold arbitrari sequenc exampl goal design algorithm best possibl rel loss bound use bregman diverg deriv analyz algorithm diverg rel entropi two exponenti distribut also use method prove rel loss bound linear regress b introduct main focu statist decis theori consist follow receiv statist inform form sampl data goal make decis minim loss expect loss respect underli distribut assum model data distribut often dene term certain paramet statist decis depend specic valu chosen paramet thu statist decis theori three import element paramet valu take decis loss function evalu decis extend abstract appear uai aw z support nsf grant ccr ccr c kluwer academ publish print netherland k s azouri m k warmuth bayesian statist decis theori prior distribut paramet data distribut addit import element inform need assess decis perform simpl case suppos given sampl data point fx also refer exampl assum exampl independ gener gaussian unknown mean known varianc one want nd paramet set here mean minim expect loss new exampl drawn data distribut bayesian framework prior distribut mean would also enter decis make process context learn theori setup bayesian not would describ batch olin learn model sinc exampl given learner ahead time decis made base inform entir data set paper focu onlin learn model olin learn fundament element paramet deci sion loss function dierenc howev exampl given learner one time thu onlin learn natur partit trial trial one exampl process trial proce follow begin current paramet set hypothesi next exampl present learner onlin algorithm incur loss loss function recent exampl current paramet set final algorithm updat paramet set new trial begin context onlin learn decis paramet updat learner goal design onlin learn algorithm good bound total loss clearli cannot meaning bound total loss onlin algorithm stand alon also hold arbitrari sequenc exampl howev onlin learn literatur certain type rel loss bound desir use success term rel mean comparison best paramet chosen olin see whole batch exampl paramet space call comparison class rel loss bound quantifi addit total loss onlin algorithm total loss best o line paramet compar sinc onlin learner see sequenc exampl advanc addit loss sometim call regret price hide futur exampl learner paper design motiv onlin algorithm paramet updat util avail inform best possibl manner lead good rel loss bound focu rel loss bound two type learn problem onlin densiti estim onlin regress densiti estim onlin algorithm receiv sequenc unlabel exampl data vector fx g start learner current paramet set use predict next exampl x make predic tion algorithm receiv exampl x incur loss l algorithm updat paramet set t con trast onlin regress problem receiv label sequenc exampl call instanc label trial t learner start current paramet receiv instanc x learner make predict label predict depend x loss l incur paramet updat t type problem densiti estim regress use abbrevi notat l denot loss incur trial t subscript indic depend exampl present trial t total loss onlin algorithm total loss best olin batch paramet b charg size b rel loss bound give upper bound dierenc two total loss prove rel loss bound densiti estim underli model member exponenti famili distribut also onlin linear regress consid two algorithm rst algorithm call increment olin algorithm predict ie choos paramet best olin algorithm would predict base exampl seen far second algorithm call forward algorithm algorithm call forward use guess futur exampl correspond futur loss form predict motiv vovk work linear regress rel loss bound algorithm grow logarithm number trial motiv paramet updat bayesian probabilist interpret howev rel loss bound prove hold arbitrari or worstcas sequenc ex ampl key element design analysi onlin learn algorithm gener notion distanc call bregman diver genc diverg interpret rel entropi two exponenti distribut k s azouri m k warmuth outlin rest paper organ follow section present brief overview previou work section dene bregman diverg give relev background exponenti famili distribut show rel entropi two exponenti distribut ama special bregman diverg conclud section list basic properti bregman diverg section introduc increment olin algorithm gener set appli algorithm problem densiti estim exponenti famili linear regress give number rel loss bound specic exampl section dene motiv forward algorithm seen gener increment olin algorithm again appli algorithm densiti estim exponenti famili case linear regress reprov rel loss bound obtain vovk vov proof concis altern simpl proof forward linear regress algorithm given for section brie discuss altern method develop vovk prove rel loss bound use integr gener posterior discuss advantag method final conclud section discuss number open problem overview previou work method prove bound addit total loss onlin algorithm total loss best paramet comparison class essenti goe back work blackwel bla hannnan han investig bound context game theori comparison class consist mixtur strategi later cover cov prove bound context mathemat nanc use comparison class constant rebalanc portfolio research paper root studi rel loss bound onlin learn algorithm comput learn theori commun even though bound may underestim perform natur data use power yardstick analyz compar onlin algorithm comput learn theori commun line research initi littleston discoveri winnow algorithm lit littleston also pioneer style amort analysi rel loss bound prove rel loss bound use certain diverg function potenti function winnow design disjunct comparison class total number mistak use loss next wave onlin algorithm design nite set expert comparison class wide rang loss function algorithm develop onlin linear least squar regress ie comparison class consist linear neuron linear combin ex pert llw cblw kw work gener case comparison class set sigmoid linear neuron hkw kw also start littleston work rel loss bound comparison class linear threshold function investig lit gl onlin algorithm cite previou paragraph use xed learn rate simpl set rel loss bound grow number trial howev alreadi linear regress squar loss rel loss bound algorithm xed learn rate grow squar root loss best linear predictor cblw kw best loss often linear number trial contrast algorithm use variabl learn rate rel loss bound grow logarithm bound proven gener bay algorithm xb yam maintain posterior paramet comparison class outlin method prove rel loss bound section bound grow logarithm also proven previous onlin linear regress import insight gain research olog rel loss bound seem requir use variabl learn rate paper learn rate appli trial ot use ot learn rate exponenti famili also suggest gordon possibl strategi lead better bound howev specic exampl work out case linear regress ot learn rate becom invers covari matrix past exampl gener framework onlin learn algorithm develop gl kw kw gor follow philosophi kivinen warmuth kw start diverg function diverg function deriv onlin updat use diverg potenti amort analysi similar method develop gl case comparison class consist linear threshold function start updat construct appropri diverg use analysi k s azouri m k warmuth recent learn diverg use onlin learn employ extens convex optim call bregmandist bre cl csi jb bregman method pick set allow model one minim distanc current model word current hypothesi project onto convex set allow model mild addit assumpt assur uniqu project assumpt gener pythagorean theorem proven bregman diverg hw latter theorem often contradict triangular inequ reason use term diverg instead distanc project respect bregman diverg recent appli hw case olin compar allow shift time project use keep paramet algorithm reason region aid recoveri process underli model shift bregman diverg exponenti famili paper use notion diverg due bregman bre deriv analyz onlin learn algorithm diverg interpret rel entropi distribut exponenti famili begin section dene bregman diverg review import featur exponenti famili distribut relev paper conclud section properti diverg arbitrari realvalu convex dierenti function g paramet space r bregman diverg two paramet e dene r denot gradient respect throughout paper vector column vector use denot dot product vector note bregman diverg g e minu rst two term taylor expans g e around word g e tail taylor expans g e beyond linear term sinc g convex g e properti list section exampl let paramet space case bregman diverg becom squar rel loss bound euclidean distanc ie also e e e e exponenti famili featur exponenti famili use throughout paper includ measur diverg two member famili intrins dualiti relationship see bn ama comprehens treatment exponenti famili multivari parametr famili fg distribut said exponenti famili member densiti function x vector r p x repres factor densiti depend ddimension paramet usual call natur or canon paramet mani common parametr distribut member famili includ gaussian function g normal factor dene z space r integr nite call natur paramet space exponenti famili call regular open subset r well known bn ama convex set g strictli convex function function g call cumul function play fundament role character member famili distribut use g denot gradient r g r g denot hessian g let repres loglikelihood view function standard regular condit loglikelihood function satisfi wellknown moment ident mn appli ident exponenti famili reveal special role play cumul function g k s azouri m k warmuth expect respect distribut pg xj rst moment ident loglikelihood function gradient loglikelihood linear x r g appli get show mean x equal gradient g let call expect paramet sinc cumul function g strictli convex map invers denot imag map g invers map g set call expect space may necessarili convex set second moment ident loglikelihood function denot transpos exponenti famili r thu second moment ident variancecovari matrix x hessian cumul function g also call fisher inform matrix sinc g strictli convex hessian symmetr posit denit dualiti natur paramet expect paramet sometim conveni parameter distribut exponenti famili use expect paramet instead natur paramet pair parameter dual relationship provid aspect dualiti relev paper first dene second function rang g follow let f rf denot gradient f rel loss bound note take gradient f respect treat function get r jacobian respect thu f invers map g two parameter relat follow transform sinc g posit denit jacobian g posit denit jacobian m thu second function f strictli convex well function call dual g ama furthermor f neg entropi pg xj respect refer measur p x ie follow hessian f invers fisher inform matrix ie r consid function v gf fisher inform matrix express term expect paramet function dene expect space m take valu space symmetr matric call varianc function varianc function play import role character member exponenti famili mor gp matrix v posit denit m v thu context exponenti famili function f g arbitrari convex function must posit denit hessian diverg two exponenti distribut consid two distribut pg xj e old paramet set e pg xj new paramet set follow amari ama one may see exponenti famili fg manifold paramet e repres two point manifold sever measur distanc diverg two point propos literatur amari introduc diverg ama relat distanc introduc csiszar csi known f diverg we use letter h below also cherno distanc renyi inform relat ama diverg k s azouri m k warmuth follow gener form h continu convex function main choic h give rel entropi anoth interest choic give opposit two entropi are respect call diverg amari ama properti diverg section give simpl properti diverg properti need g posit denit hessian henc properti hold gener denit bregman diverg allow g arbitrari realvalu dierenti convex function g paramet space throughout paper use repres r g g e convex rst argument sinc g e convex g strictli convex equal hold e gradient diverg respect rst argument follow simpl form r e diverg usual symmetr ie g e diverg linear oper ie ag rel loss bound diverg aect ad linear term g g g dot product usual sign neg contradict triangular inequ case dot product zero exploit proof gener pythagorean theorem bregman diverg see exampl g strictli convex denit dual convex function f paramet transform still hold bregman diverg follow dualiti properti rst six properti immedi properti proven appendix properti rst use wj prove rel loss bound last properti follow denit dual function f also call convex conjug roc note order argument g e switch paper need properti case g strictli con vex howev realvalu dierenti convex function g one dene dual function f paramet r roc denit properti still hold note gordon gor give eleg gener bregman diverg case convex function g necessarili dierenti sake simplic restrict dierenti case paper final g dierenti bregman diverg also written path integr integr version diverg use dene notion convex loss match increas transfer function g artici neuron ahw hkw kw k s azouri m k warmuth increment olin algorithm section give basic algorithm show prove rel loss bound gener set learn proce trial exampl process densiti estim exampl data vector x domain x regress set tth exampl consist instanc x instanc domain x label label domain y setup learn problem dene three part paramet space r realvalu loss function diverg function measur distanc initi paramet set paramet space repres model algorithm compar loss paramet vector th exampl denot l l t shorthand usual loss nonneg third compon setup initi parmet bregman diverg u initi pa ramet initi paramet may interpret summari prior learn diverg u repres measur distanc initi paramet olin or batch algorithm see exampl set paramet u t assumpt loss l for u dier entiabl convex function paramet space real furthermor assum argmin u t alway solut note olin algorithm trade total loss exampl close origin paramet altern diverg u may interpret size paramet interpret olin algorithm nd paramet minim sum size total loss onlin algorithm see one exampl time accord follow protocol onlin protocol increment olin algorithm initi hypothesi predict get tth exampl incur loss l updat hypothesi t rel loss bound goal onlin algorithm incur loss never much larger loss olin algorithm see exampl onc end trial onlin algorithm know rst exampl expect see next exampl one reason desir setup paramet updat point make onlin algorithm exactli olin algorithm would done see exampl use name increment olin onlin algorithm properti increment olin algorithm u t addit assumpt assum argmin u t for alway solut one solut interpret t argmin u t learn problem use exampl paper u t typic strictli convex one solut note nal paramet t increment olin algorithm coincid paramet b chosen batch algorithm consist protocol given abov necessari here begin index parallel index second onlin algorithm given next section second algorithm call forward algorithm use guess next loss updat paramet setup updat seem truli onlin sinc need previou exampl truli onlin yet equival setup given follow lemma lemma increment olin algorithm proof note sinc ru thu sinc u constant argmin t use denit argmin lemma qed readi show key lemma increment o line algorithm lemma compar total loss onlin algorithm total loss compar total loss compar includ diverg term u k s azouri m k warmuth lemma increment olin algorithm sequenc exampl u t proof expand diverg u t t use ru t t give us u t t sinc u t special case subtract appli u u a version give sum trial obtain u lemma follow equal u equal follow properti linear qed obtain rel loss bound choos best olin paramet b compar bound righthandsid equat lemma note case increment olin algorithm thu last diverg righthandsid zero diverg u t repres cost updat t incur onlin algorithm rel loss bound bound total cost onlin updat rel loss bound increment offlin algorithm exponenti famili appli increment olin algorithm problem densiti estim exponenti famili distribut rst give gener treatment prove rel loss bound specic member famili subsect follow make obviou choic loss function name neg loglikelihood use gener form loglikelihood loss paramet exampl x purpos rel loss bound see lemma chang loss constant depend inconsequenti thu form refer measur p x immateri befor allow algorithm initi paramet valu choos u multipl cumul function ie thu context densiti estim exponenti famili increment olin algorithm becom u t throughout paper use notat denot tradeo paramet two reason first invers tradeo paramet becom learn rate algorithm learn rate commonli denot also use instead linear regress paramet gener matric setup interpret nding maximum aposteriori map paramet diverg term correspond conjug prior hyper paramet diverg term disappear maximum likelihood esti mation altern one think initi paramet estim base hypothet exampl seen rst real exampl number exampl also one interpret paramet tradeo paramet stay close initi paramet minim loss exampl seen end trial t yet anoth interpret follow rewrit u t k s azouri m k warmuth thu u t correspond neg loglikelihood exponenti densiti cumul function exampl develop altern onlin motiv given lemma let linear follow properti diverg u thu onlin motiv lemma becom diverg measur distanc last paramet tradeo paramet updat paramet t obtain minim u t as dene strictli convex function gradient function term expect paramet set zero give updat expect paramet increment olin algorithm also express t convex combin last instanc x note altern recurs form updat use later on for thu onlin updat may seen gradient descent differ learn rate updat use gradient loss t use gradient loss evalu special case valid consist updat rel loss bound rel loss bound proven use lemma thu densiti estim equal simpli to follow lemma give concis express minimum u t see term dual cumul function lemma follow discuss interest right although essenti main develop paper use bernoulli exampl discuss later combin lemma one also get express total loss onlin algorithm lemma aw lemma min proof rewrit righthandsid equal lemma use denit dual function f express rewrit use denit loss diverg equal u t t qed the case maximum likelihood rewritten as k s azouri m k warmuth thu essenti inmum averag loss data equal expect loss paramet minim averag loss ie maximum likelihood paramet relationship use gru remain subsect discuss specic exampl give rel loss bound densiti estim gaussian deriv rel loss bound gaussian densiti estim problem consid gaussian densiti r known xed without loss gener develop bound special case ident matrix similar bound immedi follow gener case xed arbitrari variancecovari matrix linear transform argument gaussian densiti ident matrix variancecovari matrix here x shorthand x x densiti member exponenti famili natur paramet cumul paramet transform function dual convex function f loss l const note constant loss immateri bound therefor set zero sake simplic set recal increment olin updat dierenc total loss increment olin algorithm olin algorithm rel loss bound use onlin updat eg rewrit diverg righthandsid want allow case updat cannot appli howev updat rewritten follow easi nd two exampl x x dierenc depend order two exampl present develop upper bound permut invari ie depend order exampl present drop neg term use sinc obtain follow rel loss bound k s azouri m k warmuth theorem gaussian densiti estim increment olin algorithm note special case olin algorithm choos maximum likelihood paramet bound simpli densiti estim gamma give rel loss bound gamma distribut densiti shape paramet invers scale paramet member exponenti famili natur paramet densiti term written assum known xed paramet scale loss diverg invers call dispers paramet sake simplic drop ignor xed varianc case gaussian densiti estim cumul paramet transform dual convex function f loss l bound diverg t lead rel loss bound increment olin algorithm see rel loss bound use updat notat r convex combin element fx g sum diverg righthandsid bound summari follow rel loss bound theorem densiti estim gamma distribut use increment olin algorithm better rel loss bound includ case possibl bound care densiti estim gener exponenti famili give brief discuss form bound take member exponenti famili rewrit diverg second order taylor expans f t last e e k s azouri m k warmuth e convex combin t r f constant essenti gaussian gener case may seen local gaussian timevari curvatur reason method proceed casebycas basi mor gp base form r f invers varianc function recal sum last term alway give logt style bound sometim rang x need restrict done previou subsect densiti estim gaussian gamma distribut linear regress subsect bound linear regress develop instanc domain x r label domain r paramet domain also r compon paramet vector linear weight given exampl paramet vector linear model predict x squar loss use measur discrep predict label exampl note l strictli convex thu make u strictli convex initi diverg u strictli convex updat alway uniqu solut use u posit denit matrix diverg initi paramet becom u thu linear regress updat increment olin updat becom u t note use transpos notat x q instead dot product subsequ deriv use matrix algebra setup linear regress usual interpret condit densiti estim problem gaussian label given x cumul function trial diverg correspond gaussian prior develop altern onlin version done gener lemma sinc u equal linear term onlin version becom rel loss bound dierenti obtain increment olin updat linear regress standard linear least squar updat easi deriv follow recurs version for note correspond updat updat densiti estim also lemma becom follow quadrat equat see correspond equat densiti reprov bound obtain vovk vov increment olin algorithm for sake simplic choos multipl ident matrix i similar bound proven foster algorithm howev assum compar probabl vector fo theorem linear regress increment olin algorithm tg note theorem assum predict x label trial lie y assumpt satis might use clip ie algorithm predict number yy closest x clip requir algorithm know k s azouri m k warmuth littl incent work detail increment o line algorithm algorithm next section prove better rel loss bound predict need lie y proof appli updat twice diverg sum righthandsid give rst two equal below third equal follow lemma a appendix t last inequ use assumpt x fact z ln z note last inequ may hold without assumpt x theorem follow appli crude approxim equal lemma last inequ follow assumpt x qi x second last inequ follow fact ai determin symmetr matrix product diagon element see bb chapter theorem qed ideal want use crude bound method goal rewrit sum diverg telescop occur increment onlin updat abl that partial attempt follow gaussian densiti rel loss bound estim t last equal use fact symmetr note last two term nal express telescop gaussian case surprisingli forward algorithm introduc next section correspond two term telescop thu forward algorithm one prove bound one given theorem except last term bound quarter theorem relat problem densiti estim gaussian correspond improv bound with factor hold increment olin algorithm estim futur loss forward algorithm section present second algorithm call forward algorithm give lemma use prove rel loss bound trial t forward algorithm expect see next exampl allow incorpor estim loss next exampl choos paramet regress part exampl name instanc x avail trial algorithm must commit paramet algorithm use instanc x form estim loss trial t shall see linear regress incorpor estim motiv use includ current instanc learn rate algorithm lead better rel loss bound densiti estim howev instanc yet algorithm still use estim futur loss k s azouri m k warmuth onlin protocol forward algorithm regress densiti estim initi hypothesi initi hypothesi get instanc x guess loss tth exampl guess loss tth exampl updat hypothesi updat hypothesi predict predict get label tth exampl get exampl x incur loss l incur loss l dene updat analog previou section minim sum diverg plu loss past trial estim loss next trial forward algorithm u t assumpt loss l for estim loss dierenti convex function paramet space real furthermor assum argmin u t for alway solut note increment olin algorithm special case forward algorithm estim loss zero altern onlin motiv updat use diverg last paramet vector lemma forward algorithm u rewrit argument argmin as rel loss bound thu sinc u constant argmin t use denit argmin lemma qed follow key lemma gener lemma increment olin algorithm lemma forward algorithm sequenc exampl u t proof expand diverg u t t use ru t t proof lemma give us u t t sinc u t obtain special case subtract appli u u a version obtain u t equat lemma follow sum trial subtract u side qed rel loss bound forward algorithm must base bound righthandsid lemma densiti estim exponenti famili appli forward algorithm problem densiti estim exponenti famili distribut choos u k s azouri m k warmuth g done increment olin algorithm thu initi diverg becom u estim futur loss use const may seen averag loss number exampl lie instanc domain seen guess futur instanc correspond loss estim loss rewritten g const thu choic forward algorithm becom follow for u t increment olin algorithm except tradeo paramet onlin motiv becom onlin motiv increment o line algorithm except increas one updat lemma remain learn rate shift updat hold rst one hold well show choic estim sinc estim loss independ trial estim loss last equal lemma cancel get rel loss bound densiti estim gaussian section give bound forward algorithm better correspond bound increment olin al gorithm follow step simplifi follow diverg use thi becom set and thu zero choos t thu last three term equat rewritten as equat bound x theorem gaussian densiti estim forward algorithm k s azouri m k warmuth note bound forward algorithm better bound increment olin algorithm see theorem improv essenti x densiti estim bernoulli subsect give rel loss bound bernoulli distribut exampl x coin ip f g distribut typic express p probabl let distribut term member exponenti famili natur paramet cumul paramet transform e dual function f loss l consid forward algorithm case maximum likelihood forward algorithm use t t rst develop concis express total loss algorithm lemma bernoulli densiti estim forward algorithm proof rst rewrit loss trial variou way let abbrevi develop formula note trial x increas one loss l contain rel loss bound term ln trial term contribut similarli trial x one loss l contain term lnt trial term contribut fact lemma follow qed note righthandsid express lemma independ order exampl seen thu bernoulli distribut total loss forward algorithm permut invari lemma l t thu equival express use gamma function rst deriv freund fre base laplac method integr use standard approxim gamma function one bound righthandsid theorem fre bernoulli densiti estim forward algorithm linear regress subsect deriv rel loss bound forward algorithm appli linear regress increment olin algorithm let u posit denit diverg initi u use estim futur loss ie next label t guess x forward updat linear regress becom k s azouri m k warmuth u t denit u minim thu dierenti u t obtain forward updat linear regress note t depend x t thu forward algorithm differ increment olin algorithm use current instanc form predict sake simplic assum rest section recurs version updat follow rewrit equal lemma linear regress follow step use increment olin algorithm rel loss bound use abov argument sum righthandsid equat simpli note last two term telescop correspond deriv increment olin updat last two term telescop gaussian densiti estim show last three term equat zero first rewrit b as ident matrix last term becom second last term simpli to x t x sum last three term pull factor k s azouri m k warmuth thu last three term zero gaussian densiti estim forward algorithm final use upper bound see theorem sum righthandsid bound summar rel loss bound prove forward algorithm theorem linear regress forward algorithm note bound forward algorithm better correspond bound increment olin algorithm see theorem improv factor four bound rst proven vovk vov use integr altern proof exact express given forster for relationship bay algorithm altern method pioneer vovk vov freund yamanishi yam prove rel loss bound section sketch method compar our distribut maintain set paramet paramet name algorithm call expert onlin learn literatur initi work consid case nite howev method also appli continu yam begin trial rel loss bound t distribut form r e p prior learn rate follow type inequ main part method z z z la t denot loss algorithm trial t import special case occur loss l q neg loglikelihood respect parameter densiti ie call bay algorithm case p given posterior distribut see rst exampl algorithm trial predict predict distribut equal dmw gener set when loss necessarili neg loglikelihood predict algorithm chosen inequ hold matter tth exampl be also larger learn rate better result rel loss bound learn rate chosen larg possibl predict alway guarante exist inequ hold learn rate use trial inequ often tight exampl lie boundari set possibl exampl sum inequ trial one get bound z integr cannot comput exactli bound use laplac method integr around best olin paramet would like point follow distinct method algorithm present paper predict bay algorithm ie predict distribut usual repres paramet or expert instead algorithm analyz paper choos map paramet trial case bernoulli distribut bay algorithm base jerey prior coincid forward k s azouri m k warmuth algorithm section similarli case linear regress gaussian prior vovk algorithm vov coincid forward algorithm linear regress section howev clear densiti estim problem exponenti famili predict bay algorithm or algorithm produc gener case repres paramet paramet space contrast method prove rel loss bound avoid use involv integr method need co paramet maintain base simpl averag past exampl sucient statist exponenti famili conclus paper present techniqu prove rel loss bound densiti estim exponenti famili gave number exampl appli method includ case linear regress exponenti densiti cumul function g use bregman diverg g e measur distanc paramet e thu loss l diverg base function g howev lemma methodolog prove rel loss bound gener initi diverg loss need relat lemma might extend nondierenti convex function use gener notion bregman diverg introduc gordon gor paramet maintain algorithm invari permut past exampl howev total onlin loss algorithm permut invari therefor adversari could use fact present exampl order disadvantag learn algorithm suggest algorithm better rel loss bound increment olin algorithm forward algorithm incompar sens either one may larger total loss howev believ sens forward algorithm better need formal belief inspir phenomenon stein estim statist ste sinc like stein estim forward updat use shrinkag factor compar increment olin updat still need explor bound obtain paper relat larg bodi research minimum descript length rel loss bound literatur ri ri literatur lower upper bound rel loss form explor extens number paramet howev contrast setup use paper work minimum descript length literatur requir algorithm predict paramet paramet space methodolog develop paper prove rel loss bound still need work learn problem instanc alway logt style rel loss bound member exponenti famili see section anoth open problem prove logt style bound logist regress final lower bound rel loss need explor case algorithm restrict predict paramet paramet space bound shown linear regress vov particular proven constant lnt rel loss bound forward algorithm theorem tight howev gener logt style lower bound known arbitrari member exponenti famili acknowledg thank leonid gurvitz introduc us bregman diverg claudio gentil peter grunwald georey gordon eiji taki moto two anonym refere mani valuabl comment r exponenti mani local minima singl neuron rel loss bound onlin densiti estim exponenti famili distribut analog minimax theorem vector payo relax method iter rowact method interv convex pro gram univers portfolio side inform univers portfolio least squar maximum entropi learn probabilist predict function predict worst case predict binari sequenc almost well optim bias coin gener converg result linear discrimin updat technic report cmuc minimum discript length principl reason uncertainti approxim bay risk repeat play learn algorithm track chang concept investig error surfac singl arti sequenti predict individu sequenc gener loss function track best regressor conf comput addit versu exponenti gradient updat linear predict rel loss bound multidimension regress problem learn irrelev attribut abound new linearthreshold algorithm journal comput complex weight major algorithm gener linear model natur exponenti famili quadrat varianc function stochast complex statist inquiri fisher inform stochast complex convex analysi inadmiss usual estim mean multivari normal distribut asymptot minimax regret exponenti famili asymptot minimax regret bay mixtur aggreg strategi competit onlin linear regress continu discret time nonlinear gradient descent rel loss bound converg minimax redund class memoryless sourc ieee transact inform theori decisiontheoret extens stochast complex applic learn tr ctr arindam banerje inderjit dhillon joydeep ghosh srujana merugu inform theoret analysi maximum likelihood mixtur estim exponenti famili proceed twentyfirst intern confer machin learn p juli banff alberta canada shai shalevshwartz yoram singer primaldu perspect onlin learn algorithm machin learn v n p decemb jrgen forster manfr k warmuth rel loss bound temporaldiffer learn machin learn v n p april srujana merugu joydeep ghosh privacysensit approach distribut cluster pattern recognit letter v n p march s vn vishwanathan nicol n schraudolph alex j smola step size adapt reproduc kernel hilbert space journal machin learn research p nicol cesabianchi claudio gentil luca zaniboni worstcas analysi select sampl linear classif journal machin learn research p claudio gentil robust pnorm algorithm machin learn v n p decemb arindam banerje srujana merugu inderjit s dhillon joydeep ghosh cluster bregman diverg journal machin learn research p mark herbster manfr k warmuth track best linear predictor journal machin learn research p