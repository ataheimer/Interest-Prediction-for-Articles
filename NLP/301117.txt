t predict analysi wavefront applic use loggp a paper develop highli accur loggp model complex wavefront applic use mpi commun ibm sp key featur model includ elucid princip wavefront synchron structur explicit highfidel model mpisend mpirec primit mpisendrec model use deriv l o g simpl twonod microbenchmark model paramet obtain measur small applic problem size four sp node result show loggp model predict second high degre accuraci measur applic execut time larg problem run node detail perform project provid larg futur processor configur expect avail applic develop result indic scale beyond one two thousand node yield greatli diminish improv execut time synchron delay princip factor limit scalabl applic b introduct paper investig use parallel machin model call loggp analyz perform larg complex applic stateoftheart commerci parallel platform applic known sweepd interest threedimension particl transport problem identifi asci benchmark evalu high perform parallel architectur applic also interest fairli complex synchron structur synchron structur must captur analyt model order model accur predict applic execut time thu provid accur perform project larger system new architectur modif applic one question address research variant logp model best suit analyz perform sweepd ibm sp system sinc version sweepd use mpi commun primit loggp model includ addit paramet g accur model commun cost larg pipelin messag turn provid requisit accuraci possibl due block natur mpi primit content messag process resourc neglig thu recent extens logp captur impact content need previou work logp model appli import fairli simpl kernel algorithm fft lu sort algorithm spars matrix multipli two experiment studi appli model complex full applic splash benchmark howev studi effect synchron applic perform scalabl measur empir rather estim model mani previou analyt model analyz applic perform restrict simpler synchron structur sweepd eg one except determinist task graph analysi model shown accur predict perform applic complex synchron structur loggp model repres synchron structur abstractli task graph key question address research whether abstract represent suffici analyz full complex applic sweepd construct loggp model captur synchron structur also elucid basic synchron structur sweepd similar approach use commun microbenchmark deriv input paramet l o g howev show section deriv paramet somewhat complex mpi commun sp meiko cs thu explicit model mpisend mpirec primit develop although loggp input paramet deriv fourprocessor run sweepd loggp model project perform quit accur processor sever fix total problem size sever case fix problem size per processor model also quickli easili project perform larg futur processor configur expect avail applic develop research support part darpaito contract nc comput scienc technic report univers wisconsinmadison februari appear proc th acm sigplan symp principl practic parallel program ppopp atlanta ga may show sever interest result deriv analysi section provid brief overview sweepd applic section deriv model mpisend mpirec paramet valu character commun cost section present loggp equat sweepd well modif need applic util multiprocessor smp node sp latter case two type commun cost intraclust inter cluster section provid model valid result well perform project futur system section provid conclus work sweepd sweepd describ detail task graph show complex synchron among task version code analyz paper given give simpl overview version sweepd includ aspect relev loggp model structur algorithm appar loggp model present section name impli sweepd transport calcul implement seri pipelin sweep three dimension grid let dimens denot ijk grid map onto twodimension array processor size mn processor perform calcul partit j dimens size itjtk shown figur note that due problem map figur processor processor grid figur number p ij vari n indic horizont posit processor singl iter consist seri pipelin sweep grid start corner or octant grid map sweep two dimension processor grid illustr figur mo denot number angl consid problem processor perform itjtkmo calcul sweep octant creat finer granular pipelin thu increas parallel comput block data comput given processor partit angl block factor mmi kplane block factor mk paramet specifi number angl number plane kdimens respect comput boundari data forward next processor pipelin processor interior processor grid receiv boundari data two neighbor processor comput block base valu send result calcul two neighbor destin processor determin direct sweep optim version sweepd analyz block given processor calcul sweep given pair octant processor free start calcul block sweep next pair octant exampl lower left corner processor start comput first block sweep octant comput last block sweep origin octant shown greater detail loggp model sweepd section pipelin sweep octant complet one iter algorithm one energi group code analyz twelv iter execut one time step target problem interest asci program involv order group time step grid size order twenti million eg scale model project problem size shown section commun paramet l o g present loggp model sweepd sp deriv model mpisend mpirec commun primit use applic mpisendrec model need loggp model sweepd also need deriv two commun paramet valu name network latenc l process overhead o send receiv messag commun structur sweepd ignor gap g paramet time consecut messag transmiss greater minimum allow valu intermessag transmiss time give roundtrip commun time mpi commun ibm sp measur use simpl commun microbenchmark valu g gap per byte deriv directli measur discuss model sp mpisend mpirec primit use l o g paramet follow descript valu l deriv signific result deriv valu l g but differ valu o fortran c microbenchmark measur greatli increas confid valid mpi commun model figur partit grid j dimens figur sweep octant jt processor grid measur commun time roundtrip commun time function messag size simpl fortran commun microbenchmark given figur a b data point messag given size sent processor processor b receiv process processor b immedi sent back a roundtrip time measur subtract time call mpisend time mpirec oper complet figur also includ result model roundtrip commun use deriv l discuss below seen figur measur commun time increas significantli messag size henc g paramet requir accur model commun cost two point worth note figur commun cost chang abruptli messag size equal kb due handshak mechan implement messag larger kb handshak model below slope curv g chang messag size equal kb messag process overhead o also differ messag larger kb messag smaller kb due maximum ip packet size thu deriv separ valu g g l messag model mpisend mpirec model develop reflect fairli detail understand mpisend mpirec primit implement sp abl obtain author mpi softwar might necessari modifi model futur version mpi librari sweepd run differ messagepass architectur modifi use nonblock mpi primit model illustr gener approach captur impact system modif sinc sp system use poll receiv messag assum overhead send messag approxim overhead receiv messag o messag smaller kb handshak requir total endtoend cost send receiv messag model simpli as valu g depend whether messag size larger smaller kb messag larger kb endtoend commun requir handshak header initi sent destin processor destin processor must repli short acknowledg correspond receiv post receiv post header messag sent endtoend cost model follow note process overhead receiv ack model subsum process overhead send data correspond receiv yet post addit synchron delay incur delay model next section addit total cost commun given abov loggp model sweepd requir separ cost send receiv messag messag size less kb valu depend messag size messag size greater equal kb receiv cost includ time inform send processor receiv post delay messag arriv commun paramet valu use equat total_comm measur roundtrip commun time deriv valu l given tabl valu g g comput directli slope curv in figur respect rang messag size deriv l o solv three equat total_comm for messag size less kb kb greater kb respect three unknown l appli method roundtrip time measur obtain c microbenchmark yield valu l g measur figur mpi round trip commun messag size time usec messag size time usec measur model obtain fortran benchmark although valu differ shown tabl greatli increas confid valid model mpi commun primit use paramet valu deriv way measur model commun cost differ less messag kb shown figur note although measur model valu seem diverg messag size equal kb figur a figur b show valu messag size kb good agreement loggp model sweepd section develop loggp model sweepd use model mpi commun cost develop section first present model assum processor mn processor grid map differ smp node sp case network latenc commun give modifi equat case region processor grid map singl fourprocessor smp node sp roundtrip time paramet valu comput section commun processor differ smp node equat use comput intranod commun paramet basic model loggp model take advantag symmetri sweep perform execut thu calcul estim execut time sweep one octant pair use execut time obtain total execut time sweep explain below sweep describ section processor wait input two neighbor processor comput valu portion grid size mmi mk jt processor send boundari valu two neighbor processor wait receiv new input again use cost associ activ develop loggp model summar tabl directli express preced sendrec synchron constraint implement algorithm time comput one block data model equat tabl equat w g measur time comput one grid point mmi mk jt input paramet defin section specifi number angl grid point per block per processor consid octant pair sweep begin processor upperleft corner processor grid shown figur recal upperleft processor number p account pipelin wavefront sweep use recurs formula equat tabl comput time processor p ij begin calcul sweep denot horizont posit processor grid first term equat correspond case messag west last arriv processor p ij case messag north alreadi sent cannot receiv messag west process due block natur mpi commun second term equat model case messag north last arriv note startp appropri one two term equat delet processor east north edg processor grid sweepd applic make sweep across processor direct octant pair critic path time two rightdownward sweep comput equat tabl time lowerleft corner processor p m finish commun result last block sweep octant point sweep octant to upper right start processor p m proceed toward note subscript send receiv term equat includ indic direct commun event make easier understand term includ equat send receiv cost deriv section critic path sweep octant time processor grid complet calcul sweep sinc sweep octant in next iter begin processor p n finish due symmetri sweepd algorithm mention abov time sweep northeast total time sweep octant start processor p move southeast processor p nm thu comput critic path time octant shown equat tabl equat repres time processor p nm finish last calcul second octant pair processor tabl loggp model sweepd messag size tabl sp mpi commun paramet directli east p nm must start comput calcul commun need result block octant wait processor p nm receiv result last block calcul comput result base block due symmetri sweep octant sweep octant total execut time one iter comput equat tabl equat contain one term ml equat contain two term ml nl account synchron cost synchron term motiv observ measur commun time within sweepd greater measur mpi commun cost discuss section ml term captur delay caus send block destin processor post correspond receiv delay accumul j direct thu total delay m depend number processor north m furthermor synchron cost zero problem messag size smaller kb sinc case processor send messag whether correspond receiv post second synchron delay nl repres differ receiv post messag actual receiv send processor sinc processor receiv north west southeast sweep like wait messag west sinc delay cumul processor dimens processor p nm model delay nl notic receiv synchron term processor west edg processor grid sinc processor west receiv messag includ express abov model cluster smp node modif model need region processor grid map singl fourprocessor smp cluster ibm sp rather map processor grid separ smp node chang outlin here anticip next gener mpi softwar sp support full use cluster processor let l local denot network latenc intraclust messag remot denot latenc interclust messag l local l remot follow discuss assum intraclust interclust messag equat easili modifi case let l r subscript denot model variabl eg totalcomm send receiv comput use l local l remot respect use notat modifi equat comput execut time sweepd given tabl describ below recal processor number start j dimens also recal that processor p ij denot horizon posit processor grid j even incom messag intraclust outgo messag interclust vice versa true j odd mean startp ij comput totalcomm l receiv l send l for incom messag former case totalcomm r receiv r send r latter case odd j even variabl first term startp ij interclust commun commun variabl second term intraclust commun vice versa true even j odd send receiv variabl equat intraclust variabl assum number processor j dimens even map processor region smp cluster synchron term comput use l avg chang requir model modifi model valid detail simul howev sinc cannot yet valid system measur becaus effici mpi softwar intraclust commun yet exist result case processor map separ smp node given paper nevertheless chang model full cluster use simpl illustr model versatil furthermor equat use project system perform next gener mpi softwar measur work w valu work per grid point w g obtain measur valu x grid processor fact obtain accuraci result paper measur w g perprocessor grid size account differ up aris cach miss effect sinc sweepd program contain extra calcul fixup five twelv iter measur w g valu iter type although detail creator logploggp may intend increas accuraci substanti need larg scale project section furthermor recurs model sweepd repres tabl modifi loggp equat intraclust commun sp sweep sweepd code addit measur comput time main bodi code ie iter time step comput time denot w w measur singl processor run specif problem size model paramet thu measur use simpl code instrument rel short one two fourprocessor run next section investig accur model predict measur execut time sweepd applic experiment result section present result obtain loggp model valid loggp project sweepd run time measur run time processor use loggp model predict evalu scalabl sweepd thousand processor two differ problem size interest applic develop unless otherwis state report execut time one energi group one time step twelv iter time step figur compar execut time predict loggp model measur execut time fortran version sweepd sp processor fix total problem size kblock mk equal number processor increas messag size comput time per processor decreas overhead synchron increas problem size processor configur messag size vari kb kb remark high agreement model estim measur system perform across entir rang figur show larger problem size achiev reason good speedup ie low commun synchron overhead processor smaller problem size not note model highli accur case figur show predict measur applic execut time function number processor sp two differ case fix problem size per processor figur a processor partit threedimension grid size figur b processor partit size xx experi total problem size increas number processor increas agreement model estim a problem size b problem size figur valid loggp model fix total problem size a processor b processor figur sweepd speedup fix total problem size figur code mk mmi processor time sec model processor time processor processor measur execut time gener excel level abstract model howev result show model less quantit accur mk verifi mani configur loggp model qualit accur determin whether execut time mk higher lower execut time mk also verifi model quantit accur valu mk larger result xx also illustr c version code which creat fortran version use fc somewhat slower fortran code although absolut perform c differ perform trend report paper fortran code also observ c code model project figur show project execut time sweepd fix problem size per processor system scale thousand processor expect avail asci site near futur two fix perprocessor problem size consid case model predict valid processor use simul not shown measur execut time case illustr unexplain system anomali measur execut time suddenli increas given small increas number processor anomali occur coupl fix perprocessor grid size examin note anomali occur even though problem size per processor fix thu seem unlik explain cach behavior messag size one hazard model analyt simul anomal system behavior cannot predict howev model estim show jump execut time due expect commun synchron cost detail examin system implement requir discov hope correct caus anomali figur figur a b predict excel scale case memori usag per processor kept constant nevertheless solv problem size grid point per processor requir processor result figur a suggest execut time scale group time step prohibit problem configur a b figur valid loggp model fix problem size per processor a figur project sweepd execut time fix problem size per processor mmi mk number processor time sec c measur mk c loggp mk c measur mk c loggp mk measur mk loggp mk measur mk loggp mk number processor time number processor time number processor time sec measur loggp figur give project execut time sweepd system scale processor two differ total problem size interest applic develop case project execut time singl time step involv iter scale factor reflect fact comput interest scientist involv energi group rather one note problem size per processor decreas number processor increas thu sweepd configur larger mk higher perform loggp model use determin valu sweepd configur paramet ie mmi yield lowest execut time given processor configur problem size one key observ result figur point greatli diminish improv execut time number processor increas beyond one two thousand second key observ figur b even optim valu sweepd configur paramet unlimit number processor solv billion grid point problem time step appear requir prohibit execut time use current algorithm investig caus limit scalabl figur figur show breakdown execut time problem size figur breakdown show much critic path execut time due comput nonoverlap synchron nonoverlap commun key observ system scale synchron delay becom signific domin factor execut time these synchron delay model ml nl term equat tabl modif reduc synchron cost would highli desir solv larg problem interest exampl simpl modif might explor use nonblock form mpisend howev fundament algorithm chang reduc synchron delay may need figur show could yield greater benefit improv processor technolog due difficulti speed commun latenc a million grid point b billion grid point figur project sweepd execut time fix total problem size one time step a million grid point b billion grid point figur project sweepd execut time commun synchron cost one time step number processor time loggp mmi mk loggp mmi mk loggp mmi mk loggp mmi mk loggp mmi mk loggp mmi mk number processor number processor total comp comm synch number processor conclus princip contribut research loggp model analyz project perform import applic complex synchron structur wavefront applic loggp equat captur princip synchron cost also elucid basic pipelin synchron structur illustr abstract capabl domain compar simplic commun paramet l o g research provid case studi model valid extrem well measur applic perform illustr potenti loggp model analyz wide varieti interest applic includ import class wavefront applic signific result obtain sweepd applic studi paper follow first scale beyond one two thousand processor yield greatli diminish return term improv execut time even larg problem size second solv problem size order grid point group time step appear impract current algorithm final synchron overhead princip factor limit scalabl applic futur work includ gener model present research creat reusabl analyt model wavefront applic execut product parallel architectur develop model sharedmemori version sweepd develop loggp model applic complex synchron structur r analyz behavior perform parallel program loggp incorpor long messag logp model logp toward realist model parallel comput poem endtoend perform design larg parallel adapt comput system fast parallel sort logp experi cm lopc model content parallel algorithm the effect latenc occup bandwidth distribut share memori multiprocessor solut firstord form fo d discret orgin equat massiv parallel processor effect commun latenc overhead bandwidth cluster architectur logpc model network content messag pass program tr logp toward realist model parallel comput analyz behavior perform parallel program loggp predict applic behavior larg scale sharedmemori multiprocessor fast parallel sort logp effect commun latenc overhead bandwidth cluster architectur poem effect latenc occup bandwidth distribut share memori multiprocessor ctr ewa deelman gurmeet singh meihui su jame blyth yolanda gil carl kesselman gaurang mehta karan vahi g bruce berriman john good anastasia laiti joseph c jacob daniel s katz pegasu framework map complex scientif workflow onto distribut system scientif program v n p juli fumihiko ino noriyuki fujimoto kenichi hagihara loggp parallel comput model synchron analysi acm sigplan notic v n p juli gabriel marin john mellorcrummey crossarchitectur perform predict scientif applic use parameter model acm sigmetr perform evalu review v n june daniel nurmi anirban mandal john brevik chuck koelbel rich wolski ken kennedi grid schedul protocolsevalu workflow schedul use integr perform model batch queue wait time predict proceed acmiee confer supercomput novemb tampa florida kirk w cameron rong ge predict evalu distribut commun perform proceed acmiee confer supercomput p novemb ruom jin gagan agraw perform predict random write reduct case studi model share memori program acm sigmetr perform evalu review v n june vikram adv rizo sakellari applic represent multiparadigm perform model largescal parallel scientif code intern journal high perform comput applic v n p novemb rajiv bagrodia ewa deelman thoma phan parallel simul largescal parallel applic intern journal high perform comput applic v n p februari david k lowenth accur select block size runtim pipelin parallel program intern journal parallel program v n p june det buakle gregori f traci mari k vernon stephen j wright nearoptim adapt control larg grid applic proceed th intern confer supercomput june new york new york usa vikram s adv rajiv bagrodia jame c brown ewa deelman aditya dube elia n housti john r rice rizo sakellari david j sundaramstukel patricia j teller mari k vernon poem endtoend perform design larg parallel adapt comput system ieee transact softwar engin v n p novemb ruom jin gagan agraw methodolog detail perform model reduct comput smp machin perform evalu v n p may vikram s adv mari k vernon parallel program perform predict use determinist task graph analysi acm transact comput system toc v n p februari