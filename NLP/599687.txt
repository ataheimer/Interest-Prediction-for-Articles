t spars regress ensembl infinit finit hypothesi space a examin method construct regress ensembl base linear program lp ensembl regress function consist linear combin base hypothes gener boostingtyp base learn algorithm unlik classif case regress set possibl hypothes produc base learn algorithm may infinit explicitli tackl issu defin solv ensembl regress hypothesi space infinit approach base semiinfinit linear program infinit number constraint finit number variabl show regress problem well pose infinit hypothesi space primal dual space importantli prove exist optim solut infinit hypothesi space problem consist finit number hypothesi propos two algorithm solv infinit finit hypothesi problem one use column gener simplextyp algorithm adopt exponenti barrier approach furthermor give suffici condit base learn algorithm hypothesi set use infinit regress ensembl comput result show method extrem promis b introduct past year seen strong interest boost ensembl learn algorithm due success practic classic applic eg drucker et al lecun et al maclin opitz schwenk bengio bauer kohavi dietterich basic idea boost and ensembl learn gener iter gener sequenc fh g t function hypothes usual combin c kluwer academ publish print netherland regressiontex p yoshua bengio dale schuurman nonekluw v g ratsch a demiriz kp bennett hypothesi coecient use hypothes h element hypothesi class p index set hypothes produc base learn algorithm l typic one assum set hypothes h nite also consid extens innit hypothesi set classic ensembl gener label signf x weight major vote regress predict valu f x recent research eld focus better understand method extens concern robust issu mason et al bennett et al ratsch et al b shown classic ensembl method view minim function classic margin typic perform algorithm use gradient descent approach function space recent shown soft margin maxim techniqu util support vector machin readili adapt produc ensembl classic ben nett et al ratsch et al b algorithm optim soft margin error measur origin propos support vector machin certain choic error margin norm problem formul linear program lp rst glanc lp may seem intract sinc number variabl linear program proport size hypothesi space exponenti larg fact two practic algorithm exist optim soft margin ensembl rst use column gener simplex algorithm bennett et al second use barrier function interiorpoint method ratsch et al advantag linear program approach produc spars ensembl use fast nite algorithm purpos work tackl regress ensembl use analog support vector linear program methodolog regress date rel paper address ensembl regress zemel pitassi one major diculti rigor dene regress problem innit hypothesi space classic assum hypothesi nite set possibl output hypothesi space alway nite sinc nite number way label nite train set regress even rel simpl hypothesi space linear function construct use weight least squar consist uncount innit set hypoth se priori clear even express regress problem innit hypothesi space clearli practic consid spars regress ensembl ensembl function linear combin nite subset set possibl hypothes work studi directli issu innit hypothesi space begin section review boost type algorithm classic regress examin relationship ensembl method linear program section review linear program approach spars regress show easili extend ensembl regress nite hypothesi case section investig dual linear program ensembl regress section propos semiinnit linear program formul boost innit hypothesi set rst dual primal space dual problem call semiinnit innit number constraint nite number variabl import spars properti semiinnit regress problem solut consist nite number hypothes section propos two dierent algorithm ecient comput optim ensembl exact implement algorithm depend choic base learn algorithm section investig three possibl base learn algorithm result innit nite hypothesi set comput result present section notat convent use paper found tabl i tabl i notat convent n n counter number pattern counter number hypothes nite t counter number iter indexset hypothes space dimension x train data input target train pattern label set base hypothes element h set linear combin h element f hypothesi weight vector weight train set w weight vector linear model i indic function tube size tube paramet determin c regular complex paramet weight classic error k kp pnorm product scalar product featur space g ratsch a demiriz kp bennett boostingtyp algorithm brie review discuss exist boostingtyp algorithm section start classic case describ adaboost close relat arcgv breiman discuss properti solut gener boost show connect linear program lp maxim margin section brie review recent regress approach mainli motiv gradientdesc understand boost classif boost lp classic case gener assum hypothes class dene base learn algorithm l iter base learner use select next hypothesi use certain criteria ensembl gener label weight major vote signf x note hypothesi class alway nite n distinct label train data consid adaboost algorithm detail see eg freund main idea adaboost introduc weight n train pattern z g use control import singl pattern learn new hypothesi ie repeatedli run base algorithm train pattern dicult learn which misclassi repeatedli becom import increas weight shown adaboost minim error function breiman frean down friedman et al ratsch et al express term margin name iter solv problem min optim strategi adaboost also call gradient descent function space mason et al friedman et al one eectiv optim along restrict gradient direct space linearli combin function f also understood coordin descent method eg luenberg minim g possibl weight hypothes h ratsch et al one hypothesi ad time weight never chang unless hypothesi ad again spars regress ensembl wide believ breiman freund schapir schapir et al ratsch et al adaboost approxim maxim smallest margin train set problem solv exactli follow linear program problem complet hypothesi set h cf grove schuurman assum nite number basi n f breiman propos modic adaboost arcgv make possibl show asymptot converg global solut lp grove schuurman lp solv use iter linear program base approach retrospect consid column gener algorithm unfortun neither approach perform well practic margin version linear program base idea support vector machin perform well practic theoret term gener bound ratsch et al b bennett et al exampl soft margin version could n f bennett et al column gener algorithm classic propos ecient solv lp algorithm close relat ratsch et al kivinen warmuth dier gradientboost idea use motiv boostingtyp algorithm mason et al friedman et al iter gener hypothesi weight optim respect maximum margin error function gradient approach xe hypothesi weight hypothes gener purpos paper examin extens approach regress case g ratsch a demiriz kp bennett previou regress approach sever regress boost method propos provid brief descript three them note rst two describ also fisher g ridgeway reduc problem seri classic task thu elimin consider innit hypothesi space last approach friedman appli innit hypothesi space dene mean boost innit hypothesi space adaboostr rst boostingtyp algorithm regress adaboostr propos freund schapir base reduct classic case algorithm aim nd regress function problem algorithm use piecewis linear function whose number branchpoint increas exponenti number iter therefor algorithm comput intract adaboostr anoth reduct nding f x classic case propos bertoni et al here pattern predict error less count correctli classi misclassi otherwis combin regress function given again probabl weight train pattern use assumpt weight classic error iter smaller number train pattern jfx n converg quickli zero experi turn i choic rather dicult ii select next hypothesi base learner demand problem weight error usual converg quickli algorithm stop gradient boost regress friedman base understand boost gradient descent method regress algorithm propos eg interest paper friedman friedman here deriv g cost function g eg squar loss respect output fx n regress function spars regress ensembl project gradient direct a basi function h h direct true gradient found g idea work squar loss linear absolut loss huber loss howev gradient direct found optim squar loss onli linear absolut loss special treeboost algorithm friedman here task nding next hypothesi pose classic problem sign gradient determin class membership algorithm aim maxim correl gradient output base hypothesi approach similar algorithm propos section approach work well practic explicitli deal innit hypothesi case like gradient descent algorithm oer converg limit even nite hypothesi space sinc regular use potenti overt develop good stop criteria essenti next section develop altern approach base linear program advantag lp approach includ extens innit hypothesi case spars solut guarante exist spars nite solut practic fast nite algorithm linear program regress section develop nite semiinnit lp formul spars ensembl regress begin primal lp nite case investig dual nite lp extend dual primal innit hypothesi case finit spars linear regress r iid train data regress problem often state nding function f minim regular risk function vapnik rf g ratsch a demiriz kp bennett l loss function p regular oper c regular paramet determin tradeo loss complex ie size function class paper consid wellknown insensit loss vapnik scholkopf et al loss function penal error chosen priori shown sever nice properti see later cf smola howev principl analysi algorithm also work loss function cf ratsch paper consid f space linear combin base hypothes anoth space h socal base hypothesi space includ bia ie f assum h nite number hypothes j them gener innit hypothesi class section throughout paper assum h close complement henc one may enforc eectiv chang f let us consid norm hypothesi coecient regular use minim state linear program call lpregress problem min n f xed constant regular oper jjjj frequent use spars favor approach eg basi pursuit chen et al parsimoni least norm approxim bradley et al roughli speak reason induc spars fact vector far coordin axe larger respect norm respect pnorm p exampl consid vector two norm spars regress ensembl norm note use norm regular optim solut alway vertex solut or express such tend spars easili shown cf corollari independ size nite hypothesi space h optim number hypothes ensembl greater number sampl optim algorithm propos section exploit properti nice properti solut robust respect small chang train data proposit smola et al use linear program regress insensit loss function local movement target valu point insid outsid ie edg of tube uenc regress paramet usual dicult control muller et al scholkopf et al one usual know beforehand accur one abl curv problem partial resolv follow optim problem smola et al min n f dierenc lie fact becom posit constrain variabl optim problem itself core aspect captur proposit state below proposit smola et al assum follow statement hold i upper bound fraction error ie point outsid tube ii lower bound fraction point insid ie outsid edg of tube iii suppos data gener iid distribut p x ical equal fraction point insid tube fraction error g ratsch a demiriz kp bennett summar optim problem two paramet i regular paramet c control size hypothesi set therefor complex regress function ii tubeparamet directli control fraction pattern outsid tube indirectli control size tube dual finit lp formul section state dual optim problem introduc lagrangian multipli n rst constraint comput error target underestim n error measur target overestim see linear program text book specic construct dual lp problem dual problem dd constraint come reparameter here n xed constraint j jhj constraint one hypothesi h h optim point quantiti p n dene error residu complementar know error zero that point underesti point overestim f thu point within tube p n point fall tube p n point fall tube magnitud p n ect sensit object chang larger chang error larger p n quantiti constraint ect well hypothesi address residu error posit larg size hypothesi like improv ensembl must sucient larg oset penalti increas kk gener infinit hypothes consid case innit set possibl hypothes h say select nite subset h h primal dual regress lp h well dene say increas spars regress ensembl subset size dene h h h relationship optim ensembl creat two subset solut smaller h lp alway primal feasibl larger h lp h solut dual feasibl larger h lp solut also optim problem h dual feasibl key issu dene base learn algorithm l xed p dual feasibl violat h p good hypothesi ad ensembl solut may optim think h function extend dual problem innit hypothes case set dual feasibl valu p equival follow compact polyhedron dual silpregress problem dd exampl semiinnit linear program silp class problem extens studi mathemat program ming problem call semiinnit innit number constraint nite number variabl set p known index set set hypothes produc base learner nite eg fh nite problem exactli equival lpregress problem establish sever fact semiinnit program problem use result gener linear semiinnit program summar excel review paper hettich kortanek simplifi present simpli result hettich kortanek case silp addit set nite linear constraint result present easili deriv hettich chang notat increas g ratsch a demiriz kp bennett index set includ addit nite set tradit linear constraint consist deriv silpregress problem refer problem innit mani constraint dual problem problem innit mani variabl primal problem care taken sinc revers convent use mathemat program literatur dene gener dual silp compact set a function b r n b function r n r make addit assumpt problem alway feasibl feasibl region compact clearli maximum valu alway obtain sinc maxim continu function compact set ideal would like solut linear program correspond optim solut semiinnit problem dene necessari condit exist nite linear program whose optim solut also solv semiinnit program denot gener dual silp restrict nite subset b dpn linear program sinc nite number constraint rst theorem give necessari condit optim solut gener dual silp equival solut nite linear program theorem hettich kortanek theorem necessari condit nite solut assum follow slater condit hold everi set n z hap n n qz r exist exist multipli n result immedi appli dual silp regress problem sinc strictli interior point satis slater condit corollari finit solut regress ensembl prob lem d exist dpn spars regress ensembl signic result exist optimi ensembl consist n hypothes n number data point true even set possibl hypothes innit primal regress silp next look correspond primal problem semiinnit case would like semiinnit dual problem equival meaning primal problem simpli origin primal nite hypothesi case set nonneg borel measur b subset r denot set nonneg gener nite sequenc primal problem gener silp nite linear program optim object valu primal dual problem alway equal alway true semiinnit case weak dualiti alway hold is p d must ensur dualiti gap ie p d hettich kortanek theorem follow theorem sucient condit dualiti gap let convex cone ap ap close p primal minimum attain regress problem set base hypothes evalu train point obtain learn algorithm constant thu theorem simpli follow corollari sucient condit base learner let convex cone g ratsch a demiriz kp bennett close d primal minimum attain corollari impos condit set possibl base hy pothes exampl set base hypothesi would satisfi condit are set possibl hypothes nite eg fh pg nite function h continu respect p two condit sucient cover base hypothes consid paper condit possibl lp ensembl optim algorithm section propos two algorithm optim nite innit regress linear program rst use column gener execut simplextyp algorithm second adopt exponenti barrier strategi connect boost algorithm classic ratsch et al column gener approach basic idea column gener cg construct optim ensembl restrict subset hypothesi space lp solv nite subset hypothes call restrict master problem base learner call gener hypothesi assum base learner nd best hypothesi satisfi condit current ensembl optim constraint fulll not hypothesi ad problem correspond gener column primal lp silp row dual lp silp cgregress algorithm cf algorithm assum base learner lx p nite p p algorithm special case set silp algorithm known exchang method method known converg clearli set hypothes nite method converg nite number iter sinc constraint ever drop one also prove converg silp cf theorem hettich theorem converg algorithm algorithm stop nite number step solut dual regress spars regress ensembl algorithm cgregress algorithm argumentsampl regular constant c tube paramet return linear combin h function cgregx repeat let d solut use hypothes let b dual solut d ie solut return silp sequenc intermedi solut d least one accumul point solv dual regress silp theorem hold gener set exchang method algorithm exampl possibl add drop multipl constraint iter converg result unchang practic found column gener algorithm stop optim solut small number iter lp silp regress problem barrier algorithm follow propos algorithm see also ratsch et al use barrier optim techniqu bertseka frisch detail connect boostingtyp algorithm barrier method see ratsch et al similar algorithm propos duy helmbold develop independ sequel give brief introduct barrier optim goal barrier optim nd optim solut problem min f f convex function nonempti convex set feasibl solut problem solv use call barrier function eg bertseka cominetti dussault mosheyev zibulevski censor zenio exponenti barrier particularli use g ratsch a demiriz kp bennett choic purpos exp penalti paramet nding sequenc uncon strain minim f g use sequenc f g minim shown converg global solut origin problem ie hold min min barrier minim object problem use exponenti barrier written as exp simplic omit constraint rst line object second line correspond constraint n last line implement constraint n n note set r e nd minim slack variabl given b thu problem minim greatli simpli n variabl less optim section propos algorithm cf algorithm similar column gener approach last section solv sequenc optim problem call restrict master problem iter algorithm one select hypothesi solv or approxim solv unconstrain optim problem variabl hypothesi coecient previou iter bia b tube size solut restrict master problem respect master problem clearli suboptim one cannot easili appli howev known fast one decreas intermedi full master problem j spars regress ensembl algorithm barrierregress algorithm argumentsampl number iter t regular constant c tube paramet constant start return linear combin h function barregx endfor endfor return solut suboptim cf proposit cominetti dussault ratsch et al roughli speak one ensur achiev desir converg sens gradient taken respect variabl base learner need nd hypothesi larg edg hypothes correspond violat constraint dual problem wherea classic case maximum edg minim regress edg therefor dene correct edg respect constraint posit constraint violat consid case base learner nd hypothesi optim respect correct edg mean nd hypothesi much wors best hypothesi h ie constant note correct edg come regular term kk get g ratsch a demiriz kp bennett lemma run algorithm use base learner satisfi barrier paramet decreas kre k gradient taken respect variabl b proof gradient e respect b alway zero unbound variabl minim line gradient e respect j two case hypothesi alreadi restrict master problem line get j r j note case r j happen thu gradient project feasibl set alway zero hypothesi alreadi includ r j last constraint violat j hypothesi h j need includ hypothesi set thu one exploit properti base learner upperbound gradient master problem current solut learner return hypothesi exist anoth hypothesi edg larger factor assum exist violat constraint line decreas kre k use lemma one get desir converg properti algorithm theorem assum h nite base learner l satis condit output algorithm converg global solut proof let e given proposit cominetti du sault see ratsch et al one know accumul point sequenc f g satisfi kr e global solut lemma decreas kre k decreas gradient reduc nite number iter kre k thu spars regress ensembl similar condit use prove converg algorithm case nonoptim base learner sens barrier method also appli semiinnit program problem kaliski et al similar barrier algorithm use logbarri use cf also mosheyev zibulevski futur work rigor prove algorithm also converg optim solut hypothesi space innit algorithm propos incomplet without descript base hypothesi space base learner algorithm next section consid choic hypothesi space base learner eect algorithm choic hypothesi space base learner recal algorithm requir hypothesi h p solv approxim solv question solv dierent type base learner set base learner compact maximum must exist kernel function suppos wish construct ensembl function linear combin function eg kernel function use coecient ie function form k n kx n set fh g innit hypothesi set unbound unbound so one restrict consid bound norm constant eg h fh g problem close form solut let j maximum absolut sum kernel valu weight p solut p n mean boost linear combin kernel function bound norm g ratsch a demiriz kp bennett ad exactli one kernel basi function kx j per iter result problem exactli optim svm regress lp eg smola et al rst place dierenc dene algorithm optim function ad one kernel basi time pose problem semiinnit learn problem exactli equival nite svm case set hypothes boost individu kernel function kx bound use dierent norm would longer true would ad function sum mani kernel function for use norm see ratsch et al a likewis perform activ kernel strategi set kernel parameter set algorithm would chang consid problem next section activ kernel function consid case chose set kernel function parameter vector argument abov impos bound k need consid one basi function time case sinc kernel parameter set continu valu innit set hypothesi say exampl wish pick rbf kernel paramet the center the varianc ie chose hypothesi function exp paramet maxim correl weight p output the socal edg ie reason assumpt bound function p thu result semiinnit case hold sever way ecient nd straightforward way employ standard nonlinear optim techniqu maxim howev rbf kernel xed varianc fast easi implement emlik strategi set z normal factor updat comput weight center data weight spars regress ensembl depend p note given vector q one comput mstep optim center howev q depend one iter recomput q estep iter stop chang anymor object function local minima one may start random posit eg random train point svm classic function consid case use linear combin classica tion function whose output form regress function exampl algorithm treeboost algorithm friedman absolut error function treeboost construct classic tree class point taken sign residu point ie point overestim assign class point underestim assign class decis tree construct base project gradient descent techniqu exact linesearch point fall leaf node assign mean valu depend variabl train data fall node correspond dierent node decis tree iter virtual number hypothes ad sens correspond number leaf node decis tree take simpli view consid one node decis tree decis tree linear combin data specic decis function node fx b thu iter algorithm want wb note nite mani way label n point nite set hypothes innit mani possibl w b produc object valu equival boost algorithm question practic optim problem clearli upper bound best possibl valu equat obtain w b solut satisfi sens consid signp n desir class x n frequent may possibl construct f x n misclassi penal exactli jp n j thu think jp n j misclass cost x n given class misclass weight use weight sensit classic algorithm construct hypothesi g ratsch a demiriz kp bennett studi use follow problem convert lp form construct f signp n hw x becom paramet problem interest fact formul choic control capac base learner data xed choic classic function use rel xed number w nonzero user determin base experiment train data eect complex base hypothesi user may x accord desir complex base hypothesi altern weight variat svm scholkopf et al could use dynam chose like treeboost would like allow side linear decis dierent weight describ chang requir algorithm allow thi iter lp solv nd candid hypothesi instead ad singl column restrict master lp two column ad rst column second column h algorithm stop hypothes meet criteria given algorithm algorithm termin call variant algorithm cglp chang eect converg properti experi section present preliminari result indic feasibl approach start section show basic properti cg barrier algorithm regress show algorithm abl produc excel ts noiseless sever noisi toy problem base learner use three propos section denot cgk cgak cglp cg algorithm use rbf kernel activ rbf kernel classic function base learner respect likewis bark barak barlp use barrier algorithm possibl combin implement spars regress ensembl show competit algorithm perform benchmark comparison section timeseri predict problem extens studi past moreov give interest applic problem deriv computeraid drugdesign section there particular show approach use classic function base learner well suit dataset dimension problem high number sampl small experi toy data illustr i propos regress algorithm converg optim ie zero error solut ii capabl nding good noisi data signalnois appli toy exampl frequent use sinc function rang demonstr cf fig use two base hypothesi space i rbf kernel way describ section ie classic function describ section rst case use cg barrier approch lead algorithm cgk bark latter case includ demonstr purpos onli cglp design highdimension data set perform well low dimens due sever restrict natur base hypothesi set keep result compar dierent data set use normal measur error q error also call normal mean squar error dene as meaningless sinc simpli predict mean target valu result q valu one let us rst consid case rbfkernel noisefre case left panel fig observ expect proposit automat determin tube size small kept larg high nois case right panel use right tube size one get almost perfect q noisefre case excel noisi case q without retun paramet cglp produc piecewiseconst function base two classic function solut produc noisi noisefre case interestingli noisi g ratsch a demiriz kp bennett figur toy exampl left panel show sinc function without nois use rbfkernel solid classic function dash solid almost perfect q dash function simpl q right panel show use rbfkernel q noisi data sig tube size automat adapt algorithm right half pattern lie insid tube case produc almost ident function hypothesi space consist linear classic function construct lp set base hypothesi extrem restrict thu high bia low varianc behavior expect see later high dimension dataset cglp perform quit well let us compar converg speed cg barrier regress control set toy exampl run algorithm record object valu restrict master problem iter barrier algorithm one nd minim almost minim paramet b barrier function e restrict master problem implement use iter gradient descent method number gradient step paramet algorithm result shown fig one observ algorithm converg rather fast optim object valu dot line cg algorithm converg faster barrier algorithm barrier paramet usual decreas quick enough compet ecient simplex method howev number gradient descent step larg enough eg barrier algorithm produc compar result number iter note one one gradient descent step per iter approach similar algorithm propos collin et al use parallel coordin descent step similar jacobi iter spars regress ensembl object valu iter replac object valu iter figur converg toy exampl converg object function cgregress solid barrierregress optim valu dot number iter left nois right larg normal nois barrierregress dashdot dash gradient descent step iter respect use time seri benchmark section would like compar new method svm rbf network chose two wellknown data set frequent use benchmark timeseri predict i mackeyglass chaotic time seri mackey glass ii data set santa fe competit weigend na gershenfeld ed x follow experiment setup comparison use seven dierent model comparison three model use muller et al rbf net svm regress svr linear huber loss four new model cgk cgak bark barak model train use simpl cross valid techniqu choos model minimum predict error measur randomli chosen valid set origin taken muller et al data includ experiment result obtain httpidafirstgmdderaetschdatat mackey glass equat rst applic highdimension chaotic system gener mackeyglass delay dierenti equat dt g ratsch a demiriz kp bennett delay origin introduc model blood cell regul mackey glass becam quit common artici forecast benchmark integr ad nois time seri obtain train pattern valid the follow pattern set use embed dimens test set pattern noiseless measur true predict error conduct experi dierent signal nois ratio snr use uniform nois tabl ii state result given origin paper muller et al svm use insensit loss huber robust loss quadraticlinear rbf network moreov give result cg barrier algorithm use rbf kernel activ rbfkernel also appli cg algorithm use classic function cglp algorithm perform poorli q could gener complex enough function tabl ii observ four algorithm perform averag good best algorithm in case better case wors step predict low nois level rather poor compar svm great higher nois level note cg barrier algorithm perform significantli dierent cg case better case wors show simpl barrier implement given algorithm achiev high enough accuraci compet sophist simplex implement use cgalgorithm data set santa fe competit data set santa fe competit artici data gener ninedimension period driven dissip dynam system asymmetr fourwel potenti slight drift paramet weigend na gershenfeld ed system properti oper one well time switch anoth well dierent dynam behavior therefor rst segment time seri regim approxim stationari dynam accomplish appli anneal competit expert ace method describ pawelzik et al muller et al no assumpt number stationari subsystem made moreov order reduc eect continu dene snr experi ratio varianc nois varianc data entri set ital model select fail complet case select model manual chose model th percentil test error test model spars regress ensembl drift last data point train set use segment appli ace algorithm data point individu assign class dierent dynam mode select particular class data includ data point end data set train set allow us train model quasistationari data avoid predict averag dynam mode hidden full train set see also pawelzik et al discu sion howev time left rather small train set requir care regular sinc pattern extract train set previou section use valid set pattern extract quasistationari data determin model paramet svm rbf network cgregress embed paramet use method compar tabl iii tabl iii show error q valu step iter predic tion previou result muller et al support vector machin in loss better one achiev pawelzik et al pawelzik et al current record dataset given quit hard beat record method perform herebi assum class data gener last point train set one also respons rst coupl step iter continu aim predict iter predict mean base past predict and origin data new predict comput tabl ii denot step predict error q test set step iter autonom predict snr ratio varianc respect nois underli time seri snr test error svm in g ratsch a demiriz kp bennett quit well cgak improv result pawelzik et al cgk better close previou result modelselect crucial issu benchmark competit model select basi best predict valid pattern turn rather suboptim thu sophist model select method need obtain reliabl result tabl iii comparison under competit condi tion step iter predict q valu data set d prior segment data accord muller et al pawelzik et al done preprocess cg svm neural net cgk cgak in huber rbf pkm experi drug data data set taken computeraid drug design goal predict bioreact molecul base molecular structur creation quantit structureact relationship model predict model construct larg databas screen cost eectiv desir chemic prop erti small subset molecul test use tradit laboratori techniqu target dataset lc cka logarithm concentr compound requir produc percent inhibit site a choleci tokinin cck molecul cck ccklike molecul serv import role neurotransmitt andor neuromodul compound taken merck cck inhibitor data set dataset origin consist descriptor taken combin tradit d d topolog properti electron densiti deriv tae transfer atom equival molecular descriptor deriv use wavelet brenema et al data scale data obtain httpwwwrpiedu bennek well known appropri featur select dataset other essenti good perform qsar model due perform experi barrier algorithm data sinc perform expect similar spars regress ensembl small amount avail data known bioreact larg number potenti descriptor see exampl embrecht et al unrel studi bennett et al featur select done construct norm linear support vector regress machin like equat featur input dimens produc spars weight descriptor descriptor posit weight retain take reduc set descriptor given refer full data set lccka reduc dataset lcckar typic perform measur use evalu qsar data averag sum squar error predict true target valu divid true target varianc q dene q less consid good measur perfor manc fold cross valid perform report outof sampl averag fold preliminari studi modelselect use paramet select techniqu perform model consid cglp cg classic function cg k cg nonact kernel describ section cgk use three dierent valu regular constant c tubeparamet paramet base learner kernelwidth complex paramet respect thu examin dierent paramet combin cglp use paramet valu found work well reduc dataset bennett et al chose c number hypothes attribut per hypothesi similar train data research progress repeat studi use appropri model select techniqu leaveoneout cross valid model select critic perform method thu ecient model select techniqu import open question need address first tri cgk full data set lccka fail achiev good perform q simpl approach cg lp perform quit well cglp abl select discrimin featur base subset attribut kernelapproach get confus uninform featur reduc set lcckar featur alreadi pre select kernel approach improv signicantli q signicantli dierent cglp method produc spars ensembl full dataset use paramet cglp use averag ensembl contain hypothes consist of averag possibl attribut cgk rbfkernel use hypothes reduc g ratsch a demiriz kp bennett dataset use paramet use averag ensembl contain hypothes consist of averag attribut cgk approach use averag hypothes slight dierenc cglp cgk might explain presenc uninform featur summar cglp approach seem robust method learn simpl regress function highdimension space automat featur select conclus work examin lp construct regress ensembl base norm regular insensit loss function use support vector machin rst propos ensembl nite hypothesi set smola et al use dual formul nite regress lp i rigor dene proper extens innit hypothesi case ii deriv two ecient algorithm solv them shown theoret empir even hypothesi space innit small nite set hypothes need express optim solut cf corollari spars possibl due use norm hypothesi coecient vector act sparsityregular propos two dierent algorithm ecient comput optim nite ensembl here baselearn act oracl nd constraint dual semiinnit problem violat rst algorithm the cg algorithm regress base simplex method prove converg innit case cf theorem second algorithm barrier algorithm regress base exponenti barrier method connect origin adaboost method classic cf ratsch et al algorithm converg nite hypothesi class cf theorem use recent result mathemat program literatur eg mosheyev zibulevski kaliski et al claim possibl gener innit case comput algorithm nd provabl optim solut small number iter examin three type base learn algorithm one base boost kernel function chosen nite dictionari ker nel exampl nite hypothesi set also consid activ kernel method kernel basi select innit dictionari kernel final consid case use nite set linear classic function construct use lp spars regress ensembl limit hypothesi space specic design work underdetermin highdimension problem drug design data discuss paper preliminari simul toy real world data show propos algorithm behav well nite innit case benchmark comparison timeseri predict problem algorithm perform well current state art regress method support vector machin regress case data set d santa fe competit obtain result good current record by svm dataset lp classicationbas approach work extrem well highdimension drug design dataset sinc algorithm inher perform featur select essenti success dataset primari contribut paper theoret conceptu studi lpbase ensembl regress algorithm nite innit hypothesi space futur work plan rigor investig comput aspect approach one open question best perform select lp model param ter anoth open question involv best algorithm approach solv semiinnit linear program work well practic column gener barrier interiorpoint method describ current state art semiinnit linear program primaldu interior point algorithm may perform even better theoret empir especi larg dataset lastli abil handl innit hypothesi set open possibl mani possibl type base learn algorithm acknowledg g ratsch would like thank sebastian mika klausr muller bob williamson manfr warmuth valuabl discuss work partial fund dfg contract ja ja mu nation scienc foundat grant no no r empir comparison vote classi boost algorithm regress nonlinear program parsimoni least norm approxim predict game arc algorithm wavelet represent molecular electron properti applic adm parallel optim theori atom decomposit basi pursuit adaboost logist regress uni stabl exponenti penalti algorithm superlinear converg experiment comparison three method construct ensembl decis tree bag comput intellig data mine autom design discoveri novel pharmaceut simpl cost function boost ing game theori decisiontheoret gener onlin learn applic boost experi new boost algorithm addit logist regress statist view boost greedi function approxim logarithm potenti method convex pro gram logarithm barrier decomposit method semiin nite program submit elsevi scienc boost entropi project linear nonlinear program second edit oscil chao physiolog control system empir evalu bag boost improv gener explicit optim margin function gradient techniqu combin hypothes advanc kernel method boost margin new explan e new support vector algorithm adaboost neural network gerstner learn kernel natur statist learn theori time seri pre diction forecast futur understand past gradientbas boost algorithm regress problem tr ctr gill blanchard gbor lugosi nicola vayati rate converg regular boost classifi journal machin learn research pierr geurt loui wehenkel florenc dalchbuc gradient boost kernel output space proceed th intern confer machin learn p june corvali oregon jinbo bi tong zhang kristin p bennett columngener boost method mixtur kernel proceed tenth acm sigkdd intern confer knowledg discoveri data mine august seattl wa usa kristin p bennett michinari momma mark j embrecht mark boost algorithm heterogen kernel model proceed eighth acm sigkdd intern confer knowledg discoveri data mine juli edmonton alberta canada p m granitto p f verd h a ceccatto neural network ensembl evalu aggreg algorithm artifici intellig v n p april peter bhlmann bin yu spars boost journal machin learn research p gunnar rtsch manfr k warmuth effici margin maxim boost journal machin learn research p sren sonnenburg gunnar rtsch christin schfer bernhard schlkopf larg scale multipl kernel learn journal machin learn research p robust loss function boost neural comput v n p august sebastian mika gunnar rtsch jason weston bernhard schlkopf alex smola klausrobert mller construct descript discrimin nonlinear featur rayleigh coeffici kernel featur space ieee transact pattern analysi machin intellig v n p may gunnar rtsch sebastian mika bernhard schlkopf klausrobert mller construct boost algorithm svm applic oneclass classif ieee transact pattern analysi machin intellig v n p septemb ron meir gunnar rtsch introduct boost leverag advanc lectur machin learn springerverlag new york inc new york ny