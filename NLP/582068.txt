t use scalea perform analysi distribut parallel program a paper give overview scalea new perform analysi tool openmp mpi hpf mix paralleldistribut program scalea instrument execut measur program comput varieti perform overhead base novel overhead classif sourc code hwprofil combin singl system significantli extend scope possibl overhead measur examin rang hwcounter number cach miss float point oper complex perform metric control loss parallel moreov scalea use new represent code region call dynam code region call graph enabl detail overhead analysi arbitrari code region instrument descript file use relat perform inform code region input program reduc instrument overhead sever experi realist code cover mpi openmp hpf mix openmpmpi code demonstr use scalea b introduct hybrid architectur eg smp cluster becom mainstay distribut parallel process market comput commun busili develop languag softwar tool machin besid openmp mpi hpf mix program paradigm openmpmpi increasingli evalu paper introduc new perform analysi system scalea distribut parallel program cover mention program paradigm scalea base novel classica tion perform overhead share distribut memori parallel program includ data move ment synchron control parallel addit comput loss parallel unidenti over head scalea among rst perform analysi tool combin sourc code hw prole singl system signicantli extend scope possibl overhead measur examin includ use hw counter cach analysi complex perform metric control loss parallel specic instrument perform analysi conduct determin categori overhead individu code region instrument done fulli automat usercontrol direct postexecut perform analysi done base perform tracel novel represent code region name dynam code region call graph drg drg ect dynam relationship code region subregion enabl detail overhead analysi everi code re gion drg restrict function call also cover loop io commun statement etc moreov allow arbitrari code region an alyz code region vari singl statement entir program unit contrast exist approach frequent use call graph consid function call prototyp scalea implement present sever experi realist program includ molecular dynam applic openmp version nancial model hpf openmpmpi version materi scienc code mpi version demonstr use scalea rest paper structur follow section describ overview scalea section present novel classic perform overhead base scalea instrument code analys perform dynam code region call graph describ next section experi shown section relat work outlin section conclus futur work discuss section scalea postexecut perform tool in strument measur analys perform behavior distribut memori share memori mix parallel program figur show architectur scalea consist two main compon scalea instrument system si post execut perform analysi tool set si integr vfc compil translat fortran program mpi openmp hpf mix program for tranmpi mix openmpmpi program input program scalea process compil frontend gener abstract syntax tree ast si enabl user select by direct commandlin option code region interest base preselect code region si automat insert probe code collect relev perform inform set proletrac le execut program target architectur si also gener instrument descript le see section enabl gather perform data relat back input program reduc instrument overhead si target perform measur system base tau perform framework tau integr toolkit perform instrument mea surement analysi parallel multithread pro gram tau measur librari provid portabl prole trace capabl support access hardwar counter si automat instrument parallel program vfc use tau instrument librari build abstract syntax tree vfc tau measur system creat dynam code region call graph see section main function si given follow automat instrument preden code region loop procedur io statement hpf independ loop openmp parallel loop openmp section mpi sendrec etc variou perform overhead use commandlin option manual instrument si direct insert program direct also allow dene user dene code region instrument control instrument overhead size perform data gather execut program postexecut analysi sisprofil sisoverhead pprof raci vampir dynam code region call graph raw perform data instrument descript file preprocess profiletrac file compil link instrument code automat instrument si manual instrument input program fortran mpi program openmp program hybrid program execut program user instrument control execut si runtim system sisprofil tausi papi load imbal time result hardwar counter result visual raci vampir target machin subselect select command visual perform data intermedi databas train set databas perform databas data repositori data object physic resourc data process data flow extern input control control flow diagram legend figur architectur scalea manual instrument turn ono prole given code region preprocess phase scalea lter extract relev perform inform prolestrac le yield ltere perform data dynam code region call graph drg drg ect dynam relationship code region subregion use precis overhead analysi everi individu subregion contrast exist approach base convent call graph consid function call code region postexecut perform analysi also employ train set method determin specic inform eg time penalti everi cach miss over head overhead probe time access lock etc everi target machin interest follow describ scalea instrument system instrument descript le detail si scalea postexecut perform analysi found scalea instrument system base userprovid commandlin option di rectiv si insert instrument code program collect perform data inter est si support programm control prol ingtrac gener perform data select instrument specic code region type loop procedur io statement hpf independ loop openmp parallel loop openmp section openmp critic mpi barrier state ment etc si also enabl instrument arbitrari code region final instrument turn specic instrument direct order measur arbitrari code region si provid follow instrument code region direct si cr begin si cr end must be respect insert programm region start nish note sever entri exit node code re gion appropri direct must insert idf entri descript id code region identi type code region type le sourc le identi unit program unit identi enclos region line start line number region start column start column number start line end line number end column end column number end perform data perform data collect comput region aux auxiliari inform tabl content instrument descript le idf programm everi entri exit node given code region altern compil analysi use automat determin entri exit node furthermor si provid specic direct order control tracingprol direct measur enabl measur disabl allow programm turn tracingprol program code region instanc follow exampl instrument portion openmp price code version see section sake demonstr call function random path measur use facil control prolingtrac mention abov end note si direct insert programm base scalea automat instrument code instrument descript file crucial aspect perform analysi relat perform inform back origin input program instrument program si gener instrument descript le idf correl prole trace overhead inform correspond code region idf maintain everi instrument code region varieti inform see tabl code region type describ type code re gion instanc entir program outermost loop read statement openmp section openmp parallel loop mpi barrier etc program unit correspond subroutin function enclos code region idf entri perform data actual link separ repositori store inform note inform store idf actual made runtim data structur comput perform overhead properti execut program idf also help keep instrument code minim everi probe insert singl identi allow relat associ probe timer counter correspond code region classic tempor accord amdahl law theoret best sequenti algorithm take time nish program p time requir execut parallel version p processor tempor overhead parallel program dene ect dierenc achiev optim parallel divid u overhead identi u overhead fraction could analyz detail theori never neg impli speedup t p never exceed p how ever practic occur tempor overhead becom neg due super linear speedup applica tion eect commonli caus increas avail cach size figur give classic tempor overhead base perform analysi scalea conduct tempor overhead data movement local memori access remot memori access level level level level level n level sendrec putget synchronis barrier lock condit variabl control parallel schedul inspector executor forkjoin addit comput algorithm chang compil chang frontend normal loss parallel unparallelis code replic code partial parallelis code unidentifi figur tempor overhead classic data movement correspond data transfer within singl address space process local memori access process remot memori access synchron eg barrier lock use coordin process thread access data maintain consist comput data etc control parallel eg forkjoin oper loop schedul use control manag parallel program caus runtim librari user compil oper addit comput ect chang origin sequenti program includ algorithm compil chang increas parallel eg elimin data depend data local eg chang data access pattern loss parallel due imperfect parallel program classi follow unparallel code execut one processor replic code execut proce sor partial parallel code execut one processor unidenti overhead correspond overhead cover categori note mention classic stimul dier sever respect synchron part inform movement load imbal separ overhead local remot memori merg singl overhead class loss parallel split two class unidenti overhead consid all load imbal opinion overhead repres perform properti caus one overhead dynam code region call graph everi program consist set code region rang singl statement entir program unit code region be respect enter exit multipl entri exit control ow point see figur case howev code region singleentri singleexit code region order measur execut behavior code region instrument system detect entri exit node code region insert probe node basic task done support compil guid manual insert direct figur show exampl code region entri exit node select arbitrari code region user respect mark two statement entri exit statement time entri exit node code region eg use si direct compil analysi si automat tri determin entri exit node code region node repres statement program figur show exampl code region multipl entri exit node instrument tri detect node automat insert probe entri exit node respect code region overlap scalea current support instrument overlap code re gion current implement scalea support mainli instrument singleentri multipleexit code region enhanc si support also multipleentri multipleexit code region dynam code region call graph scalea set preden code region classi common eg program procedur loop function call statement program paradigm specic code region mpi call hpf independ loop openmp parallel region loop section etc moreov si provid direct dene arbitrari code region see section input program base code region dene new data structur call dynam code region call graph drg dynam code region call graph drg program q dene direct ow graph set node r set edg e node r r repres code region execut least runtim q edg r indic code region r call insid r execut q r dynam subregion r rst code region execut execut q dene s drg use key data structur conduct detail perform overhead analysi scalea notic time overhead code region r explicitli instrument subregion r r n given r time overhead explicitli instrument code region r n start r correspond overhead begin eg fork thread redistribut data etc end join thread barrier synchron process reduct oper etc r remain correspond code region explicitli instrument entri point statement begin select code region statement end select code region exit point statement control flow control flow exit point entri point figur code region sever entri exit point howev easili comput remain region r instrument well figur show excerpt openmp code togeth associ drg call graph techniqu wide use perform analysi tool vampir gprof cxperf support call graph show much time spent function children call graph use improv search strategi autom perform diagnosi howev node call graph tool repres function call contrast drg dene node arbitrari code region eg function function call loop statement etc integerx an print input n read n call call in a end call call sisf_start call sisf_stop call end program r r r r figur openmp code excerpt drg gener build dynam code region call graph call code region r insid code region r execut program establish parentchildren relationship r r instrument librari captur relationship maintain execut program code region r call insid r data entri repres relationship r r gener store appropri prolestrac le code region r encount child code region eg code region execut rst abstract code region assign parent everi code region uniqu identi includ probe insert si store instrument descript le drg data structur maintain inform code region instrument execut everi thread process build maintain subdrg execut preprocess phase cf figur drg applic built base individu subdrg thread subdrg thread comput process prolestrac le contain perform data thread algorithm gener drg describ detail figur execut time md applic experi implement prototyp scalea control commandlin option user direct code region includ arbitrari code region select specic si direct insert input program tempor perform overhead accord classic shown figur select commandlin option visual capabl current restrict textual out put plan build graphic user interfac end graphic output except tabl follow experi gener manual inform use si postexecut analysi found overhead cpu cpu cpu loss parallel control parallel synchron total execut time tabl overhead sec md applic identi unidenti total overhead respect section present sever experi demonstr use scalea experi conduct gescher smp cluster smp node connect fasteth ernet compris intel pentium iii xeon mhz cpu mb fullspe l cach gbyte ecc ram intel profast ethernet ul tra gb hard disk run linux smp patch perfctr hardwar counter perform use mpich pgf compil version portland group inc molecular dynam md applica tion md program implement simpl molecular dynam simul continu real space program obtain implement openmp program written bill magro kuck associ inc kai perform md applic measur singl smp node gescher figur tabl show execut time behavior measur overhead respect result demonstr good speedup behavior nearli linear see tabl total overhead small larg portion tempor overhead identi time sequenti code region unpar leliz chang alway execut figur l cach missescach access ratio omp region md applic one processor loss parallel unparallel code region r program q dene r processor use execut q r sequenti execut time r increas p easili shown loss parallel increas well also conrm measur shown tabl control parallel mostli caus loop schedul actual decreas increas number processor possibl explan eect larger number processor master thread process less loop schedul phase smaller number processor load balanc improv increas number processorsthread one smp node time decreas synchron time examin cach miss ratio dene number l cach miss divid number l cach access two import omp code region name omp comput omp updat shown figur ratio nearli use singl processor impli good cach behavior sequenti execut code data seem l cach case howev parallel version cach miss ratio increas substanti thread process data global array kept privat l cach cach coher protocol caus mani cach line figur execut time hpf openmpmpi version backward price applic exchang privat cach induc cach miss unclear howev master thread consider higher cach miss ratio thread overal cach behavior littl impact speedup code backward price applic backward price code implement backward induct algorithm comput price interest rate depend nancial product variabl coupon bond two parallel code version creat first hpf version exploit data parallel compil mpi program second mix version combin hpf openmp latter version vfc gener openmpmpi program hpf direct use distribut data onto set smp node within node openmp program execut commun among smp node realiz mpi call execut time version shown figur term all legend denot entir program wherea loop refer main comput loop hpf independ loop openmp parallel loop version re spectiv hpf version perform wors openmpmpi version show almost linear speedup node overal processor tabl display overhead hpf mix openmpmpi version respect case largest overhead caus control parallel overhead rise signicantli hpf version increas number node eect less sever openmpmpi version order nd caus high control parallel overhead use scalea determin individu compon overhead see tabl two routin updat halo mpi init mainli respons high control parallel overhead version updat halo updat overlap area distribut array caus commun one process requir data own anoth process dierent node mpi init initi mpi runtim system also involv commun version impli much higher overhead two routin compar openmpmpi reason employ separ process everi cpu smp node wherea openmpmpi version use one process per node lapw lapw materi scienc program calcul eectiv potenti kohnsham eigenvalu problem lapw implement fortran mpi code run across sever smp node pgf compil take care exchang data processor within across smp node use scalea local import code region lapw subdivid sequenti code region fft rean fft rean fft rean parallel code region interstiti potenti loop energi output execut time behavior speedup base sequenti execut time code region code region shown figur respect lapw examin problem size atom distribut onto processor set smp node clearli use processor cant reach optim load balanc wherea processor display much better load imbal eect conrm scalea see figur comput intens routin lapw interstiti potenti loop scale poorli due load imbal larg overhead due loss parallel data move ment synchron see tabl lapw use mani bla scalapack librari call current instrument scalea reason larg fraction unidenti overhead see tabl main sourc control parallel overhead caus mpi init see figur scalea also discov main subroutin caus loss parallel overhead fft rean fft rean fftp rean sequenti relat work parav perform analysi tool openmpmpi tool dynam instrument binari code determin variou perform param eter tool cover rang perform overhead support scalea moreov tool use dynam intercept mechan commonli problem relat perform data back input program ovaltin measur analys varieti perform overhead fortran openmp program paradyn automat perform analysi tool use dynam instrument search perform bottleneck base specic languag function call graph employ improv perform tune recent work openmp perform interfac base direct rewrit similar si instrument approach scalea scala predecessor system scalea implement interfac eg perform measur librari tau allow prole trace per form conceiv interfac could use gener perform data rest scalea system could analyz tau perform framework integr toolkit perform instrument mea surement analysi parallel multithread pro gram scalea use tau instrument librari one trace librari papi speci standard api access hardwar perform counter avail modern mi croprocessor scalea use papi librari measur hardwar counter gprof compilerbas prole framework mostli analys execut behavior count function function call vampir perform analysi tool process trace le gener vampirtrac support variou perform display includ timelin static visual togeth call graph sourc code conclus futur work paper describ scalea perform analysi system distribut parallel pro gram scalea current support perform analysi openmp mpi hpf mix parallel program eg openmpmpi scalea base novel classic perform overhead share distribut memori parallel program scalea among rst perform analysi tool combin sourc code hw prole singl system signicantli extend scope possibl overhead measur examin specic instrument perform analysi conduct determin categori overhead individu code region instrument done fulli automat usercontrol direct postexecut perform analysi done base perform tracel novel represent code region name dynam code region call graph drg drg ect dynam relationship code region subregion enabl detail overhead analysi everi code region drg restrict function call also cover loop io commun statement etc more over allow analyz arbitrari code region vari singl statement entir program unit processor sequenti n p n p n p n p np np np data movement control parallel tu total execut time tabl overhead hpf version backward price applic u identi unidenti total overhead respect n p mean smp node processor processor n p n p n p n p np np np inspector work distribut updat halo mpi init tabl control parallel overhead hpf version backward price applic contrast exist approach frequent use call graph consid function call base prototyp implement scalea present sever experi realist code implement mpi hpf mix openmpmpi experi demonstr use scalea nd perform problem caus current integr scalea databas store deriv perform data moreov plan enhanc scalea perform specic languag order support automat perform bottleneck analysi sisprofil measur librari drg overhead prole extens tau prol ing capabl hope integr featur futur releas tau perform system featur oer portabl instrument tool access api r valid singl processor approach achiev larg scale comput capabili tie automat overhead pro vfc vienna fortran compil scalabl crossplatform infrastructur applic perform tune use hardwar counter hierarch classi callgraphbas search strategi autom perform diagnosi price constant matur floater embeed option use mont carlo simul gnu gprof call graph execut pro mpi standard messag pass ing cxperf user guid high perform fortran forum introduct parallel comput ingdesign analysi parallel algorithm perform technolog complex parallel distribut sy tem toward perform tool interfac openmp approach base direct rewrit vampir visual analysi mpi resourc vampirtrac instal user guid portabl pro gescher system scalea version user guid tr introduct parallel comput highperform portabl implement mpi messag pass interfac standard portabl profil trace parallel scientif applic use c executiondriven perform analysi distribut parallel system perform technolog complex parallel distribut system scalabl crossplatform infrastructur applic perform tune use hardwar counter paradyn parallel perform measur tool callgraphbas search strategi autom perform diagnosi distinguish paper hierarch classif overhead parallel program mpi standard messag pass gprof ctr thoma fahring clvi seragiotto jnior model detect perform problem distribut parallel program javapsl proceed acmiee confer supercomput cdrom p novemb denver colorado ming wu xianh sun grid harvest servic perform system grid comput journal parallel distribut comput v n p octob