t factori hidden markov model a hidden markov model hmm proven one wide use tool learn probabilist model time seri data hmm inform past convey singl discret variableth hidden state discuss gener hmm state factor multipl state variabl therefor repres distribut manner describ exact algorithm infer posterior probabl hidden state variabl given observ relat forwardbackward algorithm hmm algorithm gener graphic model due combinatori natur hidden state represent exact algorithm intract intract system approxim infer carri use gibb sampl variat method within variat framework present structur approxim state variabl decoupl yield tractabl algorithm learn paramet model empir comparison suggest approxim effici provid accur altern exact method final use structur approxim model bach choral show factori hmm captur statist structur data set unconstrain hmm cannot b introduct due flexibl simplic effici paramet estim algorithm hidden markov model hmm emerg one basic statist tool model discret time seri find widespread applic area speech recognit rabin juang comput molecular biolog krogh brown mian sjoland haussler hmm essenti mixtur model encod inform histori time seri valu singl multinomi variableth hidden statewhich take one k discret valu multinomi assumpt support effici paramet estim algorithmth baumwelch algorithmwhich consid k set hidden state time step howev multinomi assumpt also sever limit represent capac hmm exam ple repres bit inform histori time sequenc hmm would need distinct state hand hmm distribut state represent could achiev task binari state z ghahramani mi jordan variabl william hinton paper address problem construct effici learn algorithm hidden markov model distribut state represent need distribut state represent hmm motiv two way first represent let model automat decompos state space featur decoupl dynam process gener data second distribut state represent simplifi task model time seri known priori gener interact multipl looselycoupl process exampl speech signal gener superposit multipl simultan speaker potenti model architectur william hinton first formul problem learn hmm distribut state represent propos solut base determinist learn approach present paper similar william hinton also view framework statist mechan mean field theori howev learn algorithm quit differ make use special structur hmm distribut state represent result significantli effici learn procedur anticip result section learn algorithm obviat need twophas procedur boltzmann machin exact step make use forwardbackward algorithm classic hmm subroutin differ approach come saul jordan deriv set rule comput gradient requir learn hmm distribut state space howev method appli limit class architectur hidden markov model distribut state represent particular class probabilist graphic model pearl lauritzen spiegelhalt repres probabl distribut graph node correspond random variabl link repres condit independ relat relat hidden markov model graphic model recent review smyth heckerman jordan although exact probabl propag algorithm exist gener graphic model jensen lauritzen olesen algorithm intract denselyconnect model one consid paper one approach deal issu util stochast sampl method kanazawa et al anoth approach provid basi algorithm describ current paper make use variat method cf saul jaakkola jordan follow section defin probabilist model factori hmm section present algorithm infer learn section describ empir result compar exact approxim algorithm learn basi time complex model qualiti also appli factori hmm real time seri data set consist melodi line collect choral j s bach discuss sever gener probabilist model factori hidden markov model t t t t a b figur a direct acycl graph dag specifi condit independ relat hidden markov model node condit independ nondescend given parent b dag repres condit independ relat factori hmm underli markov chain section conclud section necessari detail deriv provid appendix probabilist model begin describ hidden markov model sequenc observ model specifi probabilist relat observ sequenc hidden state fs g markov transit structur link hidden state model assum two set condit independ relat independ observ state given independ markov properti use independ relat joint probabl sequenc state observ factor condit independ specifi equat express graphic form figur a state singl multinomi random variabl take one k discret valu kg state transit probabl specifi k theta k transit matrix observ discret symbol take one valu observ probabl fulli specifi k theta observ matrix continu observ vector p y js model mani differ form gaussian mixtur gaussian even neural network present paper gener hmm state represent let state repres collect state variabl z ghahramani mi jordan take k m valu refer model factori hidden markov model state space consist cross product state variabl simplic assum k although result present trivial gener case differ k m given state space factori hmm consist k combin variabl place constraint state transit structur would result k theta k transit matrix unconstrain system uninterest sever reason equival hmm k state unlik discov interest structur k state variabl variabl allow interact arbitrarili time complex sampl complex estim algorithm exponenti therefor focu factori hmm underli state transit constrain natur structur consid one state variabl evolv accord dynam priori uncoupl state variabl graphic represent model present figur b transit structur system repres distinct k theta k matric gener allow coupl state variabl briefli discuss section shown figur b factori hmm observ time step depend state variabl time step continu observ one simpl form depend linear gaussian is observ gaussian random vector whose mean linear function state variabl repres state variabl k theta vector k discret valu correspond one posit elsewher result probabl densiti theta observ vector ae oe w m matrix theta k matrix whose column contribut mean set m c theta covari matrix denot matrix transpos j delta j matrix determin oper one way understand observ model equat a b consid margin distribut obtain sum possibl state k set state variabl thu factori hidden markov model k possibl mean vector obtain form sum column one column chosen w m matric result margin densiti thu gaussian mixtur model k gaussian mixtur compon constant covari matrix c static mixtur model without inclus time index markov dynam factori parameter standard mixtur gaussian model interest right zemel hinton zemel ghahramani model consid current paper extend model allow markov dynam discret state variabl underli mixtur unless otherwis state assum gaussian observ model throughout paper hidden state variabl one time step although margin independ becom condit depend given observ sequenc determin appli semant direct graph particular dsepar criterion pearl graphic model figur b consid gaussian model equat ab given observ vector posterior probabl set hidden state variabl proport probabl gaussian mean sinc function state variabl probabl set one state variabl depend set state variabl depend effect coupl hidden state variabl purpos calcul posterior probabl make exact infer intract factori hmm infer learn infer problem probabilist graphic model consist comput probabl hidden variabl given observ context speech recognit exampl observ may acoust vector goal infer may comput probabl particular word sequenc phonem the hidden state problem solv effici via forwardbackward algorithm rabin juang shown special case jensen lauritzen olesen algorithm probabl propag gener graphic model smyth et al case rather probabl distribut hidden state desir infer singl probabl hidden state sequenc achiev via viterbi algorithm form dynam program close relat forwardbackward algorithm also analogu graphic model literatur dawid learn problem probabilist model consist two compon learn structur model learn paramet structur learn topic current research graphic model machin learn commun eg heckerman stolck omohundro current paper deal exclus problem learn paramet given structur z ghahramani mi jordan em algorithm paramet factori hmm estim via expect maxim em algorithm dempster laird rubin case classic hmm known baumwelch algorithm baum petri soul weiss procedur iter step fix current paramet comput posterior probabl hidden state the e step step use probabl maxim expect log likelihood observ function paramet the step sinc e step em exactli infer problem describ abov subsum discuss infer learn problem descript em algorithm factori hmm em algorithm follow definit expect log likelihood complet observ hidden data qoe new log q function paramet oe new given current paramet estim oe observ sequenc fy g factori hmm paramet model consist comput q expand use equat b find q express function three type expect hidden state variabl hs m i i hdeltai use abbrevi e fdeltajo fy gg hmm notat rabin juang hs m correspond fl vector state occup probabl hs m correspond k theta k matrix state occup probabl two consecut time step hs m analogu singl underli markov model step use expect maxim q function oe new use jensen inequ baum petri soul weiss show iter e step increas likelihood p fy gjoe converg local optimum hidden markov model exact step factori hmm simpl tractabl particular step paramet output model describ equat ab found solv weight linear regress problem similarli step prior m state transit matric ident one use baumwelch algorithm detail step given appendix a turn substanti difficult problem comput expect requir e step exact infer unfortun exact e step factori hmm comput intract fact best shown make refer standard algorithm prob factori hidden markov model abilist infer graphic model lauritzen spiegelhalt although also deriv readili direct applic bay rule consid comput requir calcul posterior probabl factori hmm shown figur b within framework lauritzen spiegelhalt algorithm moral triangul graphic structur factori hmm result junction tree in fact chain cliqu size m result probabl propag algorithm time complex otmk m singl observ sequenc length present forwardbackward type recurs implement exact e step appendix b naiv exact algorithm consist translat factori hmm equival hmm k state use forwardbackward algorithm time complex otk like model multipl denselyconnect hidden variabl exponenti time complex make exact learn infer intract thu although markov properti use obtain forwardbackward like factor expect across time step sum possibl configur hidden state variabl within time step unavoid abl intract due inher cooper natur model gaussian output model exampl set state variabl one time step cooper determin mean observ vector infer use gibb sampl rather comput exact posterior probabl one approxim use mont carlo sampl procedur therebi avoid sum exponenti mani state pattern cost accuraci although mani possibl sampl scheme for review see neal present one simplestgibb sampl geman geman given observ sequenc fy g procedur start random set hidden state fs g step sampl process state vector updat stochast accord probabl distribut condit set state vector graphic model use here node condit independ node given markov blanket defin set children parent parent children node sampl typic state variabl m need examin state neighbor node sampl p s m sampl tm hidden variabl model result new sampl hidden state model requir otmk oper sequenc overal state result pass gibb sampl defin markov chain state space model assum probabl bound away zero markov chain guarante converg z ghahramani mi jordan posterior probabl state given observ geman geman thu suitabl time sampl markov chain taken approxim sampl posterior probabl first secondord statist need estim hs m collect use state visit probabl estim sampl process use approxim e step em complet factor variat infer also exist second approxim posterior probabl hidden state tractabl determinist basic idea approxim posterior distribut hidden variabl p fs gjfi g tractabl distribut qf g approxim provid lower bound log likelihood use obtain effici learn algorithm argument formal follow reason saul jaakkola jordan distribut hidden variabl qf g use defin lower bound log likelihood log log qf g log made use jensen inequ last step differ lefthand side righthand side inequ given kullbackleibl diverg cover thoma qf g log complex exact infer approxim given q determin condit independ relat paramet thu chose q tractabl structurea graphic represent elimin depend p given structur free vari paramet q obtain tightest possibl bound minim refer gener strategi use parameter approxim distribut variat approxim refer free paramet distribut variat paramet illustr consid simplest variat approxim state variabl assum independ given observ distribut written factori hidden markov model a b figur a complet factor variat approxim assum state variabl independ condit observ sequenc b structur variat approxim assum state variabl retain markov structur within chain independ across chain qs m variat paramet g mean state variabl where befor state variabl m repres kdimension vector k th posit elsewher th markov chain state k time t element vector m therefor defin state occup probabl multinomi variabl m distribut q qs m tk tk complet factor approxim often use statist physic provid basi simpl yet power mean field approxim statist mechan system parisi make bound tight possibl vari separ observ sequenc minim kl diverg take deriv respect m set zero obtain set fix point equat see appendix c defin new ae m oe m residu error given predict state variabl includ m m m z ghahramani mi jordan delta m vector diagon element w m c fdeltag softmax oper map vector vector b size element log p m denot elementwis logarithm transit matrix p m first term a project error reconstruct observ onto weight state vector mthe particular set state vector reduc error larger associ variat paramet second term aris fact second order correl hs m evalu variat distribut diagon matrix compos element m last two term introduc depend forward backward time therefor although posterior distribut hidden variabl approxim complet factor distribut fix point equat coupl paramet associ node paramet markov blanket sens fix point equat propag inform along pathway defin exact algorithm probabl propag follow may provid intuit interpret approxim made distribut given particular observ sequenc hidden state variabl markov chain time step stochast coupl stochast coupl approxim system hidden variabl uncorrel coupl mean variat meanfield equat solv determinist coupl mean best approxim stochast coupl system hidden state vector updat turn use a time complex otmk per iter converg determin monitor kl diverg variat distribut success time step practic converg rapid about iter a fix point equat converg expect requir e step obtain simpl function paramet equat cc appendix c structur variat infer approxim present previou section factor posterior probabl state variabl statist independ contrast rather extrem factor exist third approxim tractabl preserv much probabilist structur origin system scheme factori hmm approxim uncoupl hmm shown figur b within hmm effici exact infer implement via forwardbackward algorithm approach exploit tractabl substructur first suggest machin learn literatur saul jordan factori hidden markov model note argument present previou section hing form approxim distribut therefor structur variat approxim obtain use structur variat distribut q q provid lower bound log likelihood use obtain learn algorithm write structur variat approxim qs m qs m zq normal constant ensur q integr one qs m qs m tk tk tk tk last equal follow fact m vector one posit elsewher paramet distribut gthe origin prior state transit matric factori hmm timevari bia state variabl compar equat ac equat see k theta vector h m play role probabl observ p y js k set m exampl qs m joe correspond observ time state m j intuit approxim uncoupl markov chain attach state variabl distinct fictiti observ probabl fictiti observ vari minim kl diverg q p appli argument befor obtain set fix point equat h m minim klqkp detail appendix d h m new ae m oe delta m defin befor redefin residu error m m z ghahramani mi jordan paramet h m obtain fix point equat observ probabl associ state variabl m hidden markov model m use probabl forwardbackward algorithm use comput new set expect hs m i fed back a b algorithm therefor use subroutin minim kl diverg note similar equat ab equat ab complet factor system complet factor system sinc hs m fix point equat written explicitli term variat paramet structur approxim depend hs m h m comput via forwardbackward algorithm note also a contain term involv prior m transit matrix p m term cancel choic approxim choic approxim theori em algorithm present dempster et al assum use exact e step model exact e step intract one must instead use approxim like describ choic among approxim must take account sever theoret practic issu mont carlo approxim base markov chain gibb sampl offer theoret assur sampl procedur converg correct posterior distribut limit although mean one come arbitrarili close exact e step practic converg slow especi multimod distribut often difficult determin close one converg howev sampl use e step em time tradeoff number sampl use number em iter seem wast wait converg earli learn posterior distribut sampl drawn far posterior given optim paramet practic found even approxim step use gibb sampl eg around ten sampl hidden variabl tend increas true likelihood variat approxim offer theoret assur lower bound likelihood maxim minim kl diverg e step paramet updat step guarante decreas lower bound therefor converg defin term bound altern view given neal hinton describ em term neg free energi f function paramet oe observ posterior probabl distribut hidden variabl qs eq denot expect use distribut qs exact e step em maxim f respect q given oe variat e step use factori hidden markov model maxim f respect q given oe subject constraint q particular tractabl form given view seem clear structur approxim prefer complet factor approxim sinc place fewer constraint q cost tractabl experiment result investig learn infer factori hmm conduct two experi ment first experi compar differ approxim exact method infer basi comput time likelihood model obtain synthet data second experi sought determin whether decomposit state space factori hmm present advantag model real time seri data set might assum complex intern structurebach choral melodi experi perform time benchmark use data gener factori hmm structur underli markov model k state each compar time per em iter train test set likelihood five model ffl hmm train use baumwelch algorithm ffl factori hmm train exact infer e step use straightforward applic forwardbackward algorithm rather effici algorithm outlin appendix b ffl factori hmm train use gibb sampl e step number sampl fix sampl per variabl ffl factori hmm train use complet factor variat approxima tion ffl factori hmm train use structur variat approxim factori hmm consist underli markov model k state each wherea hmm k state data gener factori hmm structur state variabl could take k discret valu paramet model except output covari matrix sampl uniform distribut appropri normal satisfi sumtoon constraint transit matric prior covari matrix set multipl ident matrix train test set consist sequenc length observ fourdimension vector randomli sampl set paramet separ train set test set gener algorithm run onc z ghahramani mi jordan fifteen set paramet gener four problem size algorithm run maximumof iter em converg defin iter k log likelihood lk approxim log likelihood approxim algorithm use satisfi end learn log likelihood train test set comput model use exact algorithm also includ comparison log likelihood train test set true model gener data test set log likelihood n observ sequenc defin log p y n obtain maxim log likelihood train set disjoint test set provid measur well model gener novel observ sequenc distribut train data result averag run algorithm four problem size a total run present tabl even smallest problem size standard hmm k state suffer overfit test set log likelihood significantli wors train set log likelihood expect overfit problem becom wors size state space increas particularli seriou factori hmm log likelihood three approxim em algorithm compar exact algorithm gibb sampl appear poorest perform three smaller size problem log likelihood significantli wors exact algorithm train set test set p may due insuffici sampl howev soon see run gibb sampler sampl potenti improv perform make substanti slower variat method surprisingli gibb sampler appear quit well largest size problem although differ method statist signific perform complet factor variat approxim statist significantli differ exact algorithm either train set test set problem size perform structur variat approxim statist differ exact method three four problem size appear better one problem size although result may fluke aris random variabl anoth interest and specul explan exact em algorithm implement unconstrain maxim f defin section variat method maxim f subject constrain distribut constraint could presum act regular reduc overfit larg amount variabl final log likelihood model learn algorithm subtract log likelihood true gener model train model elimin main effect randomli sampl gener model reduc variabl due train test set one import remain sourc varianc random seed use factori hidden markov model tabl comparison factori hmm four problem vari size neg log likelihood train test set plu minu one standard deviat shown problem size algorithm measur bit per observ log likelihood bit divid nt rel log likelihood true gener model data set true true gener model the log likelihood per symbol defin zero model measur hmm hidden markov model k state exact factori hmm train use exact e step gibb factori hmm train use gibb sampl cfva factori hmm train use complet factor variat approxim sva factori hmm train use structur variat approxim k algorithm train test hmm sigma sigma exact sigma sigma gibb sigma sigma cfva sigma sigma sva sigma sigma hmm sigma sigma exact sigma sigma gibb sigma sigma cfva sigma sigma exact sigma sigma gibb sigma sigma cfva sigma sigma exact sigma sigma gibb sigma sigma cfva sigma sigma z ghahramani mi jordan iter em log likelihood bit a iter em log likelihood bit b iter em log likelihood bit c iter em log likelihood bit d figur learn curv five run four learn algorithm factori hmm a exact b complet factor variat approxim c structur variat approx imat d gibb sampl singl train set sampl size use run solid line show neg log likelihood per observ in bit rel true model gener data calcul use exact algorithm circl denot point converg criterion met run end three approxim algorithm dash line show approxim neg log likelihood train run determin initi paramet sampl use gibb algorithm algorithm appear sensit random seed suggest differ run train set found differ local maxima plateau likelihood figur variabl could elimin explicitli ad regular term view prior paramet maximuma posteriori paramet estim altern bayesian or ensembl method could use averag variabl integr paramet space time comparison confirm fact standard hmm exact extrem slow model larg state space fig factori hidden markov model timeiter hmm figur time per iter em silicon graphic r processor run matlab ure gibb sampl slower variat method even limit ten sampl hidden variabl per iter em sinc one pass variat fix point equat time complex one pass gibb sampl sinc variat fix point equat found converg quickli experi suggest gibb sampl competit timewis variat method time per iter variat method scale well larg state space experi bach choral music piec natur exhibit complex structur mani differ time scale furthermor one imagin repres state music piec given time would necessari specifi conjunct mani differ featur reason chose test whether factori hmm would provid advantag regular hmm model collect music piec data set consist discret event sequenc encod melodi line j s bach choral obtain uci repositori machin learn origin discuss conklin witten event sequenc repres six attribut describ tabl sixtysix choral event each divid train set choral test set choral use first set hidden markov model state space rang state train converg sigma step em factori hmm vari size k rang rang also train data z ghahramani mi jordan tabl attribut bach choral data set key signatur time signatur attribut constant durat choral attribut treat real number model lineargaussian observ a attribut descript represent pitch pitch event int fermata event fermata binari st start time event int note dur durat event int note approxim e step factori hmm use structur variat ap proxim choic motiv three consider first size state space wish explor exact algorithm prohibit slow second gibb sampl algorithm appear present advantag speed perform requir heurist method determin number sampl third theoret argument suggest structur approxim gener superior complet factor variat approxim sinc depend origin model preserv test set log likelihood hmm shown figur a exhibit typic ushap curv demonstr tradeoff bia varianc ge man bienenstock doursat hmm fewer state predict well hmm state overfit train data therefor provid poor model test data run highest test set log likelihood per observ gamma bit obtain hmm hidden state factori hmm provid satisfactori model choral three point view first time complex possibl consid model significantli larger state space particular fit model state second given componenti parametr factori hmm larg state space requir excess larg number paramet rel number data point particular saw evid overfit even largest factori hmm seen figur c d final approach result significantli better predictor test set likelihood best factori hmm order magnitud larger test set likelihood best hmm figur d reveal factori hmm clearli better predictor singl hmm acknowledg neither approach produc model easili interpret musicolog point view situat reminisc speech recognit hmm prove valu predict model speech signal without necessarili view causal gener model speech factori hmm clearli impoverish represent factori hidden markov model log likelihood bit a b size state space c d figur test set log likelihood per event bach choral data set function number state a hmm factori hmm b dash line line symbol repres singl run line indic mean perform thin dash line bd indic log likelihood per observ best run a factori hmm train use structur approxim method true likelihood comput use exact algorithm music structur promis perform predictor provid hope could serv step way toward increasingli structur statist model music complex multivari time seri gener model section describ four variat gener factori hmm discret observ probabilist model present paper assum realvalu gaussian observ one advantag aris assumpt condit densiti ddimension observ p y js compactli specifi mean matric dimens theta k one theta covari matrix furthermor step model reduc set weight least squar equat model gener handl discret observ sever way consid singl dvalu discret observ analog tradit hmm output probabl could model use matrix howev case factori hmm matrix would theta k entri combin state variabl observ thu compact represent would entir lost standard method graphic model suggest approxim larg matric noisyor pearl sigmoid neal model interact exampl softmax model gener sigmoid model p y js multinomi mean proport z ghahramani mi jordan exp like gaussian model specif com pact use matric size theta k as lineargaussian model w m overparametr sinc model overal mean shown appendix a nonlinear induc softmax function make e step step algorithm difficult iter numer method use step wherea gibb sampl variat method use e step see neal hinton et al saul et al discuss differ approach learn sigmoid network introduc coupl architectur factori hmm present section assum underli markov chain interact observ constraint relax introduc coupl hidden state variabl cf saul jordan exampl m depend m equat replac follow factor similar exact variat gibb sampl procedur defin architectur howev note coupl must introduc caution may result exponenti growth paramet exampl factor requir transit matric size k theta k rather specifi higherord coupl probabl transit matric one introduc secondord interact term energi log probabl function term effect coupl chain without number paramet incur full probabl transit matrix graphic model formal correspond symmetr undirect link make model chain graph jensen lauritzen olesen algorithm still use propag inform exactli chain graph undirect link caus normal constant probabl distributionth partit functionto depend coupl paramet boltzmann machin hinton sejnowski clamp unclamp phase therefor requir learn goal unclamp phase comput deriv partit function respect paramet neal condit input like hidden markov model factori hmm provid model uncondit densiti observ sequenc certain problem domain observ better thought input explanatori variabl factori hidden markov model other output respons variabl goal case model condit densiti output sequenc given input sequenc machin learn terminolog uncondit densiti estim unsupervis condit densiti estim supervis sever algorithm learn hidden markov model condit input recent present literatur cacciator nowlan bengio frasconi meila jordan given sequenc input vector g probabilist model inputcondit factori hmm theta model depend specif p y js m condit discret state variabl possibl con tinuou input vector approach use bengio frasconi input output hmm iohmm suggest model p s m separ neural network one set m decomposit ensur valid probabl transit matrix defin point input space sumtoon constraint eg softmax nonlinear use output network use decomposit condit probabl k network infer inputcondit factori hmm straightforward gener algorithm present factori hmm exact forwardbackward algorithm appendix b adapt use appropri condit probabl similarli gibb sampl procedur complex condit input final complet factor structur approxim also gener readili approxim distribut depend input similar model probabl transit structur decompos abov complex depend previou state variabl input infer may becom consider complex depend form input condit maxim step learn may also chang consider gener output transit probabl model neural network step longer solv exactli gradientbas gener em algorithm must use loglinear model step solv use inner loop iter reweight leastsquar mccullagh nelder hidden markov decis tree interest gener factori hmm result one condit input x order state variabl m depend n z ghahramani mi jordan figur hidden markov decis tree figur result architectur seen probabilist decis tree markovian dynam link decis variabl consid probabilist model would gener data first time step given top node take k valu stochast partit x space k decis region next node hierarchi subdivid region k subregion on output gener input x kway decis hidden node next time step similar procedur use gener data model except decis tree depend decis taken node previou time step thu hierarch mixtur expert architectur jordan jacob gener includ markovian dynam decis hidden markov decis tree provid use start point model time seri tempor spatial structur multipl resolut explor gener factori hmm jordan ghahramani saul conclus paper examin problem learn class gener hidden markov model distribut state represent gener provid richer model tool method incorpor prior structur inform state variabl underli dynam system gener data although exact infer class model gener intract provid structur variat approxim comput tractabl approxim form basi expect step em algorithm learn paramet model empir comparison sever approxim exact algorithm show approxim effici comput accur final shown factori hidden markov model factori hmm represent provid advantag tradit hmm predict model complex tempor pattern bach choral appendix step step equat paramet obtain set deriv q respect paramet zero start expand q use equat b tr c tr log p m hs m log z a tr trace oper squar matric z normal term independ state observ ensur probabl sum one set deriv q respect output weight zero obtain linear system equat w a assum dtheta vector let mk theta vector obtain concaten m vector w theta mk matrix obtain concaten w m matric of size theta k solv a result moorepenros pseudoinvers note model overparameter sinc theta mean w m matric add singl mean use pseudoinvers remov need explicitli subtract overal mean w m estim separ anoth paramet estim prior solv q subject constraint sum one obtain z ghahramani mi jordan similarli estim transit matric solv qp subject constraint column p m sum one element i new final reestim equat covari matrix deriv take deriv respect c gamma first term aris normal gaussian densiti function z proport jcj t jcjc substitut a reorgan get equat reduc baumwelch reestim equat hmm gaussian observ step present case singl observ sequenc extens multipl sequenc straightforward appendix exact forwardbackward algorithm specifi exact forwardbackward recurs comput posterior probabl hidden state factori hmm differ straightforward applic forwardbackward algorithm equival k state hmm depend k theta k transit matrix rather make use independ underli markov chain sum transit matric size k theta k use notat fy g r mean observ sequenc ff ff m factori hidden markov model obtain forward recurs end forward recurs likelihood observ sequenc sum k element ff similarli obtain backward recurs defin obtain posterior probabl state time obtain multipli ff algorithm shown equival jensen lauritzen olesen algorithm probabl propag graphic model probabl defin collect state variabl correspond cliqu equival junction tree inform pass forward backward sum set separ neighbor cliqu tree result forwardbackwardtyp recurs order otmk m use ff fi fl quantiti statist requir e step z ghahramani mi jordan appendix complet factor variat approxim use definit probabilist model given equat b posterior probabl state given observ sequenc written z z normal constant ensur probabl sum one similarli probabl distribut given variat approxim written expfgammah log m use notat denot expect respect variat distribut use angular bracket hdeltai kl diverg three fact verifi definit variat approxim diagf m factori hidden markov model diag oper take vector return squar matrix element vector along diagon zero everywher els kl diverg therefor expand log m c tr c trf m log p m take deriv respect m obtain log m c gammalog delta m vector diagon element w m c c term aris log zq ensur m sum one set deriv equal solv m give equat a appendix structur approxim structur approxim hq defin log h m use c write kl diverg tr c tr c diag log z d z ghahramani mi jordan sinc kl independ m p m first thing note paramet structur approxim remain equal equival paramet true system now take deriv respect log h n get log h n log h m m c log h n last term obtain make use fact log zq log h n cancel first term set term insid bracket d equal zero yield equat a acknowledg thank lawrenc saul help discuss geoffrey hinton support project support part grant mcdonnellpew foundat grant atr human inform process research laboratori gift siemen corpor grant n offic naval research zoubin ghahramani support grant ontario inform technolog research centr note relat work infer distribut state hmm see dean kanazawa speech neural network gener use model p s jy probabl convert observ probabl need hmm via bay rule column w m w n orthogon everi pair state variabl n c diagon covari matrix state variabl longer depend given observ case explain away state variabl model variabl observ along differ subspac bayesian treatment learn problem paramet also consid hidden random variabl handl gibb sampl replac m step sampl condit distribut paramet given hidden variabl for exampl see tanner wong first term replac log m second term appear sampl use learn is sampl discard begin run although ten sampl even approach converg provid runtim roughli compar variat method goal see whether impati gibb sampler would abl compet approxim method factori hidden markov model lower valu suggest better probabilist model valu one exampl mean would take one bit true gener model code observ vector standard deviat reflect variat due train set test set random seed algorithm standard error mean factor smaller variat method dash line equal minu lower bound log likelihood except normal term intract comput vari learn result appar occasion increas bound sinc attribut model real number log likelihood measur rel code cost comparison likelihood meaning wherea obtain absolut cost code sequenc necessari specifi discret level thi analog fullyconnect boltzmann machin n unit hinton sejnowski everi binari unit coupl everi unit use on paramet rather o n paramet requir specifi complet probabl tabl r maxim techniqu occur statist analysi probabilist function markov chain inputoutputhmm architectur mixtur control jump linear nonlinear plant multipl viewpoint system music predict element inform theori applic gener propag algorithm probabilist expert system model reason persist causat maximum likelihood incomplet data via em algorithm neural network biasvari dilemma stochast relax factori learn em algorithm learn relearn boltzmann machin bayesian updat recurs graphic model local comput hierarch mixtur expert em algorithm neural comput stochast simul algorithm dynam probabilist network hidden markov model comput biolog applic protein model local comput probabl graphic structur applic expert system gener linear model learn fine motion markov mixtur expert uci repositori machin learn databas connectionist learn belief network probabilist infer use markov chain mont carlo method technic report crgtr new view em algorithm justifi increment variant statist field theori probabilist reason intellig system network plausibl infer ca morgan kaufmann introduct hidden markov model mix memori markov model mean field theori sigmoid belief network journal artifici intellig research boltzmann chain hidden markov model exploit tractabl substructur intract network probabilist independ network hidden markov probabl model hidden markov model induct bayesian model merg calcul posterior distribut data augment with discuss bound convolut code asymptot optim decod algorithm mean field network learn discrimin tempor distort string minimum descript length framework unsupervis learn receiv tr probabilist reason intellig system network plausibl infer model reason persist causat learn relearn boltzmann machin element inform theori connectionist learn belief network neural network biasvari dilemma hierarch mixtur expert em algorithm probabilist independ network hidden markov probabl model hidden markov model induct bayesian model merg minimum descript length framework unsupervis learn ctr p xing michael i jordan stuart russel graph partit strategi gener mean field infer proceed th confer uncertainti artifici intellig p juli banff canada ricardo silva jiji zhang jame g shanahan probabilist workflow mine proceed eleventh acm sigkdd intern confer knowledg discoveri data mine august chicago illinoi usa andrew howard toni jebara dynam system tree proceed th confer uncertainti artifici intellig p juli banff canada raul fernandez rosalind w picard model driver speech stress speech commun v n p april robert a jacob wenxin jiang martin a tanner factori hidden markov model gener backfit algorithm neural comput v n p octob terri caelli andrew mccabe garri brisco shape track product use hidden markov model hidden markov model applic comput vision world scientif publish co inc river edg nj agnieszka betkowska koichi shinoda sadaoki furui robust speech recognit use factori hmm home environ eurasip journal appli signal process v n p januari yunhua hu hang li yunbo cao dmitriy meyerzon qinghua zheng automat extract titl gener document use machin learn proceed th acmieeec joint confer digit librari june denver co usa yunhua hu hang li yunbo cao li teng dmitriy meyerzon qinghua zheng automat extract titl gener document use machin learn inform process manag intern journal v n p septemb toni jebara risi kondor andrew howard probabl product kernel journal machin learn research p fine yoram singer naftali tishbi hierarch hidden markov model analysi applic machin learn v n p juli charl sutton khashayar rohanimanesh andrew mccallum dynam condit random field factor probabilist model label segment sequenc data proceed twentyfirst intern confer machin learn p juli banff alberta canada wang nan zheng yan li yingq xu heungyung shum learn kernelbas hmm dynam sequenc synthesi graphic model v n p juli jie tang hang li yunbo cao zhaohui tang email data clean proceed eleventh acm sigkdd intern confer knowledg discoveri data mine august chicago illinoi usa cen li gautam biswa bayesian approach structur learn hidden markov model scientif program v n p august sophi denev bayesian spike neuron i infer neural comput v n p januari lawrenc k saul michael i jordan mix memori markov model decompos complex stochast process mixtur simpler one machin learn v n p oct yong cao petro faloutso frdric pighin unsupervis learn speech motion edit proceed acm siggrapheurograph symposium comput anim juli san diego california hung h bui svetha venkatesh geoff west track surveil widearea spatial environ use abstract hidden markov model hidden markov model applic comput vision world scientif publish co inc river edg nj r anderson pedro domingo daniel s weld relat markov model applic adapt web navig proceed eighth acm sigkdd intern confer knowledg discoveri data mine juli edmonton alberta canada tom ingliar milo hauskrecht noisyor compon analysi applic link analysi journal machin learn research p martin v butz kernelbas ellipsoid condit realvalu xc classifi system proceed confer genet evolutionari comput june washington dc usa ying wu thoma s huang robust visual track integr multipl cue base coinfer learn intern journal comput vision v n p june michael i jordan zoubin ghahramani tommi s jaakkola lawrenc k saul introduct variat method graphic model machin learn v n p nov andrea torsello antonio robleskelli edwin r hancock discov shape class use tree editdist pairwis cluster intern journal comput vision v n p may charl sutton andrew mccallum khashayar rohanimanesh dynam condit random field factor probabilist model label segment sequenc data journal machin learn research p h attia independ factor analysi neural comput v n p may jinhai cai zhiqiang liu hidden markov model spectral featur shape recognit ieee transact pattern analysi machin intellig v n p decemb john binder daphn koller stuart russel keiji kanazawa adapt probabilist network hidden variabl machin learn v n p novdec xiangdong dawn jutla nick cercon privaci intrus detect use dynam bayesian network proceed th intern confer electron commerc new ecommerc innov conquer current barrier obstacl limit conduct success busi internet august fredericton new brunswick canada akio utsugi ensembl independ factor analyz applic natur imag analysi neural process letter v n p august zoubin ghahramani introduct hidden markov model bayesian network hidden markov model applic comput vision world scientif publish co inc river edg nj cristian sminchisescu atul kanaujia dimitri metaxa condit model contextu human motion recognit comput vision imag understand v n p novemb hichem snoussi ali mohammaddjafari bayesian unsupervis learn sourc separ mixtur gaussian prior journal vlsi signal process system v n p junejuli inna stainva david low gener probabilist orient wavelet model textur segment neural process letter v n p june russel greiner christian darken n iwan santoso effici reason acm comput survey csur v n p march