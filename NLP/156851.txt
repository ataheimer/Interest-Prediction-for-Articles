t weight nearest neighbor algorithm learn symbol featur a past nearest neighbor algorithm learn exampl work best domain featur numer valu domain exampl treat point distanc metric use standard definit symbol domain sophist treatment featur space requir introduc nearest neighbor algorithm learn domain symbol featur algorithm calcul distanc tabl allow produc realvalu distanc instanc attach weight instanc modifi structur featur space show techniqu produc excel classif accuraci three problem studi machin learn research predict protein secondari structur identifi dna promot sequenc pronounc english text direct experiment comparison learn algorithm show nearest neighbor algorithm compar superior three domain addit algorithm advantag train speed simplic perspicu conclud experiment evid favor use continu develop nearest neighbor algorithm domain one studi here b introduct learn classifi object fundament problem artifici intellig field one attack mani angl despit mani success domain task proven difficult due either inher difficulti domain lack suffici data learn exampl instancebas learn program also call exemplarbas salzberg nearest neighbor cover hart method learn store exampl point featur space requir mean measur distanc exampl aha aha kibler salzberg cost salzberg exampl usual vector featur valu plu categori label featur numer normal euclidean distanc use compar exampl howev featur valu symbol unord valu eg letter alphabet natur interlett distanc nearest neighbor method typic resort much simpler met ric count featur match towel et al recent use metric nearest neighbor algorithm compar studi simpler metric may fail captur complex problem domain result may perform well paper present sophist instancebas algorithm design domain featur valu sym bolic algorithm construct modifi valu differ tabl in style stanfil waltz produc noneuclidean distanc metric introduc idea except space result weight attach individu exampl combin two techniqu result robust instancebas learn algorithm work domain symbol featur valu describ seri experi demonstr algorithm pebl perform well three import practic classif problem comparison given show algorithm accuraci compar back propag decis tree learn algorithm result support claim nearest neighbor algorithm power classifi even featur symbol instancebas learn versu model power instancebas method demonstr number import real world domain predict cancer recurr diagnosi heart diseas classif congression vote record aha kibler salzberg experi demonstr instancebas learn ibl appli effect three do main featur unord symbol valu predict protein secondari structur word pronunci predict dna promot sequenc domain receiv consider attent connectionist research employ back propag learn algorithm sejnowski rosenberg qian sejnowski towel et al addit word pronunci problem subject number comparison use machin learn algorithm stanfil waltz shavlik et al dietterich et al domain repres problem consider practic im portanc symbol featur valu make difficult convent nearest neighbor algorithm show nearest neighbor algorithm pebl base stanfil waltz valu differ method produc highli accur predict model domain intent compar ibl learn method three respect classif accuraci speed train perspicu ie eas algorithm represent understood compar perform first respect superior latter two argu ibl often prefer learn algorithm type problem domain consid paper instancebas learn shown compar favor algorithm eg decis tree rule wide rang domain featur valu either numer binari eg aha aha kibler aha et al salzberg paper present similar evid term classif accuraci domain symbol featur valu howev describ domain consid advantag instancebas learn algorithm provid train time neural net learn algorithm requir vastli time train machin learn method train normal perform repeatedli present network instanc train set allow gradual converg best set weight task use for exampl back propag algorithm weiss kapoulea mooney et al shavlik et al report back propag train time mani order magnitud greater train time algorithm id frequent factor more addit neural net algorithm number paramet eg momentum paramet need tweak programm may requir much addit time weiss kapoulea experi requir mani month experiment time produc result back propag algorithm typic requir hour paramet might adjust algorithm valu r distanc metric see below consid two possibl valu nearest neighbor algorithm requir littl train time term experiment time process time present two version pebl algorithm one slightli complex other simpler version call unweight version experiment section below requir odn number exampl number featur dimens per exampl v number valu featur may have in gener n much larger v henc complex usual odn complex version pebl increment comput weight exemplar requir train instanc train classif refer system take minut real time decstat train instanc experiment time limit minut defin time nearest neighbor system worst odn admittedli slow compar algorithm nearest neighbor method lend well parallel produc significantli faster classif time exampl assign separ processor parallel architectur or enough processor exampl divid among them number processor larg train set classif time may reduc od log n implement system set four looselycoupl transput well convent architectur recent effort fgp fertig gelernt use larger number parallel processor mbrtalk system waltz stanfil implement form knearestneighbor learn use tightlycoupl massiv parallel architectur processor connect machin tm perspicu instancebas learn algorithm also transpar oper learn method algorithm noth straightforward defin measur distanc valu compar new instanc instanc memori classifi accord categori closest decis tree algorithm perhap equal straightforward classif time pass exampl tree get decis neural net algorithm fast classif time transpar weight must propag net sum pass filter eg threshold layer complic part method comput distanc ta data set ble comput via fix statist techniqu base frequenc occurr valu howev certainli complic entropi calcul decis tree method eg quinlan weight adjust routin use back propag tabl provid insight rel import differ featur notic instanc sever tabl protein fold data almost ident indic featur similar instanc memori ibl system readili access examin numer way exampl human want know particular classif made system simpli present instanc memori use classif human expert commonli use explan exampl ask justifi predict economi expert typic produc anoth similar econom situat past neural net yet provid insight made classif did although recent effort explor new method understand content train network hanson burr also rel easi modifi algorithm includ domain specif knowledg rel import featur known featur may weight accordingli distanc formula salzberg taken togeth advantag list make clear ibl algorithm number benefit respect compet model howev order consid realist practic learn techniqu ibl must still demonstr good classif accuraci problem domain symbol featur obviou distanc metric ibl count number featur differ work well thi metric call over lap metric experiment result show modifi valuediffer metric process symbol valu except well result taken togeth result domain numer featur eg aha aha et al show ibl algorithm perform quit well wide rang problem learn algorithm back propag wide use understood neural net learn algorithm use basi comparison experi paper although earlier approach notabl perceptron learn model unabl classifi group concept linearli separ back propag overcom problem back propag gradient descent method propag error signal back multilay network describ mani place eg rumelhart et al rumelhart mcclelland reader interest detail descript look there use decis tree algorithm id quinlan basi comparison decis tree algorithm addit compar perform algorithm method use data domain describ section appropri present result domainspecif classif method instancebas learn instancebas learn algorithm like algorithm store seri train instanc memori use distanc metric compar new instanc store new instanc classifi accord closest exemplar memori algorithm implement program call stand parallel exemplarbas learn system clariti use term exampl mean train test exampl shown system first time use term exemplar follow usag salzberg refer specif instanc previous store comput memori exemplar may addit inform attach eg weight term in stanc cover exampl exemplar pebl design process instanc symbol featur val ue heart pebl algorithm way measur distanc two exampl consist essenti three compo nent first modif stanfil waltz valu differ metric vdm defin distanc differ valu given featur call method mvdm modifi valu differ metric second compon standard distanc metric measur distanc two exampl multidimension featur space fi nalli distanc modifi weight scheme weight instanc memori accord perform histori salzberg compon distanc calcul describ section parallel algorithm develop speed experiment theoret import learn model pebl requir two pass train set first pass featur valu differ tabl construct instanc train set accord equat stanfil waltz vdm second pass system attempt classifi instanc comput distanc new instanc previous store one new instanc assign classif nearest store instanc system check see classif correct use feedback adjust weight old instanc thi weight describ detail section final new instanc store memori test exampl classifi manner modif made memori distanc tabl stanfillwaltz vdm stanfil waltz present power new method measur distanc valu featur domain symbol featur val ue appli techniqu english pronunci problem impress initi result stanfil waltz valu differ metric vdm take account overal similar classif instanc possibl valu featur use method matrix defin distanc valu featur deriv statist base exampl train set distanc ffi two valu eg two amino acid specif featur defin equat equat v v two possibl valu featur eg protein data would two amino acid distanc valu sum n class exampl protein fold experi section three categori number time v classifi categori i c total number time valu occur k constant usual set use equat comput matrix valu differ featur input data interest note valu differ matric comput experi quit similar overal differ featur although differ significantli valu pair idea behind metric wish establish valu similar occur rel frequenc classif term c repres likelihood central residu classifi given featur question valu v thu say two valu similar give similar likelihood possibl classifica tion equat comput overal similar two valu find sum differ likelihood classif consid follow exampl say pool instanc examin singl featur take one three valu a b c two classif ff fi possibl data construct tabl tabl entri repres number time instanc tabl number occurr valu class featur valu ff fi tabl valu differ tabl featur valu given featur valu classif inform construct tabl distanc follow frequenc occurr class ff sinc instanc classifi ff instanc valu a similarli frequenc occurr b c respect frequenc occurrnc class fi on find distanc b use equat yield complet tabl distanc shown tabl note construct differ valu differ tabl featur featur construct tabl equat defin geometr distanc fix finit set valu is properti valu distanc zero itself posit distanc valu distanc symmetr distanc obey triangl inequ summar properti follow ii iii iv stanfil waltz origin vdm also use weight term w g f make version ffi nonsymmetr eg ffia b ffib a major differ metric vdm mvdm omit term make ffi symmetr total distanc delta two instanc given by x repres two instanc eg two window protein fold domain x exemplar memori new exampl variabl x valu th featur x exampl n featur wx w weight assign exemplar describ follow section new exampl w domain numer featur manhattan distanc produc euclidean distanc experi use howev use protein secondari structur task summari four major differ mvdm stanfillwaltz vdm omit weight term w g f make stanfillwaltz vdm nonsymmetr formul ffi delta symmetr stanfil waltz use valu version equat preliminari experi indic equal good perform achiev chose valu reason simplic ad exemplar weight distanc formula describ section stanfil waltz use closest exemplar classif wherea pebl use nearest neighbor thi realli differ learn algorithm rather valu differ weight exemplar except space store instanc reliabl classifi other intuit one would like trustworthi exemplar draw power other final differ mvdm metric origin vdm capac metric treat reliabl instanc differ accomplish weight wx distanc formula reliabl exemplar given smaller weight make appear closer new exampl weight scheme first adopt system salzberg assign weight exemplar accord perform histori wx ratio number use exemplar number correct use exemplar thu accur exemplar wx unreli exemplar wx make appear away new exampl unreli exemplar may repres either nois except small area featur space normal rule appli time exemplar incorrectli use classif larger weight grow altern scheme handl noisi except instanc ibl framework discuss aha kibler elabor aha scheme instanc use nearestneighbor comput proven accept classifi accept instanc whose classif accuraci exceed baselin frequenc class fix amount for exampl baselin frequenc class instanc correct time would accept wherea baselin frequenc instanc would accept abl note techniqu design primarili filter noisi instanc rather identifi except instanc differ noisi instanc probabl ignor discard wherea except instanc retain use rel infrequ differ salzberg origin exemplar weight scheme one signific aspect way exemplar point weight initi origin scheme store point initi weight effect featur space signific consid instanc space contain two point classifi ff fi unweight two point defin hyperplan divid ndimension space ff fi region shown figur point locat left side plane classifi ff likewis fi figur two unweight point instanc space pebl comput distanc new instanc weight ex emplar distanc multipli exemplar weight intuit make less like new instanc appear near exemplar exemplar weight grow figur show that geometr use weight creat circular envelop around exemplar larger weight defin except space shrink weight differ increas point insid circl match point larger weight weight equal special case hyperplan given abov gener given space mani exemplar exemplar smallest weight or best classif perform partit space set hyperplan weight best exemplar ident partit use larg circl exemplar effect rule region space exemplar larger weight defin except space around themselv figur show within except space process may recur group exemplar figur two weight point instanc space figur partit except space approxim equal weight abil partit space larg gener rule pocket except import domain contain mani except without capabl mani point requir learn necessari surround except set nonexcept point defin edg space here two point requir defin rule except capabl becom even import ibl model store subset train exampl reduc number point must store cost salzberg given discuss clear instanc initi weight consid system train instanc train n th hierarchi instanc weight alreadi construct train repres structur domain instanc enter weight would immedi becom one influenti classifi space found better strategi initi new instanc weight equal match exemplar adopt weight strategi experi describ below weight scheme complet modifi valu differ metric domain chose comparison three domain receiv consider attent machin learn research commun word pronunci task sejnowski rosenberg shavlik et al predict protein secondari structur qian sejnowski holley karplu predict dna promot sequenc towel et al domain symbolicvalu featur thu mvdm applic wherea standard euclidean distanc not section describ three databas problem present learn protein secondari structur accur techniqu predict fold structur protein yet exist despit increasingli numer attempt solv problem techniqu depend part predict secondari structur primari sequenc amino acid secondari structur inform use construct final tertiari structur tertiari structur difficult deriv directli requir expens method xray crystallographi primari sequenc sequenc amino acid constitut protein rel easi discov attempt predict secondari structur involv classif residu three categori ff helix fi sheet coil three wide use approach problem robson garnier et al chou fasman lim produc classif accuraci rang other accur techniqu develop predict tertiari secondari structur eg cohen et al lathrop et al accur predict secondari structur proven extrem difficult task learn problem describ follow protein consist sequenc amino acid bond togeth chain sequenc known primari structur amino acid chain one twenti differ acid point two acid join chain variou factor includ chemic properti determin angl molecular bond them angl purpos character one three differ type fold ff helix fi sheet coil word certain number consecut acid hereaft residu chain join manner call ff segment chain ff helix character fold type protein known secondari structur learn problem then is given sequenc residu fix length window protein chain classifi central residu window ff helix fi sheet coil setup simpli window z z gtpgksfnlnfdtg central residu qian sejnowski holley karplu formul problem exactli manner studi found optim window size approxim residu largest window test either studi separ statist studi cost found window size five six nearli suffici uniqu identifi residu data set indic tabl tabl show percentag sequenc given size unambigu for entir data set determin fold classif protein segment exampl consid window size six center residu classifi found pattern data set uniqu addit found slightli inform contain residu left point predict right the point predict residu secondari structur must predict skew top column tabl indic left right shift pattern respect center window eg skew mean tabl percent uniqu pattern window size skew window size pattern center two posit left point predict tabl show quit clearli that one store pattern length data set one could classifi data set better accuraci obstacl good perform domain includ undersampl nonloc effect consid window size five databas use million possibl segment repres databas also protein solut form globular structur net result residu sequenti far may physic quit close signific effect other reason secondari structur probabl cannot complet determin primari structur qian claim method incorpor local inform perform much better current result rang for nonhomolog protein promot sequenc promot sequenc databas subject sever recent experi towel et al relat protein fold task involv predict whether given subsequ dna sequenc promot sequenc gene init process call transcript express adjac gene data set contain exampl posit exampl promot neg exampl gener larger dna sequenc believ contain promot see towel et al detail construct data set instanc consist sequenc nucleotid alphabet a c t classif learn nucleotid treat featur one four symbol valu pronunci english text word pronunci problem present interest challeng machin learn although effect practic algorithm develop task given rel small sequenc letter object learn sound stress requir pronounc part given word sejnowski rosenberg introduc task learn commun nettalk program nettalk use back propag learn method perform well task pronounc word continu spoken text although could match perform current speech synthesi program instanc represent text pronunci similar previou problem instanc sequenc letter make word task classifi central letter sequenc correct phonem use fix window seven charact experi sejnowski rosenberg stanfil waltz use window size class includ phonem plu stress classif phonem stress predict theta possibl class although actual occur dictionari experi emphas predict phonem onli difficulti domain aris irregular natur lan guag english languag particular rule exist except better perform data set obtain nonlearn rulebas approach kontogiorgio howev learn algorithm troubl find best set rule experiment result section describ experi result three test domain comparison use previous publish result learn method order make comparison valid attempt duplic experiment design earlier studi close possibl use data use studi protein secondari structur protein sequenc use experi origin brookhaven nation laboratori secondari structur assign ff helix fisheet coil made base atom coordin use method kabsch sander qian sejnowski collect databas protein contain protein segment which call subunit use set protein segment use parallel experi sigillito use back propag ident data reproduc classif accuraci result qian initi experi divid data train set contain protein segment test set contain segment overlap two set tabl show composit two set tabl show percentag three categori tabl composit train test set protein segment residu ff fi coil train test approxim test set train set protein segment separ main experi ie instanc drawn one segment resid togeth either train test set pebl train describ train set use equat we found preliminari experi produc slightli improv accuraci domain repeat main experi varieti differ window size rang domain pebl includ postprocess algorithm base minim sequenc length restrict use holley karplu restrict state fisheet must consist contigu sequenc fewer two residu ffhelix fewer four subsequ predict conform restrict individu residu reclassifi coil qian sejnowski use differ form postprocess call cascad neural net fed output one net anoth network attempt reclassifi residu second network design qian sejnowski care balanc overal frequenc three categori train test set attempt same addit use train set residu slightli smaller although databas ident access specif partit train test set use qian sejnowski tabl classif accuraci window size window unweight holley qian size pebl pebl karplu sejnowski take advantag correl neighbor secondari structur assign result classif accuraci given tabl un weight pebl column show result use pebl without weight wx exemplar entri tabl percentag correct predict test set tabl show highest accuraci produc pebl achiev window size qian sejnowski obtain best result use cascad network ar chitectur use singl network design similar holley karplu best result best perform pebl without postprocess best convent techniqu report holley karplu produc accuraci also perform experi use overlap metric produc accuraci tabl comparison correl coeffici algorithm correct c ff c fi c coil pebl qian sejnowski holley karplu rang differ window size match pair analysi reveal weight version pebl perform significantli better unweight version particular ttest show weight version better confid level thu exemplar weight improv perform significantli anoth frequent use measur perform domain correl coeffici provid measur accuraci categori defin follow equat mathew p ff number time ff correctli predict n ff number time ff correctli reject ff number fals posit ff u ff number miss ff correct predict similar definit use c fi c coil coeffici pebl two back propag experi appear tabl variat train set size third measur classif perform involv repeat test randomli select test set tabl show tabl train pebl vari percentag data set train set percent correct size test set perform pebl weight train vari percentag randomli select instanc entir data set use window size trial here set exampl chosen random train ing exampl remov data set test phase use remain exampl sinc protein compris mani exampl differ part singl protein could appear train test set given trial classif accuraci tabl averag ten run train set size note number report tabl reflect classif perform pebl without post process minim sequenc length restrict as explain abov post process part experi holley karplu experi thu perform compar weight algorithm use window size postprocess weight pebl figur with post process accuraci improv thu see particular composit train test set experi construct mimic design earlier experi improv accuraci learn algorithm promot sequenc experi run towel et al promot sequenc databas leaveoneout trial methodolog involv remov one element data train remain data test one element thu instanc databas pebl train test remain perform instanc databas entir procedur repeat time time use differ random order instanc towel et al also repeat entir leaveoneout experi time use differ random initi state neural net time result shown tabl compar pebl towel et al kbann algorithm addit report number obtain towel et al sever machin learn algorithm includ back propa gation id nearest neighbor overlap metric best method report biolog literatur oneil recal overlap metric measur distanc number featur differ valu also worth note test run pebl four instanc caus error three four neg one like sourc variat classif accuraci homolog train test set homolog protein structur similar algorithm may much accur predict structur protein train homolog one tabl promot sequenc predict algorithm pebl kbann pebl unweight back propag id nearest neighbor overlap oneil instanc towel note neg exampl databas the data use here deriv select substr fragment e coli bacteriophag believ contain promot site towel et al p would suggest base result four exampl reexamin four exampl might interest except gener pattern dna promot english text pronunci english pronunci task use train set defin se jnowki rosenberg nettalk program set consist instanc drawn brown corpu commonli use word english languag unabl discern differ train set somewhat restrict set shavlik shavlik et al one experiment design use train brown corpu pebl test entir word merriam webster pocket dictionari result present tabl weight unweight version pebl algorithm comparison tabl english text pronunci algorithm phonem accuraci phonemestress pebl pebl unweight back propag give result nettalk program use back propag learn algorithm shavlik et al replic sejnowski rosenberg methodolog part work although result differ sejnowski rosenberg not surprisingli sinc back propag network requir much tune make easier comparison our properti follow fact origin sejnowski rosenberg studi use distribut output encod is system produc bit sequenc rather one bit phonemestress combin first bit distribut encod phonem remain bit local encod stress type bit output vector match closest descript vector phonem shavlik et al explicitli compar encod pure local encod sinc output pebl alway local ie output specif phonem phonemestress combin appropri compar method produc output tabl show result compar back propag perceptron id quinlan preliminari experi use overlap metric databas abysm result desir improv result one reason develop mvdm tabl phonemestress accuraci output encod algorithm local encod distribut encod correct correct back propag id perceptron latter three result shavlik et al tabl show pebl perform slightli better learn method output local encod distribut encod improv result id back propag compar experi pebl would requir signific chang output function yet perform shavlik et al also test perform back propag id perceptron learn function size train set perform similar experi increas show result tabl graphic figur comparison result averag run differ randomlychosen train set run surprisingli perform improv steadili size train set increas surpris though good perform even small train set tabl pebl perform vari train set size percentag brown phonem correct corpu train full dictionari figur classif accuraci function train set size classif accuraci studi show classif accuraci pebl is gener equal slightli superior learn method domain featur notabl protein structur predict task pebl give consider better classif result back propaga tion without weight exemplar note consid fair test perform random select residu vari percentag dataset perform figur algorithm slightli wors albeit still quit good would inform see similar experi run neural network learn algorithm recent zhang waltz investig hybrid learn method protein structur predict combin nearest neighbor neural net learn statist inform figur yet publish method also outperform previou method waltz although accuraci exceed pebl dna promot sequenc predict towel et al report kbann techniqu integr neural net domain knowledg superior standard back propag certainti df kbann design specif show ad domain knowledg could improv perform neural net learn algorithm addit kbann outperform id nearest neighbor nearest neighbor use overlap metric use experiment de sign pebl exactli match perform kbann measur superior back propag id oneil method data strong perform pebl data set demonstr nearest neighbor perform well use larg protein fold small promot sequenc train set especi signific pebl use weak gener method abl match perform kbann knowledgerich approach english pronunci domain result mix best result sejnowski rosenberg superior phonemestress accuraci pebl howev shavlik et al replic ex periment best result note abov neural net result reflect distribut output encod local encod pebl produc back propag classif accuraci id somewhat lower pebl conclus techniqu perform similarli learn techniqu yet come close perform good commerci system much less nativ speaker english clearli still room consider progress domain shavlik et al conclud base experi classif accuraci versu number train exampl see figur nettalk data small amount train data back propag prefer decis tree construct id howev result indic nearest neighbor algorithm also work well train set small perform curv figur show pebl need exampl achiev rel good perform transpar represent oper train given domain pebl contain memori set inform rel perspicu comparison weight assign neural network exemplar provid specif refer instanc case histori were may cite support particular decis inform may easili gather train even test phase shed addit light domain question instanc consid attach counter exemplar increment time exemplar use exact match compar number time exemplar use get good idea whether exemplar specif except part gener rule examin weight wx attach exemplar determin whether instanc reliabl cla sifier distanc tabl reveal order set symbol valu appar valu alon hand deriv distanc perspicu deriv global characterist train data english pronunci task distribut output encod shown produc superior perform local encod shavlik et al result point weak pebl nearest neighbor method allow distribut output encod neural net handl encod quit easili decis tree handl difficulti shavlik et al built separ decis tree bit distribut encod phonemestress pair task rais question whether nearest neighbor method handl encod one possibl use knearest neighbor would allow one exemplar determin output bit eg exemplar contain bit encod predict valu bit new exampl would determin major vote k nearest neighbor bit experi requir determin strategi would advantag gener transpar oper learn classif algorithm nearest neighbor algorithm simpl basic learn routin simpli store new exampl memori pebl comput exemplar weight noth simpl recordkeep base classif perform exist exemplar ad exemplar chang weight chang way nearest neighbor algorithm partit featur space illustr except space although may hard visual three dimens nonetheless straightforward compar oper adjust weight back propag algorithm thu far research found difficult character classif perform chang connect weight chang one minor drawback pebl method nonincrement unlik back propag version decis tree method increment extens pebl would probabl quit expens sinc valu differ tabl might recomput mani time hand extend pebl handl mix symbol numer data quit straightforward algorithm could use simpl differ numer fea ture valu differ tabl symbol one final experi protein domain demonstr use weight attach exemplar improv accuraci nearest neighbor algorithm domain english pronunci weight make signific differ base result earlier result realvalu domain salzberg conclud exemplar weight offer real potenti enhanc power practic learn algorithm conclus demonstr seri experi instancebas learn algorithm perform except well domain featur valu symbol direct comparison implement pebl perform well or better than back propag id sever domainspecif learn algorithm sever difficult classif task addit nearest neighbor offer clear advantag much faster train represent rel easi interpret one yet know interpret network weight learn neural net decis tree somewhat easier interpret hard predict impact new exampl structur tree sometim one new exampl make differ all time may radic chang larg portion tree hand neural net fix size decis tree tend quit small respect method compress data way nearest neighbor not addit classif time fast depend depth net tree size input base classif accuraci though clear learn techniqu advantag nearestneighbor method respect nearest neighbor learn per se shown weight exemplar improv perform subdivid instanc space manner reduc impact unreli exampl nearest neighbor algorithm one simplest learn method known yet algorithm shown outperform consist taken togeth result indic continu research extend improv nearest neighbor learn algorithm prove fruit acknowledg thank joann houlahan david aha numer insight comment suggest thank also richard sutton three anonym review detail comment idea research support part air forc offic scientif research grant afosr nation scienc foundat grant iri r studi instancebas algorithm supervis learn task predict secondari structur protein amino acid sequenc turn predict protein use pattern match approach nearest neighbor pattern classif certain aspect anatomi physiolog cerebr cortex compar studi id backpropag english texttospeech map fgp virtual machin acquir knowledg case empir comparison id backpropag analysi accuraci implic simpl method predict secondari structur globular protein connectionist model learn learn represent connectionist network protein secondari structur predict dictionari protein secondari struc ture pattern recognit hydrogenbond geometr featur automat lettertophonem transcript speech synthesi ariadn patterndirect infer hierarch abstract protein structur recogni tion algorithm predict ffhelic betastructur region globular protein comparison predict observ secondari structur phage lysozym distribut model human learn memori context theori classif learn escherichia coli promot i comput geometri intro duction predict secondari structur globular protein use neural network model pattern recognit categor learn represent backpropag error pdp research group nest hyperrectangl exemplarbas learn learn nest gener exemplar nearest hyperrectangl learn method nettalk parallel network learn read aloud symbol neural learn algorithm experiment comparison person commun toward memorybas reason refin approxim domain theori knowledgebas neural network massiv parallel ai empir comparison pattern recognit tr ctr walter daeleman peter berck steven gilli unsupervis discoveri phonolog categori supervis learn morpholog rule proceed th confer comput linguist august copenhagen denmark david waltz simon kasif reason data acm comput survey csur v n p sept vroniqu host walter daeleman iri hendrickx antal van den bosch dutch word sens disambigu optim local context proceed acl workshop word sens disambigu recent success futur direct p juli o l mangasarian j b rosen m e thompson convex kernel underestim function multipl local minima comput optim applic v n p may tomuro question terminolog represent question type classif cole computerm second intern workshop comput terminolog p august rafael alonso jeffrey a bloom hua li chumki basu adapt nearest neighbor search part acquisit eport proceed ninth acm sigkdd intern confer knowledg discoveri data mine august washington dc michael pazzani daniel billsu learn revis user profil identif ofinterest web site machin learn v n p june luca cazzanti maya r gupta local similar discrimin analysi proceed th intern confer machin learn p june corvali oregon perrizo amal perera paramet optim vertic nearestneighborvot boundarybas classif acm sigkdd explor newslett v n p decemb ping zhang brijesh verma kuldeep kumar neural vs statist classifi conjunct genet algorithm base featur select pattern recognit letter v n p may steven salzberg arthur l delcher david heath simon kasif bestcas result nearestneighbor learn ieee transact pattern analysi machin intellig v n p june paul losiewicz dougla w oard ronald n kostoff textual data mine support scienc technolog manag journal intellig inform system v n p septoct v host i hendrickx w daeleman a van den bosch paramet optim machinelearn word sens disambigu natur languag engin v n p decemb gerard escudero llu mrquez german rigau empir studi domain depend supervis word sens disambigu system proceed joint sigdat confer empir method natur languag process larg corpora held conjunct th annual meet associ comput linguist p octob hong kong kai ming ting discretis lazi learn algorithm artifici intellig review v n p feb jorn veenstra antal van den bosch singleclassifi memorybas phrase chunk proceed nd workshop learn languag logic th confer comput natur languag learn septemb lisbon portug piotr indyk rajeev motwani prabhakar raghavan santosh vempala localitypreserv hash multidimension space proceed twentyninth annual acm symposium theori comput p may el paso texa unit state amir ahmad lipika dey featur select techniqu classificatori analysi pattern recognit letter v n p januari david w patterson mykola galushka niall rooney characteris novel index techniqu casebas reason artifici intellig review v n p june jianp zhang yeesat yim jum yang intellig select instanc predict function lazylearn algorithm artifici intellig review v n p feb hwee tou ng hian beng lee integr multipl knowledg sourc disambigu word sens exemplarbas approach proceed th annual meet associ comput linguist p june santa cruz california pedro domingo michael pazzani optim simpl bayesian classifi zeroon loss machin learn v n p novdec naoki abe hiroshi mamitsuka predict protein secondari structur use stochast tree grammar machin learn v n p novdec stephan raaijmak learn distribut linguist class proceed nd workshop learn languag logic th confer comput natur languag learn septemb lisbon portug christoph j merz use correspond analysi combin classifi machin learn v n p julyaugust eyal kushilevitz rafail ostrovski yuval rabani effici search approxim nearest neighbor high dimension space proceed thirtieth annual acm symposium theori comput p may dalla texa unit state philip k chan salvator j stolfo experi multistrategi learn metalearn proceed second intern confer inform knowledg manag p novemb washington dc unit state anandeep s pannu use genet algorithm induct reason case legal domain proceed th intern confer artifici intellig law p may colleg park maryland unit state aristid gioni piotr indyk rajeev motwani similar search high dimens via hash proceed th intern confer larg data base p septemb belur v dasarathi data mine task method classif nearestneighbor approach handbook data mine knowledg discoveri oxford univers press inc new york ny amir ahmad lipika dey method comput distanc two categor valu attribut unsupervis learn categor data set pattern recognit letter v n p januari charl x ling hangdong wang comput optim attribut weight set nearest neighboralgorithm artifici intellig review v n p feb grzegorz gra arkadiusz wojna riona new classif system combin rule induct instancebas learn fundamenta informatica v n p decemb mark stevenson yorick wilk interact knowledg sourc word sens disambigu comput linguist v n p septemb ting liu andrew w moor alexand gray new algorithm effici highdimension nonparametr classif journal machin learn research p wai lam chikin keung danyu liu discov use concept prototyp classif base filter abstract ieee transact pattern analysi machin intellig v n p august jihoon yang vasant g honavar featur subset select use genet algorithm ieee intellig system v n p march jakub zavrel walter daeleman memorybas learn use similar smooth proceed eighth confer european chapter associ comput linguist p juli madrid spain ronni kohavi daniel a sommerfield case studi public domain multipl mine task system mlc handbook data mine knowledg discoveri oxford univers press inc new york ny xin dong alon halevi jayant madhavan ema neme jun zhang similar search web servic proceed thirtieth intern confer larg data base p august septemb toronto canada philip k chan salvator j stolfo accuraci metalearn scalabl data mine journal intellig inform system v n p janfeb antal van den bosch sabin buchholz shallow pars basi word onli case studi proceed th annual meet associ comput linguist juli philadelphia pennsylvania arkadiusz wojna centerbas index vector metric space fundamenta informatica v n p august fedro domingo controlsensit featur select lazi learner artifici intellig review v n p feb arkadiusz wojna centerbas index vector metric space fundamenta informatica v n p august filippo neri lorenza saitta explor power genet search learn symbol classifi ieee transact pattern analysi machin intellig v n p novemb ding liu strong lower bound approxim nearest neighbor search inform process letter v n p octob harold somer bill black joakim nivr torbjrn lager annarosa multari luca gilardoni jeremi ellman alex roger multilingu gener summar job advert tree project proceed fifth confer appli natur languag process p march april washington dc nivr mario scholz determinist depend pars english text proceed th intern confer comput linguist pe august geneva switzerland walter daeleman antal van den bosch jakub zavrel forget except harm languag learn machin learn v n p feb juan manuel gimeno illa javier bjar alonso miquel snchez marr nearestneighbour time seri appli intellig v n p januaryfebruari xudong luo jimmi homan lee hofung leung nichola r jen prioritis fuzzi constraint satisfact problem axiom instanti valid fuzzi set system v n p june omer barkol yuval rabani tighter lower bound nearest neighbor search relat problem cell probe model journal comput system scienc v n p june omer barkol yuval rabani tighter bound nearest neighbor search relat problem cell probe model proceed thirtysecond annual acm symposium theori comput p may portland oregon unit state chaolin liu chengtsung chang jimhow ho classif cluster casebas crimin summari judgment proceed th intern confer artifici intellig law june scotland unit kingdom a m roumani d b skillicorn mobil servic discoveri select publishsubscrib paradigm proceed confer centr advanc studi collabor research p octob markham ontario canada stergio papadimitri seferina mavroudi liviu vladutu anastasio bezeriano gener radial basi function network train instanc base learn data mine symbol data appli intellig v n p mayjun piotr indyk rajeev motwani approxim nearest neighbor toward remov curs dimension proceed thirtieth annual acm symposium theori comput p may dalla texa unit state terri r payn peter edward clair l green experi rule induct knearest neighbor method interfac agent learn ieee transact knowledg data engin v n p march fink alfr kobsa user model person citi tour artifici intellig review v n p septemb dietrich wettschereck david w aha takao mohri review empir evalu featur weight method aclass lazi learn algorithm artifici intellig review v n p feb miquel montan beatriz lpez josep llu de la rosa taxonomi recommend agent theinternet artifici intellig review v n p june sunil arya david m mount nathan s netanyahu ruth silverman angela y wu optim algorithm approxim nearest neighbor search fix dimens journal acm jacm v n p nov stergio papadimitri seferina mavroudi liviu vladutu g pavlid anastasio bezeriano supervis network selforgan map classif larg data set appli intellig v n p mayjun christoph g atkeson andrew w moor stefan schaal local weight learn artifici intellig review v n p feb alfr kobsa jrgen koenemann wolfgang pohl personalis hypermedia present techniqu improv onlin custom relationship knowledg engin review v n p march francisco azuaj werner dubitzki norman black kenni adamson retriev strategi casebas reason categoris bibliographi knowledg engin review v n p decemb francisco azuaj werner dubitzki norman black kenni adamson retriev strategi casebas reason categoris bibliographi knowledg engin review v n p decemb