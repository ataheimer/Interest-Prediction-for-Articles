t nearoptim reinforc learn polynomi time a present new algorithm reinforc learn prove polynomi bound resourc requir achiev nearoptim return gener markov decis process observ number action requir approach optim return lower bound mix time optim polici in undiscount case horizon time in discount case give algorithm requir number action total comput time polynomi number state action undiscount discount case interest aspect algorithm explicit handl explorationexploit tradeoff b introduct reinforc learn agent interact unknown environ attempt choos action maxim cumul payo sut ton barto barto et al bertseka tsitsikli environ typic model markov decis process mdp assum agent know paramet process learn act directli experi thu reinforc learn agent face fundament tradeo exploit explor bertseka kumar varaiya thrun is agent exploit cumul experi far execut action current seem best execut dierent action hope gain inform experi could lead higher futur payo littl explor prevent agent ever converg optim behavior much explor prevent agent gain nearoptim payo time fashion larg literatur reinforc learn grow rapidli last decad mani dierent algorithm propos solv reinforc learn problem variou theoret result converg properti algorithm proven exampl watkin qlearn algorithm guarante asymptot converg optim valu from optim action deriv provid everi state mdp visit innit number time watkin watkin dayan jaakkola et al tsitsikli asymptot result specifi strategi achiev innit explor provid solut inher exploitationexplor tradeo address thi singh et al specifi two explor strategi guarante sucient explor asymptot converg optim action asymptot exploit qlearn sarsa algorithm a variant qlearn rum meri niranjan singh sutton sutton gullap barto jalali ferguson present algorithm learn model environ experi perform valu iter estim model innit explor converg optim polici asymptot result best knowledg result reinforc learn gener mdp asymptot natur provid guarante either number action comput time agent requir achiev nearoptim perform hand nonasymptot result becom avail one consid restrict class mdp model learn modi standard one one chang criteria success thu saul singh provid algorithm learn curv converg rate interest special class mdp problem design highlight particular exploitationexplor tradeo fiechter whose result closest spirit our consid discountedpayo case make learn protocol easier assum avail reset button allow agent return set startstat arbitrari time other provid nonasymptot result predict uncontrol markov process schapir warmuth singh dayan thu despit mani interest previou result reinforc learn literatur lack algorithm learn optim behavior gener mdp provabl nite bound resourc action comput time requir standard model learn agent wander continu unknown environ result present paper void essenti strongest possibl sens present new algorithm reinforc learn prove polynomi bound resourc requir achiev nearoptim payo gener mdp observ number action requir approach optim return lower bound algorithm mix time optim polici in undiscountedpayo case horizon time in discountedpayo case give algorithm requir number action total comput time polynomi number state undiscount discount case interest aspect algorithm rather explicit handl exploitationexplor tradeo two import caveat appli current result well prior result mention abov first assum agent observ state environ may impract assumpt reinforc learn problem second address fact state space may larg resort method function approxim result avail reinforc learn function approxim sutton singh et al gordon tsitsikli roy partial observ mdp chrisman littman et al jaakkola et al asymptot natur extens result case left futur work outlin paper follow section give standard denit mdp reinforc learn section argu mix time polici must taken consider order obtain nitetim converg result undiscount case make relat technic observ denit section make similar argument horizon time discount case provid need technic lemma heart paper contain section state prove main result describ algorithm detail provid intuit proof converg rate section elimin technic assumpt made conveni main proof section discuss extens main theorem defer exposit final section close discuss futur work preliminari denit begin basic denit markov decis process denit markov decis process mdp state action transit probabl p action a state j specifi probabl reach state j execut action state thu state action a payo distribut state i mean rm i where r max rm i varianc var i var max distribut determin random payo receiv state visit simplic assum number action k constant easili veri k paramet resourc requir algorithm scale polynomi k sever comment regard benign technic assumpt make payo order here first common assum payo actual associ stateact pair rather state alon choic latter entir technic simplic result paper hold standard stateact pay os model well second assum xed upper bound r max var max mean varianc payo distribut restrict necessari nitetim converg result third assum expect payo alway nonneg conveni easili remov ad sucient larg constant everi payo note although actual payo experienc random variabl govern payo distribut paper abl perform analys term mean varianc except section need translat high expect payo high actual payo move standard denit stationari determinist polici mdp denit let markov decis process state action polici map g later occas dene use nonstationari polici is polici action chosen given state also depend time arriv state mdp combin polici yield standard markov process state say ergod markov process result ergod that is wellden stationari distribu tion develop exposit easiest consid mdp everi polici ergod socal unichain mdp put erman unichain mdp stationari distribut polici depend start state thu consid unichain case simpli allow us discuss stationari distribut polici without cumbersom technic detail turn out result unichain alreadi forc main technic idea upon us result gener nonunichain multichain mdp small necessari chang denit best perform expect learn algo rithm gener multichain mdp given section meantim howev import note unichain assumpt impli everi polici eventu visit everi state even exist singl polici quickli thu exploitationexplor dilemma remain us strongli follow denit nitelength path mdp repeat technic use analysi denit let markov decis process let polici tpath sequenc p probabl p travers upon start state execut polici pr dene two standard measur return polici denit let markov decis process let polici let p path expect undiscount return along r expect discount return along p discount factor make futur reward less valuabl immedi reward tstep undiscount return state u pr tstep discount return state pr case sum path p start i dene u unichain case u i independ i simpli write u furthermor dene optim tstep undiscount return u fu similarli optim tstep discount return also u unichain case u i independ i simpli write u exist limit guarante unichain case final denot maximum possibl step return g undiscount case g discount case g tr max undiscount case mix time easi see seek result undiscount return learn algorithm nite number step need take account notion mix time polici mdp put simpli undiscount case move asymptot return nitetim return may longer wellden notion the optim polici may polici eventu yield high return for instanc nalli reach remot highpayo state take mani step approach high return polici yield lower asymptot return higher shortterm return polici simpli incompar best could hope algorithm compet favor polici amount time compar mix time polici standard notion mix time polici markov decis process quanti smallest number step requir ensur distribut state step within stationari distribut induc distanc distribut measur kullbackleibl diverg variat distanc standard metric furthermor wellknown method bound mix time term second eigenvalu transit matrix p also term underli structur properti transit graph conduct sinclair turn state result weaker notion mix requir expect return step approach asymptot return denit let markov decis process let ergod polici return mix time smallest ju suppos simpli told polici whose asymptot return u exce valu r unknown mdp reward r reward figur simpl markov process demonstr nitetim converg result must account mix time return mix time principl sucient clever learn algorithm for instanc one manag discov quickli could achiev return close u much step convers without assumpt reason expect learn algorithm approach return u mani fewer step simpli may take assum polici order step approach asymptot return exampl suppos two state one action see figur state payo selfloop probabl probabl go state absorb state payo r small return mix time order start state realli requir order step reach absorb state start approach asymptot return r relat notion return mix time standard notion mix time markov decis process n state let ergod polici let smallest valu state i probabl state step within stationari probabl return mix time trmax proof lemma follow straightforward way linear expect omit import point return mix time polynomi bound standard mix time may case substanti smaller would happen instanc polici quickli settl subset state common payo take long time settl stationari distribut within subset thu choos state result undiscount return term return mix time alway translat standard notion via lemma notion return mix time precis type result reason expect undiscount case would like learn algorithm number action polynomi return learn algorithm close achiev best polici among mix time motiv follow denit denit let markov decis process dene t class ergod polici whose return mix time let opt t denot optim expect asymptot undiscount return among polici t thu goal undiscount case compet polici t time polynomi n eventu give algorithm meet goal everi simultan interest special case mix time asymptot optim polici whose asymptot return u time polynomi n algorithm achiev return exceed u high probabl clear that modulo degre polynomi run time result best one could hope gener mdp discount case horizon time discount case quantic polici learn algorithm compet straightforward sinc discount make possibl principl compet polici time proport horizon time word unlik undiscount case expect discount return polici step approach expect asymptot discount return made precis follow lemma markov decis process let polici state i call valu lower bound given horizon time discount mdp proof lower bound v i follow trivial denit sinc expect payo nonneg upper bound x innit path p let r expect payo along path path prex innit path p solv yield desir bound sinc inequ hold everi xed path also hold distribut path induc polici in discount case must settl notion compet slightli dierent undiscount case reason undiscount case sinc total return alway simpli averag learn algorithm recov youth mistak low return earli part learn possibl discount case due exponenti decay eect discount factor ask that time polynomi horizon time learn algorithm polici that current state discount return within asymptot optim state thu time reiniti current state start state learn polici would nearoptim expect return goal algorithm achiev gener mdp discount case main theorem readi describ new learn algorithm state prove main theorem name new algorithm will gener mdp achiev nearoptim perform polynomi time notion perform paramet run time mix horizon time describ preced section eas exposit rst state theorem assumpt learn algorithm given input target mix time optim return opt t achiev polici mix within step for undiscount case optim valu function v i for discount case simpler case alreadi contain core idea algorithm analysi assumpt entir remov section theorem main theorem let markov decis process n state undiscount case recal t class ergod polici whose return mix time bound opt t optim asymptot expect undiscount return achiev t exist algorithm a take input nt opt t total number action comput time taken polynomi n r max probabl least total actual return exce opt t discount case let v i denot valu function polici optim expect discount return exist algorithm a take input n v i total number action comput time taken polynomi n horizon time r max probabl least halt state i output polici remaind section divid sever subsect describ dierent central aspect algorithm proof full proof theorem rather technic underli idea quit intuit sketch rst outlin highlevel sketch proof algorithm although dierenc algorithm analys undiscount discount case easiest think singl algorithm algorithm commonli refer indirect modelbas name rather maintain current polici valu function algorithm actual maintain model transit probabl expect payo subset state unknown mdp import emphas although algorithm maintain partial model may choos never build complet model necessari achiev high return easiest imagin algorithm start call balanc wander mean algorithm upon arriv state never visit befor take arbitrari action state upon reach state visit befor take action tri fewest time state break tie action randomli state visit algorithm maintain obviou statist averag payo receiv state far action empir distribut next state reach that is estim transit probabl crucial notion algorithm analysi known state intuit state algorithm visit so mani time and therefor due balanc wander tri action state mani time transit probabl expect payo estim state veri close true valu import aspect denit weak enough so mani time still polynomi bound yet strong enough meet simul requir outlin shortli fact denit known state achiev balanc shown section state thu divid three categori known state state visit befor still unknown due insuci number visit therefor unreli statist state even visit onc import observ cannot balanc wander indenit least one state becom known pigeonhol principl soon start accumul accur statist state fact state formal section perhap import denit knownstat mdp set current known state current knownstat mdp simpli mdp natur induc full mdp brie y transit state preserv transit redirect lead singl addit absorb state intuit repres unknown unvisit state although learn algorithm direct access virtu denit known state approxim rst two central technic lemma prove section show that appropri denit known state good simul accuraci is expect step return polici close expect step return here either mix time compet against undiscount case horizon time discount case thu time partial model part algorithm know well second central technic lemma section perhap enlighten part analysi name explor exploit lemma formal rather appeal intuit either optim t step polici achiev high return stay high probabl set current known state which importantli algorithm detect replic nding highreturn exploit polici partial model optim polici signic probabl leav within step algorithm detect replic nding explor polici quickli reach addit absorb state partial model thu perform two olin polynomialtim comput section algorithm guarante either nd way get nearoptim return quickli nd way improv statist unknown unvisit state pigeonhol principl latter case cannot occur mani time new state becom known thu algorithm alway make progress worst case algorithm build model entir mdp happen analysi guarante happen polynomi time follow subsect esh intuit sketch abov provid full proof theorem section show remov assum knowledg optim return simul lemma section prove rst two key technic lemma mention sketch section name one sucient accur approxim anoth mdp actual approxim step return polici quit accur step return eventu appeal lemma show accur assess return polici induc knownstat mdp comput return algorithm approxim that is appeal lemma use set import technic point good approxim requir depend polynomi t thu denit known state requir polynomi number visit state begin denit approxim requir denit let markov decis process state space say approxim if state i rm i r state j action a p state prove simul lemma say provid sucient close sens dene step return polici similar lemma simul lemma let markov decis process state undiscount ontg polici t state i u discount case let let ontg approxim polici state i note lemma undiscount case state respect polici whose return mix time oppos return mix time howev return return mix time linearli relat standard eigenvalu argument proof let us x polici start state i let us say transit state state j action small p probabl step state follow polici cross least one small transit nt total probabl small transit state action i n independ opportun cross transit impli total expect contribut either u walk cross least one small transit ntg similarli sinc p impli p approxim total contribut either u walk cross least one small transit thu bound dierenc u u restrict walk eventu determin choic solv thu restrict attent walk length cross small transit note transit satisfi p convert addit approxim p multipl approxim p thu path p that cross small transit path p approxim error payo yield sinc inequ hold xed path travers small transit also hold take expect distribut path induc thu addit term account contribut path travers small transit bound equat upper bound use follow taylor expans complet analysi undiscount case need two condit hold rst condit would satis solv obtain t g t g valu also impli constant therefor satisfi second condit would requir recal earlier constraint given equat choos nd t g satis choic given lemma similar argument yield desir lower bound complet proof undiscount case analysi discount case entir analog except must addit appeal lemma order relat step return asymptot return simul lemma essenti determin denit known state be one visit enough time ensur with high probabl estim transit probabl estim payo state within ontg valu follow lemma whose proof straightforward applic cherno bound make translat number visit state desir accuraci transit probabl payo estim lemma let markov decis process let state visit least time action execut least bmkc time i let p ij denot empir probabl transit estim obtain visit i probabl least p state j action a rm i rm var i maximum varianc random payo state thu get formal denit known state denit let markov decis process say state known action execut least time explor exploit lemma lemma indic degre approxim requir sucient simul accuraci led denit known state let denot set known state specifi straightforward way known state dene induc mdp induc mdp addit new state intuit repres unknown state transit denit let markov decis process let subset state induc markov decis process s denot state fs g transit payo dene follow state s rm payo determinist zero varianc even payo stochast action a p ms absorb state state action a p ms ij thu transit state preserv state action a p ms is p ij thu transit state redirect denit describ mdp directli induc true unknown mdp preserv true transit probabl state s cours algorithm approxim transit probabl lead follow obviou approxim denot obviou empir approxim natur approxim follow lemma establish simul accuraci immedi lemma lemma lemma let markov decis process let set current known state probabl least undiscount case polici t ms state i u ms ms ms discount case let polici state i ms i v ms i v ms that is state simpli state visit far transit probabl observ transit frequenc reward observ reward let us also observ return achiev and thu approxim achiev achiev real world lemma let markov decis process let set current known state polici state s u ms ms proof follow immedi fact ident s expect payo nonneg outsid payo possibl heart analysi identi part unknown mdp algorithm know well form approxim key lemma follow demonstr fact and thu simul lemma must alway provid algorithm either polici yield larg immedi return true mdp polici allow rapid explor unknown state or both lemma explor exploit lemma let markov decis pro cess let subset state let induc markov decis process s either exist polici u ms respect v ms exist polici probabl walk step follow termin proof give proof undiscount case argument discount case analog let polici satisfi u u suppos u ms wit claim lemma may write u pr pr r pr sum over respect path p start state path q start state everi state q s path r start state least one state s keep interpret variabl p q r xed may write pr pr ms qu ms q u ms i equal follow fact path q everi state s pr ms q um inequ fact u ms take sum path avoid absorb state thu pr impli x r pr r pr r pr x r pr desir olin optim polici comput let us take moment review synthes combin lemma establish basic line argument time set current known state step return polici lower bound step return ani extens of time must either polici whose step return nearli optim must polici quickli reach absorb state case polici execut quickli reach state current known set s section discuss two olin polynomialtim comput nd polici highest return the exploit polici one highest probabl reach absorb state step the explor polici essenti follow fact standard valu iter algorithm dynam program literatur abl nd step optim polici arbitrari mdp n state on comput step discount undiscount case sake complet present undiscount discount valu iter algorithm bertseka tsitsikli below optim step polici may nonstationari denot sequenc i optim action taken state th step tstep undiscount valu iter initi i u i ms ms iju t j ms ms iju t j undiscount valu iter work backward time rst produc optim polici time step optim polici time step on observ nite polici maxim cumul step return also maxim averag step return tstep discount valu iter initi ms ms ijv t j ms ms ijv t j again discount valu iter work backward time rst produc optim polici time step optim polici time step on note total comput involv on discount undiscount case use valu iter straightforward certain point execut algorithm perform valu iter olin twice use either undiscount discount version depend measur return second time denot on comput use undiscount valu iter regardless measur return transit probabl dierent payo absorb state payo r max state payo thu reward explor as repres visit rather exploit polici return valu iter polici return valu iter guarante either step return current known state approach optim achiev which assum know thu detect probabl execut reach unknown unvisit state step signic probabl which also detect put togeth technic piec need place give detail descript algorithm tie loos end section remov assumpt know optim return achiev sequel use express balanc wander denot step algorithm current state known state algorithm execut action tri fewest time current state note state becom known denit never involv step balanc wander again use known denot number visit requir state becom known state dierent undiscount discount case given denit call algorithm explicit explor exploit or whenev algorithm engag balanc wander perform explicit olin comput partial model order nd step polici guarante either exploit explor descript follow freeli mix descript step algorithm observ make ensu analysi easier digest explicit explor exploit initi initi set known state empti balanc wander time current state s algorithm perform balanc wander discoveri new known state time state visit known time balanc wander enter known set s longer particip balanc wander observ clearli nm known step balanc wan dere pigeonhol principl state becom known worst case term time requir least one state becom known gener total number step balanc wander algorithm perform ever exce nm known everi state known even step balanc wander consecut known state account known step balanc wander olin optim upon reach known state balanc wander algorithm perform two olin optim polici comput describ section attempt exploit result exploit polici achiev return least u respec tive discount case least v i algorithm execut next step respect halt output given mix time given algorithm input respect horizon time attempt explor otherwis algorithm execut result explor polici deriv olin comput step lemma guarante probabl least g leav set s balanc wander time attempt exploit attempt explor visit state s algorithm immedi resum balanc wander observ thu everi action taken algorithm either step balanc wander part step attempt exploit attempt explor conclud descript algorithm wrap analysi one main remain issu handl condenc paramet statement main theorem undiscount discount case theorem ensur certain perform guarante met probabl least essenti three dierent sourc failur algorithm known state algorithm actual poor approxim nextstat distribut action thu sucient strong simul accuraci repeat attempt explor fail yield enough step balanc wander result new known state undiscount case onli repeat attempt exploit fail result actual return near u handl failur probabl simpli alloc sourc failur fact make probabl rst sourc failur a bad known state control small quanti lemma formal use lemma meet requir state known simultan second sourc failur fail attempt explor standard cherno bound analysi suce lemma attempt explor view independ bernoulli trial probabl least g least one step balanc wander worst case must make everi state known exploit requir nm known step balanc wander probabl fewer nm known step balanc wander smaller number t step attempt explor og nish analysi discount case discount case ever discov polici whose return current state close v i attempt exploit algorithm nish argument alreadi detail sinc with high probabl accur approxim part must nearoptim polici well lemma long algorithm nish must engag balanc wander attempt explor alreadi bound number step with high probabl everi state known set s contain state actual accur approxim entir mdp lemma ensur exploit must possibl sinc explor not emphas case eventu contain state worst case analysi algorithm may discov abl halt nearoptim exploit polici long ever occur use valu known given discount case denit total number action execut algorithm discount case thu bound time maximum number attempt explor given equat bound total comput time bound on the time requir olin comput time maximum number attempt explo ration give undiscount case thing slightli complic sinc want simpli halt upon nding polici whose expect return near u want achiev actual return approach u third sourc failur fail attempt exploit enter alreadi argu total number step attempt explor algorithm perform contain state polynomi bound action algorithm must account step attempt exploit step attempt exploit expect return least u probabl actual return restrict attempt exploit less u made smaller number block exce o log standard cherno bound analysi howev also need make sure return restrict exploit block sucient domin potenti low return attempt explor dicult show provid number attempt exploit exce og time number attempt explor bound equat condit satis total number action bound ot time number attempt explor total comput time thu on t time number attempt explor thu bound conclud proof main theorem remark seriou attempt minim worstcas bound made immedi goal simpli prove polynomi bound straightforward manner possibl like practic implement base algorithm idea given would enjoy perform natur problem consider better current bound indic see moor atkeson relat heurist algorithm elimin knowledg optim return mix time order simplifi present main theorem made assumpt learn algorithm given input target mix time optim return opt t achiev mix time in undiscount case valu function v i in discount case horizon time impli knowledg discount factor section sketch straightforward way assumpt remov without chang qualit natur result brie discuss altern approach may result practic version algorithm let us begin note knowledg optim return opt t v i use attempt exploit step algorithm must compar return possibl current state best possibl entir unknown mdp absenc knowledg explor exploit lemma lemma ensur us safe bia toward explor precis time arriv known state i rst perform attempt explor olin comput modi knownstat mdp describ section obtain optim explor polici sinc simpl matter comput probabl reach absorb state step compar probabl lower bound long lower bound exceed may attempt visit state s lower bound guarante olin comput attempt exploit step must result exploit polici close optim befor discount case halt output undiscount case execut continu note explorationbias solut remov knowledg result algorithm alway explor state reach reason amount time ex ploitat although simpl way remov knowledg keep polynomialtim algorithm practic variant algorithm might pursu balanc strategi standard approach strong bia toward exploit instead enough explor ensur rapid converg optim perform instanc maintain schedul t total number action taken algorithm far upon reach known state algorithm perform attempt exploit execut attempt explor execut t choic analys ensur still explor enough will polynomi time contain polici whose return near optim return enjoy meantim may much greater explorationbias solut given abov note approach similar spirit greedi method augment algorithm qlearn explor compon crucial dierenc greedi explor probabl t attempt singl action design visit rare visit state propos probabl t execut multistep polici reach unknown state polici provabl justi undiscount case still remain remov assumpt algorithm know target mix time inde would like state main theorem valu is long run algorithm number step polynomi paramet total return exceed opt t probabl easili accomplish ignor paramet alreadi algorithm at that given input run p t step xed polynomi p meet desir criterion propos new algorithm need input simpli run sequenti amount time must run execut at still polynomi need run sucient mani step rst step domin lowreturn period took place p t step similar analysi done undiscount case toward end section note solut sucient polynomi time far one would implement practic instanc would clearli want modifi algorithm mani sequenti execut share accumul common partial model multichain case main issu extend result arbitrari multichain mdp asymptot undiscount return polici independ start state make undiscount case multichain mdp look lot like usual discount case inde result extend arbitrari multichain mdp discount case without modic therefor one way deal undiscountedcas multichain mdp ask given polynomi time algorithm state polici expect return nearoptim state anoth way modifi expect compet polici instead expect compet largest asymptot return start state polici compet lowest asymptot return start state polici thu modifi denit follow markov decis process let polici return mix time smallest ij i definit let arbitrari markov decis process dene class polici whose return mix time let opt t i optim expect asymptot undiscount return among polici t rene denit undiscountedcas result unichain mdp extend without modic arbitrari mdp futur work number interest line research practic implement although polynomi bound proven far larg immedi claim practic relev algorithm feel underli algorithm idea promis eventu result competit algorithm current examin practic issu choic aris implement discuss brie section hope report implement experi soon modelfre version partial relat last item would nice nd algorithm similar requir maintain partial model polici or perhap sever current investig well larg state space would interest studi applic recent method deal larg state space function approxim algorithm recent investig context factor mdp kearn koller acknowledg give warm thank tom dean tom dietterich tommi jaakkola lesli kaelbl michael littman lawrenc saul terri sejnowski rich sutton valuabl comment satind singh support nsf grant ii portion work done univers colorado boulder r sequenti decis problem neural network dynam program determinist stochast model parallel distribut compu tation numer method athena scienti expect mistak bound model onlin reinforc learn stabl function approxim dynam program ming converg indirect adapt asynchron valu iter algorithm converg stochast iter dynam program algorithm reinforc learn algorithm partial observ markov decis problem distribut asynchron algorithm expect averag cost dynam program stochast system estim markov decis process learn curv bound markov decis process undiscount reward worstcas analysi temporaldier learn algorithm machin learn algorithm random gener count markov chain approach analyt mean squar error curv tempor di reinforc learn soft state aggreg converg result singlestep onpolici reinforc learn algo rithm reinforc learn replac elig trace gener reinforc learn success exampl use spars coars code reinforc learn introduc tion role explor learn control asynchron stochast approxim q learn learn delay reward tr ctr david wingat kevin d seppi pvi partit priorit parallel valu iter proceed twentyfirst intern confer machin learn p juli banff alberta canada alexand l strehl michael l littman theoret analysi modelbas interv estim proceed nd intern confer machin learn p august bonn germani alexand l strehl lihong li eric wiewiora john langford michael l littman pac modelfre reinforc learn proceed rd intern confer machin learn p june pittsburgh pennsylvania shie mannor duncan simest peng sun john n tsitsikli bia varianc valu function estim proceed twentyfirst intern confer machin learn p juli banff alberta canada revelioti theologo bountour effici pac learn episod task acycl state space discret event dynam system v n p septemb carlo diuk alexand l strehl michael l littman hierarch approach effici reinforc learn determinist domain proceed fifth intern joint confer autonom agent multiag system may hakod japan eyal evendar shie mannor yishay mansour action elimin stop condit multiarm bandit reinforc learn problem journal machin learn research p daniela pucci de faria nimrod megiddo combin expert advic reactiv environ journal acm jacm v n p septemb pieter abbeel andrew y ng explor apprenticeship learn reinforc learn proceed nd intern confer machin learn p august bonn germani daniela pucci de faria benjamin van roy costshap linear program averagecost approxim dynam program perform guarante mathemat oper research v n p august amol deshpand zachari ive vijayshankar raman adapt queri process foundat trend databas v n p januari