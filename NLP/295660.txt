t spacetimeeffici schedul execut parallel irregular comput a articl investig tradeoff time space effici schedul execut parallel irregular comput distributedmemori machin employ acycl task depend graph model irregular parallel mix granular use direct remot memori access support fast commun propos new schedul techniqu runtim activ memori manag scheme improv memori util retain good time effici provid theoret analysi correct perform work implement context rapid system use inspectorexecutor approach parallel irregular comput runti me demostr effect propos techniqu sever irregular applic spars matrix code fast multipol method particl simul experiment result crayt show problem larg size solv limit space capac loss execut effici caus extra memori manag overhead reason b introduct consider effort parallel system research spent timeeffici par allel articl investig tradeoff time space effici execut irregular comput memori capac processor limit sinc timeeffici parallel may lead extra space requir definit irregular comput literatur actual clear normal scientif comput code chaotic adapt comput commun pattern consid irregular provid effect system support low overhead irregular problem difficult identifi one key issu parallel system research kennedi number project address softwar techniqu parallel differ class irregular code wen et al da et al lain banerje gerasouli author address t yang depart comput scienc univers california santa barbara ca c fu siemen pyramid mailstop sj san jose ca work support part nsf ccr cda int ccr darpa subcontract umd no z preliminari version articl appear th acm symposium principl practic parallel program ppopp yang c fu et al fink et al articl address schedul issu parallel model direct acycl depend graph dag sarkar mix granular model found use perform predict code optim parallel applic static slowli chang depend pattern spars matrix problem particl simul chong et al fu yang b gerasouli et al jiao problem commun comput irregularli interleav vari granular asynchron schedul fast commun techniqu need exploit data local balanc load low synchron cost howev techniqu may impos extra space requir exampl direct memori access normal much lower softwar overhead highlevel messagepass primit remot address howev need known time perform data access thu access remot space must alloc advanc schedul optim techniqu may prefetch presend data object overlap comput commun may requir extra temporari space hold data object therefor use advanc optim techniqu add difficulti design softwar support achiev high util processor memori resourc articl present two dag schedul algorithm minim space usag retain good parallel time effici basic idea use volatil object earli possibl space releas reus design activ memori manag scheme increment alloc necessari space processor effici execut dag schedul use direct remot memori access provid analysi perform correct techniqu propos techniqu implement rapid program tool fu yang a parallel irregular applic runtim origin schedul execut scheme rapid support increment memori allo cation suffici space rapid deliv good perform sever test irregular program spars choleski factor spars triangular solver fu yang fast multipol method nbodi simul fu particular show rapid use parallel spars lu gaussian elimin dynam partial pivot import open parallel problem literatur deliv high megaflop craytdt fu yang b anoth usag rapid perform predict sinc static schedul rapid predict potenti speedup given dag reason accuraci exampl rapid spars lu code achiev predict speedup fu yang b found size problem rapid solv restrict avail amount memori motiv us studi space optim note form space overhead exist space oper system kernel hash tabl index irregular object task graph articl focus optim space usag dedic store content data object rest articl organ follow section summar relat work section describ comput memori model section present spaceeffici schedul algorithm section discuss use t yang c fu delta schedul algorithm rapid activ memori manag scheme execut schedul section give experiment result relat work previou research static dag schedul sarkar wolski feo yang gerasouli address memori issu schedul algorithm dynam dag propos blelloch et al requir space usag processor sequenti space re quirement p total number processor depth dag work provid solid theoret ground spaceeffici schedul space model differ our sinc assum global share memori pool model space upper bound impos individu processor stronger constraint cilk runtim system blumof et al address space effici issu space complex os per processor good gener high solv larg problem space limit memoryoptim techniqu propos articl space usag close p per processor anoth distinct differ work blelloch et al blumof et al rapid dag obtain runtim inspector stage execut thu make schedul scheme static model dag grow onthefli comput proce dynam schedul prefer two reason use static schedul practic difficult minim runtim control overhead dynam schedul parallel spars code mix granular distribut memori machin applic problem consid iter natur optim static schedul use mani iter work use hardwar support directli access remot memori avail sever modern parallel architectur workstat cluster ibel et al stricker et al schauser scheiman advantag direct remot memori access identifi fast commun research activ messag von eicken et al thu expect research benefit result use fast commun support design softwar layer comput memori model comput model consist set task set distinct data object task read write subset data object data depend graph ddg deriv partit code normal three type depend task true depend antidepend output depend poli chronopoulo ddg anti output depend edg may redund subsum true data depend edg antioutput depend edg elimin program transform articl deal transform depend graph contain acycl true depend extens classic task graph model commut task mark task graph captur parallel aris commut oper detail parallel model describ fu yang a defin term use task model follow yang c fu a dag contain set task direct depend edg task data object access task let rt x set object task x read w t x set object task x write each data object assign uniqu owner processor processor may alloc temporari space object own processor p x own m call perman object p x call volatil object p x defin permp x set perman data object processor p x v olat ilep x set volatil data object processor p x perman object stay alloc execut owner processor a static schedul give processor assign task execut order task processor defin tap x set task execut processor p x let term t x execut processor schedul let t x denot task x execut immedi given dag g static schedul g schedul graph deriv mark execut edg g eg add edg x g x schedul legal correspond schedul graph acycl x depend edg g each task x weight denot predict comput time call x complet task edg x weight denot predict commun delay call c xy send correspond messag two processor use weight inform static schedul algorithm optim expect parallel time executiont a b c fig a dag b schedul dag processor c anoth schedul t yang c fu delta figur a show dag task access data object task repres either ij read j write j j read write j tabl list read write set task part b c figur two schedul dag a perman object evenli distribut two processor g g owner comput rule use assign task processor exampl assum task messag cost one unit time messag sent asynchron processor overhead send receiv messag ignor tabl i read write set task figur a notic task graph model use ssa form static singl assign cytron et al notic differ task modifi data object main reason target applic task graph deriv runtim normal data depend ssa form use would mani object manag spacetimeeffici schedul algorithm need balanc two conflict goal shorten length schedul minim space requir memori suffici hold object execut stage space recycl volatil data necessari follow two strategi use releas space volatil data object certain execut point valu object longer use object longer access discuss section second strategi reduc overhead data manag therefor choos approach strategi space requir estim follow definit given task execut order processor volatil object processor call live task access task access still access futur formal follow hold live x z t x otherwis call dead definit let sizem size data object m task tw processor p x ie tw tap x comput volatil space requir tw p x live volatil object tw yang c fu memori requir schedul px f schedul figur b volatil object processor p dead task dead assum data object unit size easi calcul mem howev schedul figur c mem req lifetim volatil object disjoint p space share possibl space time effici schedul given dag p processor schedul approach contain two stage the owner comput rule use assign task processor task modifi data object map cluster cluster evenli assign processor exampl given dag figur a two processor set task assign processor p g set task processor p ft g base task assign ment determin perman volatil data object processor task processor order follow dag depend edg focu optim task order previous order algorithm call yang gerasouli propos time effici may requir extra space hold volatil object order aggress execut timecrit task section propos two new order algorithm call mpo dt idea volatil object referenc earli possibl avail local memori shorten lifetim volatil object potenti reduc memori requir processor rcp algorithm briefli discuss algorithm detail descript found yang gerasouli heurist order task simul execut task let exit task task children given dag time prioriti task x call tp t x length longest path task exit task name tp t x x exit task otherwis ty child simul execut task call readi parent select execut need data object receiv point schedul cycl processor select readi task highest time prioriti t yang c fu delta least one unschedul task find processor px earliest idl time schedul readi task tx highest prioriti processor px updat readi task list processor endwhil fig rcp algorithm rcp algorithm summar figur line exist descript want illustr differ rcp mpo mpo memorypriorityguid order mpo order algorithm list figur differ mpo rcp prioriti use mpo select readi task size total object space alloc divid size total object space need execut task if tie rcp time prioriti tp use break tie mpo need estim total object space alloc schedul cycl updat memori prioriti task line least one unschedul task find processor px earliest idl time schedul readi task tx highest prioriti processor px alloc volatil object tx use alloc yet processor px updat prioriti tx children sibl processor px updat readi task list processor endwhil fig mpo algorithm exampl figur c show schedul produc mpo b produc rcp order differ figur b c start time processor time select rcp chosen mpo illustr figur three readi task processor time select longest path exit task the path length mpo highest space prioriti avail local time s space prioriti space alloc time assum object unit size result mpo schedul requir less memori longer parallel time algorithm complex potenti time consum part updat space prioriti unschedul task line suffici updat space prioriti children sibl newli schedul task task possibl candid readi task next round space prioriti children candid task updat later candid task schedul use e x e x denot number yang c fu readi task list proc a b fig schedul scenario time number parenthes next readi task mpo space prioriti rcp time prioriti time a remain unschedul task b partial schedul time incom edg outgo edg task x respect complex line o number task e number depend edg term e x v use v bound number children parent x may have addit complex line v log p v log v m vp log v respect here p total number processor total number data object sinc p usual small compar v total time complex mpo ove dt dataaccessdirect time slice dt aggress space optim thu intend case memori usag primari import design base fact memori usag processor improv volatil object short life span word time period alloc dealloc short basic idea dt slice comput base data access pattern task task within slice access small group volatil object task schedul physic processor slice slice task within slice order use depend criticalpath inform algorithm given dag set task oper set data object describ step dt algorithm follow construct data connect graph dcg node dcg repres distinct data object edg repres tempor order data access comput cycl may occur access two data object interleav simplic use name data object correspond data node unless caus confus construct dcg follow rule appli base origin dag if task x v use modifi data object x associ data object node t yang c fu delta if x modifi one object may use object compu tation use object x associ modifi object it possibl task associ multipl data node case doubli direct edg ad among data node make strongli connect a direct edg ad data node data node j exist task depend edg t x x associ data node associ data node j last two rule reflect tempor order data access comput step deriv strongli connect compon dcg edg compon constitut dag task appear one compon compon associ set task usemodifi data object compon defin one slice task slice consid schedul togeth runtim processor execut task slice slice follow topolog order slice impos depend among correspond strongli connect compon note topolog order slice impos constraint task order processor assign task follow owner comput rule must suppli use dt produc actual schedul step use prioriti base preced schedul approach gener dt schedul slice deriv step prioriti assign task base slice belong to two readi task slice task higher criticalpath prioriti schedul first readi task slice prioriti lower unschedul task processor task schedul task higher slice prioriti processor schedul use prioriti guarante task execut accord deriv slice order exampl figur show exampl dt order dag figur a part a dcg mark node correspond data name task within rectangl associ correspond data object sinc dcg acycl data node maxim strongli connect compon treat one slice topolog order node produc slice processor execut task follow slice order shown part b slice mark numer illustr execut order memori requir mem req compar figur b produc rcp figur c mpo hand schedul length increas rcp mpo dt less less critic path inform use algorithm complex step complex deriv access pattern task ie read andor write access data object oe log v complex map task data node ov complex gener edg data node oe log m check need prevent duplic edg ad thu total complex step v step deriv strongli connect compon cost yang c fut slice slice slice slice slice slice slice slice slice slice slice slice b a fig a sampl dcg deriv dag b dt schedul dag processor gener preced edg among slice cost om log m topolog sort slice cost om e therefor total complex step om m cost step ov log v e give overal complex dt oelog v number task e number edg number data object origin dag space effici dt lead good memori util follow theorem give memori bound dt first definit introduc definit given processor assign r task volatil space requir slice l processor p x denot vpx r l defin amount space need alloc volatil object use execut task l p x maximum volatil space requir l r defin assum processor assign r task use owner comput rule produc even distribut data space perman data object among processor show follow result theorem given processor assign r task dt schedul processor slice order l schedul execut space usag per processor sequenti space complex proof first all sinc r lead even distribut perman object perman data space need processor p suppos task t yang c fu delta x slice l need alloc space volatil object d enough space accord definit h claim space alloc volatil data object associ slice l freed therefor extra h space processor enough execut task l need show claim correct suppos not volatil data object cannot dealloc slice l igamma associ l i least one task l j j i use modifi perman data object modifi accord dt algorithm belong slice l instead l j thu contradict dcg acycl data node dcg constitut strongli connect compon therefor slice associ one data object impli h defin theorem size largest data object thu follow corollari corollari dcg task graph acycl maximum size object unit dt produc schedul execut processor use per processor sequenti space complex appli theorem corollari import applic graph columnblockbas spars lu dag fu yang b matrix partit set spars column block task kj use one spars column block k modifi column block j figur actual spars lu graph dt produc acycl dcg columnblockbas spars lu task graph illustr figur a let w maximum space need store column block accord corollari processor need w volatil space execut dt schedul spars lu blockbas spars choleski approach describ fu yang matrix divid n spars column block column block divid n submatric submatrix index i j mark ij choleski task graph structur n layer layer repres elimin process use column block k modifi column block k n specif choleski factor comput diagon block kk use scale nonzero submatric within kth column block ie ik nonzero submatric use updat rest matrix ie ij updat task step k belong slice associ data object ik henc extra space need execut slice summat submatric column block k accord theorem dt schedul choleski execut p w space processor w maximum space need store column block result summar follow corollari normal w dt schedul space effici two problem corollari columnblockbas spars lu task graph blockbas spars choleski graph dt schedul execut use yang c fu space i k merg l l space els space fig dt slicemerg algorithm per processor w size largest column block partit input matrix optim avail memori space processor known say av ail mem time effici dt algorithm optim merg sever consecut slice memori suffici slice appli prioritybas schedul algorithm merg slice assum k slice valid slice order given task assign r merg strategi summar figur set new slice gener sinc calcul memori requir slice take oe log m time complex merg process ove log m shown merg algorithm produc optim solut given slice order theorem given order slice sequenc slicemerg algorithm figur produc solut minimum number slice proof theorem proven contradict let new slice sequenc produc algorithm figur optim sequenc u e f slice contain set consecut l slice l merg algorithm figur group mani first l slice possibl hre take origin l slice slice f add f new f ident e let new f call thu produc optim sequenc appli transform f compar final transform sequenc f anoth optim sequenc contradict sinc new sequenc cannot complet cover l slice unless slice empti note optim merg algorithm restrict given slice order interest topic studi exist slice sequenc algorithm follow partial order impli given dcg lead minimum number slice minimum parallel time shown tang heurist use binpack techniqu develop number slice within factor two optimum t yang c fu delta memori manag schedul execut section first briefli describ rapid runtim system schedul techniqu appli then discuss necessari runtim support effici execut dag schedul deriv propos schedul algorithm rapid system depend transform analysi depend task schedul cluster user specif task data object data access pattern data depend graph ddg complet task graph iter asynchron task assign data object owner schedul execut fig process runtim parallel rapid figur show runtim parallel process rapid circl action perform system box either side circl repres input output action api rapid includ set librari function specifi irregular data object task access object inspector stage depict left three circl figur rapid extract dag data access pattern produc execut schedul executor stage the rightmost circl schedul comput execut it er sinc target applic iter natur exampl spars matrix factor use extens solv set nonlinear differenti equat numer method newtonraphson iter spars depend graph deriv jacobian matrix spars pattern jacobian matrix remain one iter anoth nine applic studi karmarkar typic number iter execut comput graph rang averag thu optim cost spent inspector stage pay long simul problem shown fu rapid inspector stage test spars matrix factor triangular solver fast multipol method fmm rel larg problem size take total time schedul reus iter inspector idea found previou scientif comput research georg liu inspector optim call preprocess compar previou in spectorexecutor system irregular comput da et al executor phase rapid deal complic depend structur executor stage rapid use direct remot memori access rma execut schedul deriv inspector stage rma avail modern multiprocessor architectur crayt shmem meiko cs dma sci cluster memori map rma processor write memori processor given remot address rma allow data transfer directli sourc destin locat without buffer impos much lower overhead higherlevel commun layer mpi use rma complic design runtim execut control data consist yang c fu howev find dag gener rapid satisfi follow properti simplifi design distinct data object task x receiv data object identif differ parent readwrit depend path either x x depend path either x x d dag sequenti task execut consist sequenti execut follow topolog sort dag name rt x valu x read memori execut produc one x s parent gener dag satisfi d d may alway sequentializ yang dag properti call dependencecomplet exampl dag discuss figur section dependencecomplet ddg deriv sequenti code transform dependencecomplet dag fu yang execut scheme activ memori manag maintain reus data space execut new research topic complic use lowoverhead rmabas commun sinc remot data address must known advanc discuss two issu relat execut dag schedul rma address consist address data object processor becom stale valu data object longer use space object releas use classic cach coher protocol maintain address consist introduc substanti amount overhead taken simpl approach volatil object consid dead object name access processor way volatil object name alloc processor strategi lead slightli larger memori requir reduc complex maintain address consist memori requir estim section follow design strategi address buffer also use rma transfer address sinc address packag sent infrequ use address buffer processor cannot send new address inform unless destin processor read previou address packag reduc manag overhead execut model use activ memori manag scheme present below map memori alloc point insert dynam two consecut task execut processor first map alway begin execut processor map follow dealloc space dead volatil object dead inform static calcul perform data flow analysi given dag complex proport size graph t yang c fu delta map alloc map alloc d addr addr suspend send d suspend map alloc addr send send a map stop data object readi map ye rec end ra cq next task send addr complet comput b fig a map execut schedul figur c bthe control flow processor alloc volatil space task execut current point execut chain assum remain task processor alloc stop k enough space execut k next map right k assembl address packag processor address packag may differ depend object access processor figur a illustr map address notif execut schedul figur c avail amount memori processor unit memori volatil object p addit map begin task chain anoth map right task space freed space alloc address p sent p p send content p receiv address figur b show control flow execut scheme system five differ state execut rec wait receiv desir data object processor rec state cannot proceed object current task need avail local exe execut task snd send messag remot address messag avail messag enqueu map processor could block map state attempt send address packag processor previou address packag consum destin processor end state processor execut task still need clear send queue might block address suspend yang c fu messag still unavail three block state ie state selfcycl figur b follow two oper must conduct frequent order avoid deadlock make execut evolv quickli ra read new address packag deliv suspend messag address avail analysi deadlock consist theorem given dag g legal schedul execut activ memori manag deadlock free name system eventu execut task proof assum commun processor reliabl prove theorem induct use follow two fact proof fact deadlock situat happen processor block wait cycl eg circular chain state rec map end eventu processor circular chain two thing ra cq space alloc releas activ complet ani fact processor wait receiv data object local address data object must alreadi notifi processor processor alway alloc space send address object use object let g schedul graph g g acycl given schedul legal without loss gener assum topolog sort g produc linear task order induct follow order induct base must entri task g ie task without parent execut processor call map execut complet space alloc p x send newli creat address deadlock occur processor involv circular wait chain p x block state map await avail address buffer destin processor ra cq sinc destin processor must circular chain also ra cq accord fact address buffer eventu free newli creat p x sent out p x abl leav map state parent data need avail local henc complet success induct assumpt assum task x x execut k parent g complet execut show k execut success suppos not ie deadlock occur let p x k s processor state p x either map rec cannot state end discuss follow two case case p x state rec induct assumpt reason p x cannot receiv data object k object sent t yang c fu delta remot processor p sinc k s parent finish caus p send data object unavail remot address p x accord fact address must alreadi sent p p x wait receiv object henc p eventu read address oper ra fact deliv messag p x therefor p x execut k case p x state map situat one discuss induct base processor abl leav map state theorem given dependencecomplet dag g legal schedul execut activ memori manag consist name task read data object produc parent specifi g proof theorem task execut runtim scheme depend edg t x check inde read object produc x execut illustr figur two case could read inconsist copi m prove contradict imposs assum schedul processor p x schedul p x time time depend pathedg writesend object case case fig illustr two case prove theorem case sendersid inconsist p p x execut x p x tri send p messag may suspend destin address may avail sinc buffer use content p x may modifi actual sent p time t assum case true let u task overwrit processor sent p u must intend produc anoth task v p x sinc execut u x happen v accord properti d must exist depend path x v u accord properti d must exist depend path u x yang c fu exist depend path x u order among x u sequenti execut must x would abl read copi produc x contradict properti d exist depend path u x similarli show v would abl read produc u sequenti execut contradict properti d case receiversit inconsist object produc x success deliv local memori p content p may overwritten anoth task call u time t assum case true let v task assign p task suppos read produc u accord properti d v illustr figur accord properti d depend structur among case similarli find contradict experiment result implement propos schedul heurist activ memori manag scheme rapid craytdt meiko cs section report perform approach te three irregular program spars choleski factor block data map rothberg rothberg schreiber fu yang task graph static depend structur long nonzero pattern spars matrix given runtim preprocess stage spars gaussian elimin lu factor partial pivot problem unpredict depend storag structur due dynam piv ote parallel sharedmemori platform address li howev effici parallel distributedmemori machin still remain open problem scientif comput literatur use static factor approach estim worstcas depend structur storag need fu yang b show approach overestim space much test matric rapid code deliv breakthrough perform tdte fast multipol method fmm simul movement nonuniformli distribut particl given irregular particl distribut spatial domain divid box differ level repres hierarch tree leaf contain number particl iter particl simul fmm comput consist upward downward pass tree end iter particl may move one leaf anoth comput commun weight dag repres fmm comput may chang slightli sinc particl movement normal slow dag repres fmm comput reus mani iter found jiao static schedul reus approxim iter without much perform degrad recent optim code use special schedul mechan elimin rapid control overhead set new perform record shen et al t yang c fu delta detail descript fmm parallel use rapid found fu first examin memorymanag scheme impact parallel perform space limit studi effect schedul heurist reduc memori requir reason present order propos schedul algorithm effect without proper runtim memori manag present order also allow us separ impact runtim memori manag new schedul algorithm te machin use mb memori per node bla gemm routin dongarra et al achiev megaflop rma primit shmem put achiev s overhead mbsec peak bandwidth test matric use articl harwellbo matrix bcsstk aris structur engin analysi spars choleski goodwin matrix fluid mechan problem spars lu matric medium size solvabl one three schedul heurist compar perform fmm use distribut k particl experi test case reach similar conclus report parallel time differ memori constraint manual control avail memori space processor tot tot total memori space need given task schedul without space recycl obtain tot calcul sum space perman volatil object access processor let tot maximum valu among processor rapid without activ memori manag perform spars choleski factor te processor perform fast multipol method te processor a b fig speedup without memori optim a spars choleski b fmm bcsstk dimens million nonzero includ fillin goodwin dimens million nonzero includ fillin yang c fu tabl ii absolut perform megaflop spars lu partial pivot matrix p p p p p p goodwin rapid without memori manag figur tabl ii show overal perform rapid te without use memori optim three test program version rapid recycl space executor stage result serv comparison basi assess perform memori manag scheme note speedup choleski fmm compar highqual sequenti code result consist previou work rothberg jiao speedup choleski reason sinc deal spars matric speedup fmm high leaf node fmm hierarch tree normal computationintens suffici parallel spars lu sinc approach use static symbol factor overestim comput list megaflop perform calcul megaflop use accur oper count superlu li divid correspond numer factor time rapid activ memori manag tabl iii examin perform degrad use activ memori manag rcp still use task order show later much improv space effici obtain use dt mpo result tabl spars choleski bcsstk spars lu goodwin fmm differ space constraint column pt inc ratio parallel time increas use memori manag scheme comparison base parallel time rcp schedul memori avail without memori manag overhead entri mark impli correspond schedul nonexecut memori constraint result basic show trend perform degrad increas number processor increas avail memori space decreas overhead contribut address notif space recycl howev degrad reason consid amount memori save exampl memori manag scheme save space choleski parallel time degrad observ schedul like execut reduc memori capac number processor increas processor lead volatil object processor give memori manag scheme flexibl alloc dealloc space even maximum memori requir schedul activ memori manag still execut processor rapid without support fail execut tabl iii also list averag number map requir one processor processor use fewer map requir sinc less space need store perman object processor note fmm execut time activ memori manag sometim even shorter without memori manag explan t yang c fu delta tabl iii perform degrad use activ memori manag map pt inc map pt inc map pt inc p p lu map pt inc map pt inc map pt inc fmm map pt inc map pt inc map pt inc p although comput associ leaf node particl partit tree intens mix much intens commun incur downward upward pass compar choleski lu interprocessor messag fmm downward upward pass insert memorymanag activ enlarg gap consecut commun messag lead less network content overhead map three type memori manag activ result time increas ra cq map experi found deliveri address packag map never hinder wait previou content address buffer consum tabl iv report overhead impos map clear overhead insignific compar total time increas studi tabl iii howev activ frequent address checkingdeliv oper prolong messag send caus execut delay task critic path effect comparison memoryschedul heurist subsect compar memori time effici rcp mpo dt memori scalabl first examin much memori save use mpo dt defin memori scalabl or memori reduct ratio sequenti space requir p space requir per processor schedul produc algorithm p processor delta t yang c fu tabl iv map overhead term percentag total execut time p comparison memori requir spars choleski processor memori requir reduct ratio x mpo comparison memori requir spars lu processor memori requir reduct ratio x mpo a b comparison memori requir fmm processor memori requir reduct ratio x mpo c fig memori scalabl comparison three schedul heurist a spars choleski b spars lu c fmm figur show memori reduct ratio three schedul algorithm choleski lu fmm uppermost curv graph p perfect memori scalabl figur show mpo dt significantli reduc memori requir dt memori requir close optimum choleski lu case consist corollari hand rcp time effici memori scalabl particularli spars lu fmm find dt algorithm result singl slice ie task belong slice reason lot t yang c fu delta depend among task dt actual reduc rcp thu experi show allow complex increas oelogvmv log v oev m mpo appli schedul task within slice instead rcp improv space effici time differ rcp mpo dt also compar parallel time differ among three heurist tabl v vi differ memori constraint two tabl algorithm compar b ie vs b entri mark indic correspond b schedul execut memori constraint schedul not mark indic b schedul nonexecut tabl v increas parallel time rcp mpo rcp vs mpo ratio p lu fmm tabl show actual parallel time increas switch rcp mpo averag increas reason sometim mpo schedul outperform rcp schedul even though predict parallel time rcp shorter although mpo use much criticalpath inform rcp doe reduc number map need improv execut effici furthermor reus object soon possibl potenti improv cach perform factor mix togeth make actual execut time mpo schedul competit rcp dt aggress memori save util criticalpath inform comput slice tabl vi show time slowdown use dt instead mpo clear mpo substanti outperform dt term execut time even though dt effici memori usag differ especi signific larg number processor mpo optim memori usag parallel time howev time need dt instanc lu case avail memori dt schedul yang c fu tabl vi increas parallel time mpo dt mpo vs dt ratio lu p execut processor mpo schedul space costli run note dt space effici improv use mpo schedul slice slice merg dt avail amount memori space known dt schedul optim slicemerg process call dtsm discuss section list time reduct ratio use slice merg tabl vii dt vs dtsm result encourag case substanti improv obtain result parallel time dt schedul slice merg get close rcp schedul merg slice give schedul flexibl util criticalpath inform dt also effect improv cach perform thu dt algorithm slice merg valuabl problem size big avail amount space known tabl vii reduct parallel time dt dtsm ratio p lu p impact solvabl problem size new schedul algorithm help solv problem unsolv origin rapid system optim space usag exampl previous biggest matrix could solv t yang c fu delta use rapid lu code er contain million nonzero fillin use runtim activ memori manag dt schedul algorithm rapid abl solv larger matrix call ex million nonzero achiev megaflop te node term singlenod perform get megaflop per node node megaflop per node node consid code parallel softwar tool number good te conclus optim memori usag import solv larg parallel scientif problem softwar support becom complex applic irregular comput data access pattern main contribut work develop schedul optim techniqu effici memori manag scheme support use fast commun primit avail modern processor architectur propos techniqu integr rapid runtim system achiev good time space effici theoret analysi correct memori perform corrobor design techniqu experi spars matrix fmm code show overhead introduc memori manag activ reason mpo heurist competit criticalpath schedul algorithm deliv good memori time effici dt aggress memori save achiev competit time effici slice merg conduct space effici improv incorpor mpo slice schedul note propos techniqu use semiautomat program tool rapid still challeng develop fulli automat system futur interest studi automat gener coarsegrain dag sequenti code cosnard loi extend result complic depend structur chakrabarti et al girkar polychronopoulo ramaswami et al investig use propos techniqu perform engin parallel system darpa massiv parallel distributedmemori machin still valuabl highend largescal applic problem futur eg doe asci program extens smp cluster use dt schedul actual also improv cach perform use result data placement smp memori hierarchi need studi acknowledg would like thank apostolo gerasouli keshav pingali ed rothberg vivek sarkar rob schreiber kathi yelick comment work anonym refere siddhartha chatterje vegard holmedahl valuabl feedback improv present theorem point one refere also thank xiangmin jiao help implement rapid jia jiao provid us fmm code test case xiaoy li provid lu test matric r provabl effici schedul cilk effici multithread runtim system model benefit mix data task parallel multiprocessor runtim support finegrain irregular dag automat task graph gener techniqu effici comput static singl assign form control depend graph httpwww commun optim irregular scientif comput distribut memori architectur extend set basic linear algebra subroutin flexibl commun mechanismsfor dynam structur applic schedul runtim support parallel irregular comput spars lu factor partial pivot distribut memori machin also ucsb technic report trc comput solut larg spars posit definit system schedul structur unstructur comput automat extract functin parallel ordinari program implement activ messag splitc sci cluster architectur implic softwar support parallel process irregular dynam comput new parallel architectur spars matrix comput base finit project geometri high perform fortran problem progress spars gaussian elimin high perform comput parallel program compil exploit memori hierarchi sequenti parallel spars choleski factor improv load distribut parallel spars choleski factor partit schedul parallel program execut multiproc sor experi activ messag meiko cs elimin forest guid spars lu factor decoupl synchron data transfer messag pass system parallel comput person commun activ messag mechan integr commun comput runtim support portabl distribut data structur program parititon numa multiprocessor comput sy tem schedul code gener parallel architectur comput scienc list schedul without commun delay parallel comput dsc schedul parallel task unbound number processor revis juli tr algorithm extend set basic linear algebra subprogram model implement test program effici comput static singl assign form control depend graph new parallel architectur spars matrix comput base finit project geometri activ messag program partit numa multiprocessor comput system list schedul without commun delay techniqu overlap comput commun irregular iter applic commun optim irregular scientif comput distribut memori architectur schedul code gener parallel architectur improv load distribut parallel spars choleski factor provabl effici schedul languag finegrain parallel model benefit mix data task parallel decoupl synchron data transfer messag pass system parallel comput runtim compil parallel spars matrix comput runtim techniqu exploit irregular task parallel distribut memori architectur elimin forest guid spars lu factor spars lu factor partial pivot distribut memori machin partit schedul parallel program multiprocessor parallel program compil comput solut larg spars posit definit automat extract function parallel ordinari program experi activ messag meiko cs flexibl commun mechan dynam structur applic softwar support parallel process irregular dynam comput spars gaussian elimin highperform comput schedul runtim support parallel irregular comput ctr roxan adl marc aiguier franck delaplac toward automat parallel spars matrix comput journal parallel distribut comput v n p march heejo lee jong kim sung je hong sunggu lee task schedul use block depend dag blockori spars choleski factor proceed acm symposium appli comput p march como itali heejo lee jong kim sung je hong sunggu lee task schedul use block depend dag blockori spars choleski factor parallel comput v n p januari