t natur languag grammat infer recurr neural network a abstractthi paper examin induct infer complex grammar neural networksspecif task consid train network classifi natur languag sentenc grammat ungrammat therebi exhibit kind discriminatori power provid principl paramet linguist framework governmentandbind theori neural network train without divis learn vs innat compon assum chomski attempt produc judgment nativ speaker sharpli grammaticalungrammat data recurr neural network could possess linguist capabl properti variou common recurr neural network architectur discuss problem exhibit train behavior often present smaller grammar train initi difficult howev implement sever techniqu aim improv converg gradient descent backpropagationthroughtim train algorithm signific learn possibl found certain architectur better abl learn appropri grammar oper network train analyz final extract rule form determinist finit state automata investig b introduct paper consid task classifi natur languag sentenc grammat ungrammat attempt train neural network without bifurc learn vs innat compon assum chomski produc judgment nativ speaker sharpli grammaticalungrammat data recurr neural network investig comput reason comput recurr neural network power feedforward network recurr architectur shown least ture equival investig properti variou popular recurr neural network architectur particular elman narendra parthasarathi np william zipser recurr network also frasconigorisoda fg local recurr network find elman wz recurr neural network abl learn appropri grammar implement techniqu improv converg gradient descent base backpropagationthroughtim train algorithm analyz oper network investig rule approxim recurr network learn specif extract rule form determinist finit state automata previou work compar neural network machin learn paradigm problem work focus recurr neural network investig addit network analyz oper network train algorithm investig rule extract paper organ follow section provid motiv task attempt section provid brief introduct formal grammar grammat infer describ data section list recurr neural network model investig provid detail data encod network section present result investig variou train heurist investig train simul anneal section present main result simul detail investig oper network extract rule form determinist finit state automata investig section section present discuss result conclus motiv represent power natur languag tradit handl use symbol comput recurs process success stochast languag model base finitest descript ngram hidden markov model howev finitest model cannot repres hierarch structur found natur languag past year sever recurr neural network architectur emerg insideoutsid reestim algorithm extens hidden markov model intend use learn hierarch system algorithm current practic rel small grammar use grammat infer recurr neural network use sever smaller natur languag problem eg paper use elman network natur languag task includ neural network model shown abl account varieti phenomena phonolog morpholog role assign induct simpler grammar address often eg learn tomita languag task consid differ grammar complex recurr neural network investig paper constitut complex dynam system shown recurr network represent power requir hierarch solut ture equival languag acquisit certainli one import question studi human languag is peopl unfailingli manag acquir complex rule system system complex resist effort linguist date adequ describ formal system coupl exampl kind knowledg nativ speaker often take grant provid section instanc nativ speaker english know adject eager obligatorili take complement sententi complement contain overt subject verb believ cannot moreov eager may take sententi complement nonovert ie impli understood subject believ cannot i eager john believ john eager john i believ john eager i believ grammat judgment sometim subtl unargu form part nativ speaker languag compet case judgment fall accept aspect languag compet interpret consid refer embed subject predic talk follow exampl john stubborn mari talk john stubborn talk john stubborn talk bill first sentenc clear mari subject embed predic everi nativ speaker know strong contrast corefer option understood subject second convent asterisk use indic ungrammat third sentenc despit surfac similar third sentenc john must impli subject predic talk to contrast john understood object predic second sentenc subject arbitrari refer word sentenc read john stubborn arbitrari person talk john point emphas languag faculti impress discriminatori power sens singl word seen exampl abov result sharp differ accept alter interpret sentenc consider furthermor judgment shown robust sens virtual nativ speaker agre data light exampl fact contrast crop english languag for exampl stubborn contrast also hold dutch linguist chiefli chomski hypothes reason knowledg partial acquir lack variat found across speaker inde languag certain class data suggest exist fix compon languag system word innat compon languag faculti human mind govern languag process languag obey socal univers principl sinc languag differ regard thing like subjectobjectverb order principl subject paramet encod systemat variat found particular languag innat hypothesi languag paramet plu languagespecif lexicon acquir speaker particular principl learn base assumpt studi languageindepend principl becom known principlesandparamet framework governmentandbind gb theori paper investig whether neural network made exhibit kind discriminatori power sort data gblinguist examin precis goal train neural network scratch ie without divis learn vs innat compon assum chomski produc judgment nativ speaker grammaticalungrammat pair sort discuss abov instead use innat knowledg posit neg exampl use a second argument innat possibl learn grammar without neg exampl data first provid brief introduct formal grammar grammat infer natur languag thorough introduct see harrison fu detail dataset use experi formal grammar grammat infer briefli grammar g four tupl fn set termin nontermin compris alphabet grammar p set product rule start symbol everi exist languag l set string termin symbol grammar gener recogn also exist automata recogn gener grammar grammat infer concern mainli procedur use infer syntact product rule unknown grammar g base finit set string lg languag gener g possibl also finit set string complement lg paper consid replac infer algorithm neural network grammar english languag simpl grammar use elman shown tabl contain structur complet english grammar eg agreement verb argument structur interact rel claus recurs cat mari tabl simpl grammar encompass subset english languag from phrase full sentenc chomski hierarchi phrase structur grammar simplest grammar associ automata regular grammar finitestateautomata fsa howev firmli establish syntact structur natur languag cannot parsimoni describ regular languag certain phenomena eg center embed compactli describ contextfre grammar recogn pushdown automata other eg crossedseri depend agreement better describ contextsensit grammar recogn linear bound automata data data use work consist english posit neg exampl taken introductori gblinguist textbook lasnik uriagereka exampl organ minim pair like exampl eager john wini eager john win abov minim natur chang involv suggest dataset may repres especi difficult task model due small sampl size raw data name word first convert use exist parser major syntact categori assum gbtheori tabl summar part speech use partofspeech tag repres sole grammat inform suppli model particular sentenc addit grammat statu import refin implement categori exampl noun n john book destruct verb v hit sleep adject a eager old happi preposit p without complement c thought eager determin d man man adverb adv sincer sincer believ john want marker mrkr possess s of john mother destruct want help tabl part speech includ subcategor inform major predic name noun verb adject preposit experi show ad subcategor bare categori inform improv perform model exampl intransit verb sleep would place differ class obligatorili transit verb hit similarli verb take sententi complement doubl object seem give persuad would repres class flesh subcategor requir along line lexic item train set result class verb noun adject preposit exampl input data shown tabl sentenc encod grammat statu eager john n v c n v adv eager john n v n v adv eager n v v adv tabl exampl partofspeech tag tag done complet contextfre manner obvious word eg to may part one partofspeech tag result sever contradictori duplic sentenc variou method follow classic gb theori class synthes thetagrid individu predic via canon structur realiz csr mechan pesetski test deal case howev remov altogeth result report here addit number posit neg exampl equal by randomli remov exampl higher frequenc class train test set order reduc effect due differ priori class probabl when number sampl per class vari class may bia toward predict common class neural network model data encod follow architectur investig architectur topolog restrict number hidden node equal sens may represent capabl model expect frasconigorisoda fg architectur unabl perform task includ primarili control case frasconigorisoda local recurr network multilay perceptron augment local feedback around hidden node localoutput version use fg network also studi network call fg paper line narendra parthasarathi recurr network feedback connect output node hidden node np network architectur also studi jordan network call np paper line elman recurr network feedback hidden node hidden node train elman network backpropagationthroughtim use rather truncat version use elman ie paper elman network refer architectur use elman train algorithm william zipser recurr network node connect node diagram architectur shown figur input neural network data encod fix length window made segment contain eight separ input correspond classif noun verb adject etc subcategori class linearli encod input manner demonstr specif valu noun input noun noun class class linear order defin accord similar variou subcategori two output use neural network correspond grammat ungrammat classif data input neural network window pass sentenc tempor fix length window made segment contain separ input correspond classif noun class noun class verb class etc also test prove inferior figur frasconigorisoda local recurr network connect shown fulli figur narendra parthasarathi recurr network connect shown fulli figur elman recurr network connect shown fulli figur william zipser fulli recurr network connect shown fulli order begin end sentenc see figur size window variabl one word length longest sentenc note case input window small greater interest larger input window greater capabl network correctli classifi train data without form grammar exampl input window equal longest sentenc network store inform simpli map input directli classif howev input window rel small network must learn store inform shown later network implement grammar determinist finit state automaton recogn grammar extract network thu interest small input window case network requir form grammar order perform well gradient descent simul anneal learn backpropagationthroughtim use train global recurr network gradient descent algorithm describ author use fg network standard gradient descent algorithm found impract problem techniqu describ improv converg investig due depend initi paramet number simul perform differ initi weight train settest set combin howev due comput complex task possibl perform mani simul backpropagationthroughtim extend backpropag includ tempor aspect arbitrari connect topolog consid equival feedforward network creat unfold recurr network time realtim recurr learn rtrl also test show signific converg present problem modifi standard gradient descent algorithm possibl train network oper larg tempor input window network forc model grammar memor interpol train data individu simul section took averag two hour complet sun figur depict neural network input come input window sentenc window move begin end sentenc desir standard deviat nmse valu includ help assess signific result tabl show result use use techniqu list below except note result section elman network use two word input hidden node quadrat cost function logist sigmoid function sigmoid output activ one hidden layer learn rate schedul shown below initi learn rate weight initi strategi discuss below million stochast updat target valu provid end sentenc standard nmse std dev variat nmse std dev updat batch updat stochast learn rate constant learn rate schedul activ logist activ tanh section section ye cost function quadrat cost function entropi tabl comparison use use variou converg techniqu paramet constant case elman network use two word input ie slide window current previou word hidden node quadrat cost function logist activ function sigmoid output activ one hidden layer learn rate schedul initi learn rate weight initi strategi discuss below million stochast updat nmse result repres averag four simul standard deviat valu given standard deviat four individu result detect signific error increas nmse increas significantli train network weight restor previou epoch perturb prevent updat point techniqu found increas robust algorithm use learn rate larg enough help avoid problem due local minima flat spot error surfac particularli case william zipser network target output target output use logist activ function use tanh activ function help avoid satur sigmoid function target set asymptot sigmoid would tend to a drive weight infin b caus outlier data produc larg gradient due larg weight c produc binari output even incorrect lead decreas reliabl confid measur stochast versu batch updat stochast updat paramet updat pattern presen tation wherea true gradient descent often call batch updat gradient accumul complet train set batch updat attempt follow true gradient wherea stochast path follow use stochast updat stochast updat often much quicker batch updat especi larg redund dataset addit stochast path may help network escap local minima howev error jump around without converg unless learn rate reduc second order method work well stochast updat stochast updat harder parallel batch batch updat provid guarante converg to local minima work better second order techniqu howev slow may converg poor local minima result report train time equal reduc number updat batch case for equal number weight updat batch updat would otherwis much slower batch updat often converg quicker use higher learn rate optim rate use stochast updat henc alter learn rate batch case investig howev signific converg obtain shown tabl weight initi random weight initi goal ensur sigmoid start satur small correspond flat part error surfac ad dition sever set random weight test set provid best perform train data chosen experi current problem found techniqu make signific differ learn rate schedul rel high learn rate typic use order help avoid slow converg local minima howev constant learn rate result signific paramet perform fluctuat entir train cycl perform network stochast updat gener toler high learn rate batch updat due stochast natur updat alter significantli begin end final epoch moodi darken propos search converg learn rate schedul form jt learn rate time t j initi learn rate constant found learn rate final epoch still result consider paramet fluc henc ad addit term reduc learn rate final epoch our specif learn rate schedul found later section found use learn rate schedul improv perform consider shown tabl activ function symmetr sigmoid function eg tanh often improv converg standard logist function particular problem found differ minor logist function result better perform shown tabl cost function rel entropi cost function receiv particular attent natur interpret term learn probabl investig use quadrat rel entropi cost function quadrat cost function defin rel entropi cost function defin where correspond actual desir output valu k rang output and also pattern batch updat found quadrat cost function provid better perform shown tabl possibl reason use entropi cost function lead increas varianc weight updat therefor decreas robust paramet updat section train data investig divid train data subset initi one subset use train correct classif obtain prespecifi time limit expir addit subset ad work set continu work set contain entir train set data order term sentenc length result obtain epoch involv stochast updat mislead surpris find quit signific differ onlin nmse calcul compar static calcul even algorithm appear converg shortest sentenc first enabl network focu simpler data first elman suggest initi train constrain later train use way howev problem use section consist decreas perform shown tabl also investig use simul anneal simul anneal global optim method minim function downhil step accept process repeat new point uphil step may also accept therefor possibl escap local minima optim process proce length step declin algorithm converg global optimum simul anneal make assumpt regard function optim therefor quit robust respect nonquadrat error surfac previou work shown use simul anneal find paramet recurr network model improv perform comparison gradient descent base algorithm use simul anneal investig order train exactli elman network success train correct train set classif use backpropagationthroughtim detail section signific result obtain trial use simul anneal found improv perform simard et al howev problem pariti problem use network four hidden unit wherea network consid paper mani paramet result provid interest comparison gradient descent backpropagationthroughtim bptt method bptt make implicit assumpt error surfac amen gradient descent optim assumpt major problem practic howev although difficulti encount bptt method significantli success simul anneal which make assumpt problem experiment result result four neural network architectur given section result base multipl trainingtest set partit multipl random seed addit set japanes control data use test set we consid train model japanes data larg enough dataset japanes english opposit end spectrum regard word order is japanes sentenc pattern differ english particular japanes sentenc typic sov subjectobjectverb verb less fix argument less avail freeli permut english data cours svo argument permut gener avail exampl canon japanes word order simpli ungrammat english henc would extrem surpris englishtrain model accept japanes ie expect network train adapt simul anneal code lester ingber use english gener japanes data find model result signific gener japanes data error averag five simul perform architectur simul took approxim four hour summar result obtain variou network order make number weight architectur approxim equal use singl word input wz model two word input other reduct dimension wz network improv perform network contain hidden unit full simul detail given section goal train network use small tempor input window initi could done addit techniqu describ earlier possibl train elman network sequenc last two word input give correct averag trial classif train data gener test data result correct classif averag better perform obtain use network howev still quit low data quit spars expect increas gener perform obtain amount data increas well increas difficulti train addit dataset handdesign gb linguist cover rang grammat structur like separ train test set creat test set contain mani grammat structur cover train set william zipser network also perform reason well correct classif test set note test set perform observ drop significantli extend train indic use valid set control possibl overfit would alter perform significantli train classif std dev elman fg wz english test classif std dev elman fg wz tabl result network architectur comparison classif valu report averag five individu simul standard deviat valu standard deviat five individu result complet detail sampl elman network follow other network differ topolog except wz better result obtain use input window one word network contain three layer includ input layer hidden layer contain node hidden layer node recurr connect hidden layer node network train total million stochast updat input within rang zero one target output either bia input use best random weight set chosen base train set perform weight initi shown haykin weight initi node node basi uniformli distribut random number rang gammaf fanin neuron i logist output activ function use quadrat cost function use search converg learn rate schedul use learn rate learn rate train epoch current train epoch c train set consist noncontradictori exampl describ earlier english test set consist noncontradictori sampl japanes test set consist noncontradictori sampl take closer look oper network error train sampl network architectur shown figur error point graph nmse complet train set note natur william zipser learn curv util detect correct signific error increas figur show approxim complex error surfac base first deriv error criterion respect weight sum weight network nw total number weight valu plot epoch train note complex natur plot william zipser network figur show sampl plot error surfac variou network error surfac mani dimens make visual difficult plot sampl view show variat error two dimens note plot indic quantit conclus drawn test error plot plot shown figur respect two randomli chosen dimens case center plot correspond valu paramet train taken togeth plot provid approxim indic natur error surfac differ network type fg network error surfac appear smoothest howev result indic solut found perform well indic minima found poor compar global optimum andor network capabl implement map low error william zipser fulli connect network greater represent capabl elman architectur in sens perform greater varieti comput number hidden unit howev compar elman wz network error surfac plot observ wz network greater percentag flat spot graph conclus becaus show two dimens plot around one point weight space howev back learn curv william zipser network made smoother reduc learn rate tend promot converg poorer local minima epoch epoch epoch epoch wz figur averag nmse log scale train set train top bottom frasconigorisoda elman narendra parthasarathi william zipser hypothesi wz network perform wors error surfac present greater difficulti train method automata extract extract symbol knowledg train neural network allow exchang inform connectionist symbol knowledg represent great interest understand neural network actual addit symbol knowledg insert recurr neural network even refin train order tripl discret markov process fstate input nextstateg extract rnn epoch epoch epoch epoch wz figur approxim complex error surfac train top bottom frasconigorisoda elman narendra parthasarathi william zipser use form equival determinist finit state automata dfa done cluster activ valu recurr state neuron automata extract process recogn regular grammar howev natur languag cannot parsimoni describ regular languag certain phenomena eg center embed compactli describ contextfre grammar other eg crossedseri depend agreement better describ contextsensit grammar henc network may implement parsimoni version grammar unabl extract techniqu regular grammar g tupl start symbol n nontermin termin symbol respect p repres product form ab a b n weight weight weight weight weight weight weight weight weight weight weight weight weight weight weight weight weight weight weight weight weight figur error surfac plot fg network plot respect two randomli chosen dimens case center plot correspond valu paramet train weight weight weight weight weight weight weight weight weight weight weight weight weight weight weight weight weight weight weight weight weight figur error surfac plot np network plot respect two randomli chosen dimens case center plot correspond valu paramet train algorithm use automata extract from work follow network train or even train appli procedur extract network learn ie network current concept dfa learn dfa extract process includ follow step cluster recurr network activ space s form dfa state construct transit weight weight weight weight weight weight weight weight weight weight weight weight weight weight weight weight weight weight weight weight weight weight weight figur error surfac plot elman network plot respect two randomli chosen dimens case center plot correspond valu paramet train weight weight weight weight weight weight weight weight weight weight weight weight weight weight weight weight weight weight weight weight weight weight weight weight figur error surfac plot wz network plot respect two randomli chosen dimens case center plot correspond valu paramet train diagram connect state togeth alphabet label arc put transit togeth make full digraph form loop reduc digraph minim represent hypothesi train network begin partit or quantiz state space fairli wellsepar distinct region cluster repres correspond state finit state automaton recent prove arbitrari dfa stabli encod recurr neural network one simpl way find cluster divid neuron rang q partit equal width thu n hidden neuron exist q n possibl partit state dfa construct gener state transit diagram ie associ input symbol partit state left partit state activ initi partit state start state dfa determin initi valu t next input symbol map partit state valu assum loop form otherwis new state dfa form dfa thu construct may contain maximum q n state practic usual much less sinc partit state reach t eventu process must termin sinc finit number partit avail and practic mani partit never reach deriv dfa reduc minim dfa use standard minim algorithm note dfa extract method may appli discretetim recurr net regardless order hidden layer recent extract process proven converg extract dfa learn encod neural network extract dfa depend quantiz level q extract dfa use valu q start use standard minim techniqu compar result automata pass train test data set extract dfa found extract automata correctli classifi train data test data smaller valu q produc dfa lower perform larger valu q produc significantli better perform sampl extract automata seen figur difficult interpret extract automata topic futur research analysi extract automata view aid interpret addit import open question well extract automata approxim grammar implement recurr network may regular grammar automata extract may also use improv perform system via iter combin rule extract rule insert signific learn time improv achiev train network prior knowledg may lead abil train larger network encompass target grammar paper investig use variou recurr neural network architectur fg np elman wz classifi natur languag sentenc grammat ungrammat therebi exhibit kind discriminatori power provid principl paramet linguist framework governmentandbind theori best worst perform architectur were elman wz np fg surpris elman network outperform fg np network comput power elman network shown least ture equival np network shown ture equival within linear slowdown fg network figur automata extract elman network train perform natur languag task start state state bottom left accept state state top right string reach accept state reject recent shown comput limit elman network special case wz network fact elman wz network top perform surpris howev theoret elman network outperform wz network open question experiment result suggest train issu represent issu backpropag throughtim bptt iter algorithm guarante find global minima cost function error surfac error surfac differ elman wz network result suggest error surfac wz network less suitabl bptt train algorithm use howev architectur learn represent grammar network learn grammar hierarchi architectur increas comput power for given number hidden node give insight whether increas power use model complex structur found grammar fact power elman wz network provid increas perform suggest abl find structur data may possibl model fg network addit investig data suggest correct classif train data two word input would possibl unless network abl learn signific aspect grammar anoth comparison recurr neural network architectur gile horn compar variou network randomli gener state finit memori machin local recurr narendra parthasarathi network prove good superior power network like elman network indic either task requir increas power vanilla backpropag throughtim learn algorithm use unabl exploit it paper shown elman wz recurr neural network abl learn appropri grammar discrimin sharpli grammaticalungrammat pair use gblinguist howev gener limit amount natur data avail expect increas difficulti encount train model data use clear consider difficulti scale model consid larger problem need continu address converg train algorithm believ improv possibl address natur paramet updat gradient descent howev point must reach improv gradient descent base algorithm requir consider natur error surfac relat input output encod rare chosen specif aim control error surfac abil paramet updat modifi network behavior without destroy previous learn inform method network implement structur hierarch recurs relat acknowledg work partial support australian telecommun electron research board sl r sequenti connectionist network answer simpl question microworld comparison criterion function linear classifi supervis learn probabl distribut neural network dynam discretetim comput three model descript languag lectur govern bind knowledg languag natur finit state automata simpl recurr network note learn rate schedul stochast optim toward faster stochast gradient search structur represent connectionist model distribut represent comput capabl localfeedback recurr network act finitest machin unifi integr explicit rule learn exampl recurr network local feedback multilay network syntact pattern recognit applic network learn phonolog learn extract finit state automata secondord recurr neural network extract learn unknown grammar recurr neural network higher order recurr network role similar hungarian vowel harmoni connectionist account connectionist perspect prosod structur repres variabl inform simpl recurr network introduct formal languag theori neural network introduct theori neural comput introduct automata theori learn algorithm probabl distribut feedforward feedback network gile experiment comparison recurr neural network fast simul reanneal adapt simul anneal asa attractor dynam parallel connectionist sequenti machin serial order parallel distribut process approach simul anneal inform theori statist cours gb syntax lectur bind empti categori gile natur languag grammat infer comparison recurr neural network machin learn method effici learn second order method learn past tens english verb use recurr neural network languag learn cue rule encod inputoutput represent connectionist cognit system focus backpropag algorithm tempor pattern recognit control dynam system use neural network construct determinist finitest automata recurr neural network extract rule discretetim recurr neural network rule revis recurr neural network path categori induct dynam recogn learn past tens english verb combin symbol neural learn comput beyond ture limit comput capabl recurr narx neural network comput power neural net analysi recurr backpropag acceler learn layer neural network learn appli contextu constraint sentenc comprehens learn featurebas semant simpl recurr network dynam construct finitest automata exampl use hillclimb rule map connectionist symbol process mani map local recurr global feedforward network critic review architectur induct finit state languag use secondord recurr network induct finitest languag use secondord recurr network effici gradientbas algorithm onlin train recurr network trajec tori learn algorithm continu run fulli recurr neural network learn finit state machin selfclust recurr network tr ctr michal eransk matej makula ubica beukov organ state space simpl recurr network train recurs linguist structur neural network v n p march marshal r mayberri iii risto miikkulainen broadcoverag pars neural network neural process letter v n p april peter tio ash j s mill learn beyond finit memori recurr network spike neuron neural comput v n p march edward kei shiu ho lai wan chan analyz holist parser implic robust pars systemat neural comput v n p may peter c r lane jame b henderson increment syntact pars natur languag corpora simpl synchroni network ieee transact knowledg data engin v n p march juan c vallelisboa florencia reali hctor anastasa eduardo mizraji elman topolog sigmapi unit applic model verbal hallucin schizophrenia neural network v n p septemb henrik jacobsson rule extract recurr neural network taxonomi review neural comput v n p june