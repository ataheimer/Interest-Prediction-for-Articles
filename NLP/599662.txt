t dual formul regular linear system convex risk a paper studi gener formul linear predict algorithm includ number known method special case describ convex dualiti class method propos numer algorithm solv deriv dual learn problem show dual formul close relat onlin learn algorithm furthermor use dualiti show new learn method obtain numer exampl given illustr variou aspect newli propos algorithm b introduct consid statist learn problem find paramet random observ minim expect loss risk given loss function lff x n observ independ drawn fix unknown distribut d want find ff minim expect loss x z assumpt underli distribut x natur method solv use limit number observ empir risk minim erm method cf is choos paramet ff minim observ riskn specif consid follow type regular linear system convex loss shall assum f g convex function appropri chosen posit regular paramet balanc two term typic choos gw function penal larg w formul natur aris mani statist learn applic exampl order appli formul linear regress let howev formul also interest exampl robust estim one interest use order appli formul purpos train linear classifi choos f decreas function fdelta exampl includ support vector machin logist regress one interest theoret result vc analysi concern primal formul dimension independ gener error obtain vc analysi extend howev turn care nonvc analysi actual suitabl particular problem both primal form dual form introduc later paper paper studi numer learn aspect dual form certain attract properti compar primal formul dualiti certain learn problem gener gametheoret sens also lead discoveri new learn methodolog paper organ follow section deriv dual formul propos relax algorithm solv dual problem section gener deriv includ equal constraint section provid applic propos algorithm machin learn section provid numer exampl section studi learn aspect dual formul conclus final remark made section dual formul sinc convex program problem involv linear transform primal variabl w dual form obtain introduc auxiliari variabl data point sup n kdelta dual transform fdelta assum f lower semicontinu see well known k convex switch order inf w sup valid minimax convexconcav program problem a proof interchang ie strong dualiti given appendix a obtain n w minim fix at n use rg denot gradient g respect w note main bodi paper assum gradient convex function exist requir case constraint need dealt introduc appropri lagrangian multipli section main reason treatment numer consider howev mathemat proof strong dualiti appendix direct numer consequ use gener definit convex function dualiti without assum differenti exampl gener case constraint convex function cz regard modif cz cz z satisfi constraint substitut obtain n st rgw n simplifi notat consid dual transform gdelta hdelta also convex function follow thu rewrit follow dual formul n optim solut w primal problem obtain n system nondegener also see section give follow solut n comparison also follow equat f differenti note deriv equat use two legendr transform k dual f h dual g write x matrix data x row legendr transform point view dual transform xw respect f k dual transform w respect g h consequ regard x interact dual variabl w interact linear interact simplest possibl interact w paper propos follow gener relax algorithm solv dual problem denot dimens w d algorithm dual gaussseidel find delta approxim minim updat v n delta updat inner iter algorithm algorithm essenti fix dual variabl j j find perturb delta dual variabl reduc object function sinc object function reduc step algorithm alway converg exact optim use converg true optim solut also hessian exist wellbehav around optim solut local approxim quadrat function follow asymptot converg rate algorithm linear minim exactli rate converg depend spectrum hessian matrix optim solut main attract featur algorithm simplic one might also consid precondit conjug gradient acceler algorithm precondition howev mani interest case kdelta nonsmooth contain simpl bound henc cg might help interest aspect dual formul dual variabl correspond data point x algorithm step one look one data point similar onlin updat algorithm differ algorithm keep dual variabl comput far typic onlin algorithm inform kept actual difficult convert algorithm onlin learn method set use appropri scale factor n step minimax style mistak bound also deriv accordingli howev fullscal studi relat issu left anoth report paper shall provid simpl analysi mistak bound empir risk estim appendix b illustr basic idea demonstr theoret connect dual batch formul onlin learn note primal form dual form strike similar besid connect onlin algorithm sever reason interest studi dual form exampl later see paper g quadrat regular function algorithm especi simpl form anoth reason f nonsmooth g smooth onedimension optim problem dual gaussseidel algorithm easier onedimension optim problem primal gaussseidel algorithm furthermor even primal problem infinit dimension that is infin dual problem still finit dimension assum finit sampl size elimin primal dimension import gener perform consequ machin learn see section section final dual form relationship gener kernel form svm origin investig vapnik constraint order dual formul valid equat solut howev certain circumst possibl singular henc solut right hand side lie rang rgw w r impos constraint x x matrix row consist data x simplic shall discuss case equal constraint inequ constraint handl similarli sinc origin problem convex optim problem therefor dual problem also convex optim problem impli equal constraint dual variabl must linear constraint situat usual aris gw flat linear transform w c matrix vector sinc crgw j d therefor correspond constraint impos dual variabl follow form equat modifi n s order preserv structur dual formul employ gaussseidel updat algorithm propos use augment lagrangian method method modifi dual hv primal regular term gw lagrangian multipli vector correspond constraint small posit penalti paramet follow algorithm solv constraint util modifi dual h s cf page algorithm dual augment lagrangian solv hdelta replac h s delta use algorithm use current v initi point updat v els delta one nice properti augment lagrangian approach augment dual problem solv algorithm suffici high accuraci step execut finit number time mean upon termin bound away zero next would like briefli discuss situat primal problem convex constraint cw compon cdelta convex function let correspond lagrangian multipli primal problem rewritten well known compon c j w indic cw convex thu regard regular term similar gw select appropri lagrangian multipli essenti solv problem regular term gw replac gw learn problem exact valu lagrangian either known for exampl entropi regular probabl distribut investig later noncruci sinc appropri way determin lagrangian regular paramet kind crossvalid anyway also mani case constraint primal variabl w becom flat segment dual variabl case appli unmodifi algorithm comput relationship obtain w modifi accordingli regard primal inequ constraint regular term observ paramet increas correspond c j w decreas optim solut thu also possibl use algorithm suggest paper inner solut engin adjust j appropri examin c j w optim solut idea similar modifi lagrangian method propos deal dual constraint although modif augment lagrangian method appli mani case usual recommend due varieti reason subtl method employ howev due limit space also issu noncruci learn problem shall skip discuss use dual pair section list exampl convex legendr dualiti relev learn problem use pu denot primal function primal variabl u use qv denot dual function dual variabl v assum k symmetr posit definit oper dual pair p dual pair p dual pair set posit prior note pu flat dimens pu vector ident compon therefor v satisfi constraint dual transform contain free lagrangian paramet s nonneg lagrangian paramet dual pair p note mani exampl contain constraint dual some also primal variabl case also ignor constraint consid correspond function constraint satisfi last exampl interest relev classif problem also use note dual linear transform either primal variabl primal convex function easili comput assum nonsingular linear transform regular term section briefli discuss regular condit interest squar regular one import regular condit squar penalti k symmetr posit definit oper mani applic one choos ident matrix case henc algorithm replac minim note system particularli simpl actual reduc small problem constant size solv constant time inner product b comput larg problem comput b a actual precomput domin pnorm regular let interest data qnorm bound sinc x w bound gener perform also dimension independ case approxim newton method employ solv newton iter lambda usual one may use one newton iter need comput deriv part second deriv part ij mani case comput costli evalu b squar regular formul howev trick employ allevi problem sinc accur estim second deriv part ij less import safe use good upperbound second deriv note essenti constraint v order obtain w equat modifi similarli essenti constraint regular term given interest data norm or entropi bound typic let larg number case again approxim newton method requir deriv part requir evalu addit second deriv part requir evalu ij entropi regular usual interest either data infinitenorm bound weight vector w give probabl distribut case unlik use norm regular condit lead gener perform degrad logarithm dimens gener perform entropi regular dimension independ first consid normal entropi probabl distribut prior distribut dual choos lagrangian paramet deriv term hv second deriv term hv n n therefor two term correspond expect varianc x distribut w note second deriv varianc vanish small posit number use regular solut consid nonnorm entropi posit weight w one easili deal gener situat ad neg weight part w dual newton approxim straightforward remark practic absolut reason need choos regular term base simpl primal form perfectli reason and highli recommend design learn algorithm base simpl choic dual function hv whether dual gw complic irrelev far dual algorithm concern exampl consid dual huber function simpl form hv v jvj solv rel easili regular condit gw good replac norm regular gener one choos hv piecewis linear quadrat function would like w concentr around appropri form hv also shape concentr around howev uncertainti princip freedom dual dimens invers proport freedom primal dimens easili seen case squar regular sinc kernel k gamma dual problem invers kernel k primal problem hand would like design algorithm w bias toward nonzero prior case entropi regular one need use hv monoton increas function easili construct use piecewis quadrat function loss term regress first consid standard squareloss regress regular term line search step algorithm solv analyt as robust estim interest case regular term line search step algorithm solv analyt as robust natur explain dual updat sinc bound i therefor contribut one data point final weight w well control principl use directli design dual form robust regress without resort primal form distribut estim consid modif maximumentropi method given normal entropi regular formul becom maximumentropi method howev conjectur modif advantag standard maximumentropi method reason softmargin svm formul often prefer optimalsepar hyperplan method practic in sens choos nonzero regular paramet svm theoret advantag appropri nonzero regular paramet demonstr even linearli separ classif problem possibl achiev exponenti rate converg appropri nonzero regular param eter similar result known optimalsepar hyperplan method fact conjectur gener perform optimalsepar hyperplan method slower exponenti worst case case modifi maximumentropi method newton updat correspond line search step algorithm given note the standard maximumentropi method updat fail reason quadrat penalti method becom illcondit penalti paramet close zero easi remedi use modifi lagrangian method alreadi discuss likelihood consid mixtur model estim problem want find distribut w loglikelihood maxim case normal entropi regular note initi valu set zero anymor sinc shall thu start posit valu newton updat correspond line search step algorithm given w correspond nice behav probabl densiti vector random variabl r n instead entropi regular norm base regular appropri kernel such fisher kernel also util next consid logist regress classif problem deriv maximumlikelihood estim given case newton step correspond algorithm becom start nonzero initi valu exampl good choic binari classif binari classif typic choos nonneg decreas f f one exampl logist regress investig abov section examin exampl let choos gw case exact analyt solut obtain fu replac squar regular condit normal entropi regular newton updat becom shall chosen base class label maxim margin see section x shall data vector inclass member neg data vector outofclass member algorithm correspond normal winnow exponenti gradient updat posit weight way support vector machin correspond perceptron updat better understand thi observ perceptron updat correspond squar norm regular winnow updat correspond entropi regular for exampl see proof method comparison exponenti gradient versu gradient descent optim margin svm linearli separ problem modifi perceptron minim norm margin constraint correspond minim entropi margin constraint winnow or exponenti gradient famili algorithm softmargin svm modifi optim margin method introduc decreas loss function f squar regular case winnow algorithm translat choic entropi regular soft margin svmlike loss function f updat rule addit use form fu standard one use standard svm formul discuss shortli algorithm point view relationship svm perceptron algorithm mention chapter platt compar svm dual updat rule relat perceptron updat algorithm analog winnow or exponenti gradient famili algorithm readili observ due normal exponenti form rhdelta shall note mani discuss exponenti gradient algorithm emphas proper match loss function see exampl formul care choic match loss replac proper regular condit combin loss function also notic nonnorm entropi use regular condit obtain algorithm correspond nonnorm exponenti gradient updat svm choos f as let gw given updat exact solut mention abov platt deriv updat rule howev deriv employ bregman techniqu suitabl specif problem particular techniqu convex dualiti adopt paper gener bregman approach also platt comment maxim margin true fact found statist signific differ method standard svm applic although special care taken avoid potenti zerodenomin problem see discuss end section implement consider shall also interest mention author alreadi adopt updat applic worthwhil mention standard svm shift b non regular absorb b last compon w append constant featur typic regular condit gw symmetri gw c vector one last compon zero elsewher case algorithm employ correspond dual constraint exact solut modifi as introduct nonregular shift b standard svm formul creat complex optim although significantli illustr section also seem advers effect gener analysi logn factor note expect gener perform on full regular see section cannot achiev nonregular dimens well known nonregular dimens vcdimens contribut logarithm factor gener perform term on worst case tight for exampl see howev increas varianc may compens potenti reduc bia practic differ use nonregular shift versu regular shift unclear experi goal section illustr propos algorithm exampl reader develop feel converg rate propos algorithm context exist algorithm due broad scope paper theoret natur illumin provid exampl everi specif instanc learn formul mention section thu provid two exampl one algorithm one algorithm instanc algorithm similar behavior sinc research interest mainli natur languag process shall illustr algorithm text categor exampl standard data set compar text categor algorithm reuter set news stori publicli avail httpwwwresearchattcomlewisreutershtml use reuter modapt split partit document train valid set data contain class document train set document test set experi use word stem without stopword remov also use inform gain criterion select inform wordcount featur use binari valu select featur which indic word either appear appear document text categor perform usual measur precis recal rather classif error posit true posit posit theta posit true posit neg theta sinc linear classifi contain threshold paramet adjust tradeoff precis recal convent report breakeven point precis equal recal sinc document reuter dataset multipli categor common studi dataset separ binari classif problem correspond categori overal perform measur microaverag precis recal breakeven point comput overal confus matrix defin sum individu confus matric correspond categori follow exampl use top ten categori the remain categori typic small experi done pentium ii pc linux time includ train use train data featur select test test data pars document winnow versu regular winnow exampl studi perform regular winnow correspond updat standard winnow algorithm posit weight use learn rate standard winnow algorithm approxim optim exampl is predict wrong shrink weight expgammax data x class multipli weight expx data x class predict rule is w x impli outofclass w inclass w normal predefin paramet regular winnow algorithm let outofclass data sign data revers train phase ff paramet attempt maxim decis margin inclass outofclass data fix exampl illustr intuit correspond let start point weight updat inclass data ff start point weight updat outofclass data goal achiev margin size ff regular paramet fix gamma run algorithm iter data comparison note kwk data x f g compon thu w x also taken sparsiti data account reason us pick reflect good rang threshold choic microaverag breakeven point winnow cpu time second microaverag breakeven point regular winnow cpu time second microaverag breakeven point winnow cpu time second microaverag breakeven point regular winnow cpu time second better feel report time one note c decis tree induc even featur featur cannot handl take hour finish train partli spars structur cannot util smo algorithm svm one fast text categor algorithm requir three minut tabl show breakeven point algorithm ten categori regular winnow consist superior differ statist sig nific compar differ svm perceptron algorithm indic tabl categori winnow regular winnow acq moneyfx grain crude trade interest ship wheat corn microaverag tabl breakeven point winnow versu regular winnow although result good svm which achiev state art perform text categor comparison unfair sinc allow posit weight implement winnow style algorithm indic tri find indic word particular topic ignor word even though appear document may strongli suggest document belong topic util addit featur need regular version standard winnow algorithm posit neg weight convers obtain vector version dualiti formul the weight w matrix dual data point vector sinc extens investig work shall thu skip comparison perceptron versu svm exampl compar perceptron algorithm propos svm method smo method svm describ chapter current prefer method solv svm problem textcategor regular paramet svm fix gamma learn rate perceptron is predict wrong updat weight data x class x data x class predict rule is w x impli outofclass w x impli inclass also normal weight kwk end iter data enhanc perform total run time perceptron iter train data second faster speed compar winnow algorithm indic winnow algorithm spend time normal step propos formul exactli implement describ use iter call algorithm iter algorithm therefor total use iter train data run time second smo algorithm tricki implement made best judgment tradeoff among intern paramet suggest given start point run time particular implement second tabl includ comparison breakeven point three algorithm ten categor microaverag breakeven point observ svm consist better perceptron algorithm howev statist insignific random discrep among svm due differ converg criteria shall also use point j order gamma end algorithm ten categori impli dual constraint larg satisfi result similar svm microaverag breakeven obtain use iter algorithm total run time second case constant featur valu append data point categori perceptron smo algorithm acq moneyfx grain crude trade interest ship wheat corn microaverag tabl breakeven point perceptron versu svm would like mention although exampl faith implement suggest algorithm import stick specif formul given paper exampl magic reason start valu decreas constraint valu updat algorithm also recommend use updat valu delta smaller quantiti implement winnow normal frequent inner iter avoid numer instabl suggest newton method gain better numer stabil exampl observ algorithm use use small fraction exact deltaupd given equat especi import sinc denomin x zero caus larg chang one replac regular version smaller increment allevi problem heavi oscil phenomenon less problem sinc small give larg denomin learn consequ differenti convex function h defin distanc function h w w also call bregman diverg properti h w w strictli convex h w w consid n sampl x n x y let partit n sampl batch subsampl batch xi contain n data let denot optim solut let denot solut approxim minim ith batch subsampl denot empir expect respect ith batch n sampl ffl posit approxim error control practic check dualiti gap primal problem let restraint ith batch subsampl n h h h note first order condit optim solut impli follow estim equat anoth form thu obtain follow inequ d similar analysi primal problem shall ignor contribut simplic thu obtain follow fundament inequ dual d shall defin v x bound converg v v in term hbregman diverg converg v also interest compar inequ primal form approxim empir risk minim w n primal problem satisfi g w w use dualiti bregman diverg relationship note give better constant fact asymptot tight constant compar asymptot estim given although learn bound primal formul investig insight studi implic term dual variabl one bound given vapnik term number supportvector ie number nonzero dual variabl see although vapnik bound interest two fundament drawback one drawback bound asymptot correct sens give expect gener error bound slower on n sampl sinc number support vector usual grow unbound howev demonstr expect gener error grow rate on gener case furthermor misclassif error train linear classifi problem linearli separ exponenti n note later situat clearli vapnik bound intend for therefor bound asymptot far inferior correct rate drawback vapnik bound handl situat number support vector small although indic number support vector larg gener perform becom poor bound tend lead statist learn commun toward think obtain good gener perform somewhat desir reduc number support vector exampl clearli case design support vector machin howev analysi lead correct larg sampl converg rate indic minim number support vector import addit drawback support vector concept also character learn problem predict lead small number support vector therefor now nontrivi bound sole reli empir evid entir satisfactori theori goal section studi learn aspect dual problem care character gener perform term dual variabl analysi complement primal analysi studi let n x n chosen way approxim underli distribut d let denot optim solut limit regard random variabl respect that is shall assum solut continu version exist assum n simplic denot n take expect ex nover n randomli chosen data x n obtain denot empir estim data x n n denot approxim solut error ffl random variabl usual wellbehav continu case estim equat given relationship deriv exist and subgradi otherwis see appendix a w optim solut primal problem note right hand side becom converg empir expect random variabl mean estim standard probabl techniqu illustr typic h squarelik right hand side variancelik thu converg rate on detail case studi given shall repeat addit expect hbregman diverg also obtain expect exponenti lim d use independ assumpt x n take limit obtain ff use obtain larg deviat type exponenti probabl bound moment bound also similarli obtain see exampl obtain expect gener error bound larg deviat style probabl bound use techniqu point earlier primal estim dual estim rather similar constant factor shall regard nonsignific purpos deriv gener bound therefor shall repeat analysi exampl given order see expect error bound cannot improv consid linear loss quadrat regular case solv primal problem obtain kex dual problem j b becom equal sinc local g usual approxim quadrat regular f linear loss b x y depend we shall intent remov quadrat term f expans correspond drop k term therefor asymptot tight conclud remark paper introduc dual formul class regular linear learn method convex risk new formul relat onlin learn algorithm stochast gradient descent method dual form also lead gener kernel formul support vector machin kernel formul one usual substitut primaldu relationship primal formul approach emphas complet differ dual risk dual formul lead new learn algorithm well new insight certain learn problem intermedi primaldu formul relat minimax onlin mistak bound framework howev pose problem pac style batch set relationship thu bridg onlin learn mistak bound analysi pac analysi numer point view abl obtain new batch learn method dual formul importantli method deriv systemat way mention possibl transform deriv batch learn method onlin learn algorithm shall leav anoth report appendix b demonstr basic idea obtain mistak bound use natur ask gener dualiti machin learn primaldu formul view learn problem gametheoret set learner choos primal weight variabl w minim certain risk oppon choos dual variabl control random sampl behavior maxim risk although strong dualiti convexconcav program difficult extend gener problem gametheoret point view still adopt conjectur even strong dualiti violat may still possibl design dual formul appropri learn method howev mani open issu area requir futur studi also studi learn aspect dual formul specif abl obtain gener bound better asymptot tight constant primal analysi interest consequ dual analysi evid number nonzero dual variabl or support vector signific may surpris sinc number stabl even slight perturb loss function gener perform bound base number support vector asymptot suboptim especi linearli separ classif problem bound intend for dual formul also provid valuabl insight learn problem exam ple sinc number dual variabl alway independ dimension primal problem therefor appropri regular assumpt dimension primal problem appear learn bound suggest frequent mention notion the curs dimension realli issu mani learn problem might seem surpris first understand implic consid high dimension densiti estim problem standard argument the curs dimension high dimens order approxim densiti function one need fill number point box exponenti dimension howev reason partial valid appropri way measur whether two distribut similar compar expect bound function respect two distribut therefor instead pointwis converg criterion shall consid converg weak topolog close two densiti measur close action set test function impli proper question ask given fix number data mani test function util obtain stabl densiti estim methodolog use maximum entropi method note appropri regular condit number test function use dimension independ howev sinc choos weak topolog densiti estim need determin topolog test function given famili test function need find appropri structur hierarchi approxim famili proper norm alway defin test function dimension independ partit obtain induc dual metric densiti use measur converg densiti estim exampl typic partit bound test function base logexponenti criterion induc entropi metric densiti space see analysi dualiti test function densiti weight essenti dualiti investig paper furthermor method measur complex learn problem dual vc point view directli measur complex paramet space in exampl densiti function estim advantag dual point view reli specif parametr function famili one choos importantli mani learn problem complex measur converg rate random test function mean test space see indic wellestablish probabl tool appli obtain complex measur dimension independ proof strong dualiti would like show valid interchang order inf w sup primaldu formul assum gw as well function involv analysi take valu equival dataindepend constraint also assum solut primal problem exist uniqu simplic notat purpos let goal show exist sup wellknown for exampl see dualiti gap sup valid is find w case w call saddl point follow demonstr exist saddl point construct consid w minim primal problem known page exist subgradi which gener gradient concept convex function see definit section denot delta thatn reader familiar convex analysi equat becom follow estim equat empir risk differenti wn n relationship subgradi dualiti see page w achiev minimum n is also definit gamma subgradi fdelta relationship subgradi dualiti page achiev maximum is finish proof convent subgradi set denot delta howev notat conveni use delta denot member subgradi set proof onlin mistak bound consid follow scenario given sampl primal variabl w weight obtain exact solut dual problem v dual v project v onto first v project v onto ith sampl case proof inequ fundament bridg onlin learn formul batch dual formul refer paper see inequ import note h squar like distanc which usual true right hand side order ok impli kd h v small standard onlin learn telescop techniqu appli deriv mistak bound case done fashion similar parallel exampl studi howev gener case treat anoth dedic report follow shall mere provid illustr use squar regular term draw conclus v k is w v k make assumpt is bregman diverg l squar like which true smooth loss function use estim equat v k sum v k give typic mistak bound correct growth order logn standard assumpt exampl assum c c vs k gamma norm bound b well known logarithm growth optim see thi consid onedimension easi verifi let x larg k w henc growth rate logn achiev simpli sum equal k note logarithm factor indic correct batch learn rate on cannot obtain typic random techniqu for exampl see modifi onlin algorithm and mistak bound batch algorithm also import note match loss function concept cf import analysi allow us analyz problem loss function role match loss function correspond proper choic regular term analysi data depend rather loss function depend r relax method find common point convex set applic solut problem convex program induct learn algorithm represent text categor practic method optim matrix comput gener converg result linear discrimin updat rel loss bound singl neuron discrimin framework detect remot protein homolog addit versu exponenti gradient updat linear predict learn quickli irrelev attribut abound new linearthreshold algorithm mit press convex analysi smola editor advanc kernel method estim depend base empir data statist learn theori compar studi featur select text categoriza tion analysi regular linear function classif problem primal formul regular linear system convex risk tr ctr fred j damerau tong zhang sholom m weiss nitin indurkhya text categor comprehens timedepend benchmark inform process manag intern journal v n p march tong zhang fred damerau david johnson text chunk base gener winnow journal machin learn research zhang yue pan tong zhang focus name entiti recognit use machin learn proceed th annual intern acm sigir confer research develop inform retriev juli sheffield unit kingdom tong zhang leaveoneout bound kernel method neural comput v n p june ron meir tong zhang gener error bound bayesian mixtur algorithm journal machin learn research tong zhang cover number bound certain regular linear function class journal machin learn research p qiang wu yime ying dingxuan zhou multikernel regular classifi journal complex v n p februari ron meir gunnar rtsch introduct boost leverag advanc lectur machin learn springerverlag new york inc new york ny