t processor map techniqu toward effici data redistribut a abstractruntim data redistribut enhanc algorithm perform distributedmemori machin explicit redistribut data perform algorithm phase differ data decomposit expect deliv increas perform subsequ phase comput redistribut howev repres increas program overhead algorithm comput discontinu data exchang among processor memori paper present techniqu minim amount data exchang block cyclicc or viceversa redistribut arbitrari number dimens preserv semant target destin distribut pattern techniqu manipul data logic processor map target pattern implement ibm sp map techniqu demonstr redistribut perform improv approxim tradit data processor map rel tradit map techniqu propos method afford greater flexibl specifi precis data element redistribut element remain onprocessor b introduct effort standard dataparallel fortran program distributedmemori machin high perform fortran forum compos forti academ industri government agenc propos hpf high perform fortran mani concept origin propos fortran vienna fortran dataparallel fortran languag incorpor hpf fundament compon hpf specif distribut align data array compil direct due nonuniform memori access time characterist distributedmemori machin determin appropri data decomposit critic perform dataparallel program machin data distribut deal data array distribut among processor memori data align specifi colloc data array goal data decomposit maxim system perform balanc comput load among processor minim remot memori access or commun messag data distribut wellsuit one phase algorithm may good term perform subsequ phase therefor hpf support explicit runtim data redistribut redistribut may also occur implicitli subprogram boundari result runtim oper eg data realign use data redistribut repres perform tradeoff expect higher effici new distribut subsequ comput commun cost redistribut data among processor memori consequ minim execut time data redistribut obviou merit reduc amount data exchang among processor memori one possibl optim toward reduc overal redistribut execut time subject paper present techniqu minim amount data exchang among processor memori block cyclicc or viceversa redistribut arbitrari number dimens preserv semant target destin distribut pattern techniqu manipul data logic processor map target pattern clearer present map techniqu view data static manipul processor map data implement techniqu redistribut oper would specifi map data processor sinc map techniqu work within realm logic processor architectureindepend thu could incorpor differ redistribut implement variou distributedmemori architectur section provid summari data redistribut issu relat work section present map techniqu appli onedimension data prove map techniqu achiev minim data movement section show natur extens techniqu mdimension data section discuss impact use techniqu programm compil respect addit compar data redistribut execut time perform propos map techniqu tradit dataprocessor map section conclud paper data redistribut mani research espous util incorpor redistribut capabl dataparallel languag kali languag one first incorpor runtim data redistribut mechan dino address implicit redistribut data subprogram boundari hall et al discuss global optim employ set redistribut call fortran program hypertask compil dataparallel c program incorpor runtim redistribut facil chapman et al introduc vienna fortran dynam data distribut capabl discuss highlevel implement vienna fortran engin vfe figur present segment hpf code illustr use data redistribut hpf array real number a initi distribut onto threedimension processor configur p distribut pattern block cyclic appli dimens respect p contain forti processor processor own thetatheta data element follow amount comput redistribut new set pattern cyclic cyclic block onto differ shape logic processor configur q figur illustr initi distribut left subsequ redistribut right shade portion figur depict data element own processor wherea own data global contigu locat initi own data global noncontigu locat follow redistribut note processor configur q also consist forti processor howev p q vari shape exampl illustr one redistribut possibl mani distinct combin sourc destin distribut pattern possibl hpf requir howev data processor distribut pattern declar equal dimension eg fig data processor pattern declar threedimension hpf requir sourc target processor configur equal size henceforth denot initi sourc distribut pattern program destin target distribut pattern similarli shall refer sourc target processor configur p p respect note p embed primit fig real comput comput figur exampl hpf figur redistribut blockcycl cycliccyclicblock previou exampl illustr data redistribut result chang map data processor thu necessit data exchang among processor perspect send processor data exchang view simultan scatter data element p p ie processor p perform onetomani send oper differ data recipi p altern perspect receiv processor data exchang view simultan gather data element p p ie processor perform manytoon receiv oper distinct data sender p figur illustr commun pattern jp processor particip scatter gather oper data exchang view alltoal person commun order perform data redistribut processor must determin ident processor receiv data well ident processor must send data set comput processor exchang data recal defin redistribut logic processor therefor logic physic processor map dictat actual data movement among memori machin exampl logic processor map physic node would obvious requir messagepass exchang data figur redistribut commun pattern sever research effort focus effici method determin send receiv processor set redistribut gupta et al deriv close form express commun set base fortran ds block cyclic blockcycl distribut pattern approach global array or templat indic combin knowledg use comput send receiv set altern approach node scan local data array onc determin destin processor element place element messag packet bound processor onedimension data redistribut perform distinct algorithm differ combin distribut pattern multidimension redistribut implement seri sequenti onedimension redi tribut stichnoth et al propos method comput ownership set array assign statement due similar determin send receiv set advoc comput togeth send processor commun inform togeth data receiv approach chiefli intend commun righthand side operand array assign statement incorpor data redistribut pitfal mathemat represent regular distribut facilit determin processor set data redistribut pitfal robustli handl arbitrari sourc target processor configur arbitrari number data array dimens pitfal develop inclus paradigm compil project univers illinoi research present focu effici comput sendrec processor set rather effici actual data exchang portion redistribut found latter oper sever order magnitud costli term execut time send receiv set determin data exchang distributedmemori machin perform either pointtopoint collect commun messagepass primit techniqu effici multipl scatter gather oper present mckinley et al survey issu regard alltoal commun wormholerout machin effici simultan redistribut data among physic processor affect topolog rout switch mechan underli machin techniqu communicationeffici data redistribut address messag content certain topolog present author propos data redistribut commun cost model parameter number messag size network content model express commun sequenc permut may execut fix number contentionfre step multiphas redistribut defin redistribut data intermedi distribut pattern eventu arriv destin distribut model use conjunct multiphas redistribut show lower overal cost achiev compar singlephas redistribut reshap perfect poweroftwo size array hypercub discuss earliest work optim data redistribut minim data exchang regular hpf distribut pattern present relat redistribut optim strategi propos wakatani wolf explor logic physic processor map redistribut reduc commun overhead techniqu assum underli toru topolog map data way commun processor partit nonoverlap set thu keep commun local group messag content physic network reduc drawback approach logic physic data map base local inform word techniqu impart map base sole best optim redistribut howev redistribut part larger dataparallel program logic physic data map must remain consist throughout execut program logic physic processor map determin redistribut oper may best map overal perform entir applic assert compil privi global inform ought determin logic physic processor map possibl howev manipul data elementlog processor map target ie map p long semant target distribut pattern violat topic remaind paper onedimension logic processor map begin illustr util propos map techniqu onedimension data figur illustr initi block distribut sixteen element data array a onto eight logic processor subsequ redistribut array use cyclic pattern map initi distribut data onto physic node machin establish program initi see arrow label subsequ redistribut among processor must retain consist logic physic processor map see arrow label logic processor id lpid bold ital superimpos data processor own two contigu element block two noncontigu element stride p number processor cyclic lpid map increas numer order specifi claim unnecessari restrict permut lpid conform semant cyclic pattern sinc data distribut processor hpf requir specif order processor cyclic requir global data element own processor global indic separ stride p figur illustr benefit permut lpid map data element use sixteen element array fig show two altern redistribut array cyclic choic show convent cyclic map lpid data result two sixteen data element mark fill rectangl remain processor follow redistribut call number data hit among processor choic show altern cyclic map lpid permut result total eight data element one per processor remain origin memori elimin exchang six data element among processor anoth advantag processor must send data two processor use choic one processor use choic thu reduc number destin factor reduc interprocessor commun overhead reduct redistribut cost exampl fig small given size exampl extend sixteen million data element permut lpid elimin exchang six million data element across processor processor map techniqu consist minim size data transfer arbitrari data block processor set size develop systemat method determin permut lpid map data tech henceforth logic processor unless otherwis state assum number processor kept same block size block two million block size cyclicc one million physic node logic processor id onto p onto p initi distribut data physic node perform compil redistribut data perform runtim bold ital figur data distribut redistribut onto physic node niqu ensur processor retain maximum amount data possibl conform semant sourc target distribut pattern establish upper bound ratio amount data retain p processor total number data element n present function determin lpid data map redistribut block cyclicc achiev upper bound make follow assumpt let p number processor number p gamma n total number data element number n gamma distribut p assum processor own b element thu consid redistribut block cycliccpattern blockb pattern b variabl consid assumpt cyclicc assum c divid b ie integ z redistribut symmetr term amount data movement among processor ie redistribut block cyclicc redistribut cyclicc block result equal amount data movement direct redistribut choic choic figur logic processor data map altern data initi distribut among p processor assum data redistribut among p processor r number data element global array remain data hit origin processor follow redistribut block cyclicc defin hitratio r n maxhitratio upper bound hitratio c block size block cyclicc pattern respect z integ maxhitratio integ everi cycl cp data element map c contigu data element cyclicc sinc complet cycl lpid map one complet data block own lpid j block pattern processor lpid j map exactli ic element ie ic data hit p processor number data hit cp ecp hitratio b case integ sinc b icp cannot complet cycl lpid map lpid origin block data block thu number hit greater lpid consequ number hit across processor greater icp remaind proof follow case maxhitratio achiev permut lpid z integ multipl number processor ie z ip howev goal achiev upper bound valu z i ip order satisfi aim must consid differ permut lpid maxim number data hit figur demonstr permut lpid yield maxhitratio next use semant defin function determin permut lpid map data array map function defin ptupl q place holder let f function map lpid place holder q j assign lpid place holder specifi permut lpid repres map lpid data element cyclicc distribut pattern p possibl permut p processor may case mani permut yield ratio maxhitratio howev exhaust test permut determin whether produc ratio would impract sinc would requir exponenti amount comput gener p therefor present function determin permut achiev maxhitratio b p c lpid map uniqu q fi equat specifi function map lpid q fi intuit behind map function first view place holder q j circular list function map lpid place holder q map lpid z place lpid map lpid z place lpid on gener map lpid i z mod p place lpid figur illustr behavior f appli differ valu z p simplic choos reduc cyclic part a show exampl case map broken row better illustr distanc consecut lpid distinct two exampl onetoon part a f onetoon part b is part b one lpid map locat lpid map place holder formal depend valu z p possibl f yield one permut part a produc six possibl permut part b sinc lpid lpid map q turn optim term maxhitratio arbitrarili map two lpid place holder q q hold true lpid lpid place holder q q lpid lpid place holder q q shall prove result later lemma figur map lpid place holder optim map function given arbitrari z p gcdz p determin whether f onetoon not lemmata establish thi lemmata establish f achiev maxhitratio whether onetoon z natur number z bc let gcdz p greatest common divisor z p gcdz establish onetoon map place holder q word gcdz proof proof contradict assum gcdz choos k j recal map function f map lpid i z mod p place lpid let map distanc rz mod p place holder lpid j sinc map place holder distanc modp zero ie rz mod must z mod impli assumpt lemma let p z natur number z bc gcdz q fi proof show divid z jz p m mp mod arbitrari m thu second term sum jz p left establish two lemma captur behavior f equat natur shown relationship gcdz p function f fig a thu f onetoon part b gcdz map two lpid place holder q next establish two lemma show f produc permut alway yield maxhitratio lemma let p z natur number z bc gcdz determin singl permut lpid achiev maxhitratio proof sinc f map lpid i z place lpid lpid map cyclic first data element data block block figur illustr situat arbitrari lpid thu alway least one data hit per lpid case z p exactli c data hit per lpid map cycl begin ensur c data hit sinc z p equival b cp lpid cannot map anoth c data element bsize data block did would violat semant cyclic map thu exactli c hit per processor cp hit processor follow case z p least c data hit per lpid establish abov sinc lpid map first element data block also map cp th element well sinc z p see fig let integ divis lpid map element number total cj element thu cp ec data hit per lpid b cp ecpdata hit lpid again b c cp element data figur lpid map first element data block lemma let p z natur number z bc k lpid map q ik f remap k way place holder permut yield maxhitratio proof lemma establish k k lpid name lpid ijpk place holder q fi consequ k lpid map first data element lpid data block block case z p b cp exactli c data hit per lpid figur illustr situat z p k z thu ck cz b furthermor k lpid c data element fit lpid bsize data block lpid map place holder q fi remap one first ck data element regardless permut k lpid sinc lpid group c data hit it lpid cannot appear within data block sinc b cp therefor exactli c data hit lpid cp d b cp ecp data hit total among p processor case z p b cp least c data hit per lpid establish abov integ m mcp cp mc data hit lpid sinc p lpid data element number b gamma data block own lpid c data data element element figur z p map least cycl bsize data block claim m c data hit even figur illustr situat group p processor map csize block lpid data block block must show lpid map one last element data block word show ck b gamma mcp proof contradict assum ck exist two integ s z rk ck c z z z z sinc pk contradict thu cgcdz mcp given result lpid csize block must appear within last b gamma mcp element data block regardless permut order therefor m data hit per lpid m cp p processor cp cp b mcp cp group element data figur z p lemmata prove follow result theorem redistribut block cyclicc b c respect block integ z p number processor number global data array element logic processor map function equat achiev maxhitratio multidimension logic processor map section extend logic processor map techniqu present section mdimension data array specif extend techniqu optim logic processor map redistribut block block block cyclicc addit demonstr approach redistribut twodimension data initi distribut dimens subsequ redistribut one dimens eg blockblock cyclic initi distribut one dimens redistribut dimens eg block mdimension redistribut extend techniqu appli onedimension lpid map dimens global data array let mdimension data array n theta n theta n theta theta nmgamma data element processor direct hpf declar processor arrang specifi name rank extent dimens let processor arrang distribut set block size cyclic block size respect dimens a extend equat equat map rectilinear set lpid p j n j data element jth dimens mdimension data redistribut maxhitratio similar ratio present section product maxhitratio dimens present lemma substanti result lemma maxhitratio mdimension global array defin d b c map function equat achiev upper bound proof lemmata establish b maxhitratio onedimension data map function f achiev upper bound mdimension data product upper bound dimens yield maximum data hit ratio mdimension array function g gener f appli g respect dimens upper bound achiev figur appli data matrix distribut across theta processor grid lpid bold ital redistribut data matrix blockblockto cycliccycl lpid data map dimens mark block tradit cyclic map lpid data indic trad optim techniqu use g shown opt key right figur indic data hit follow redistribut tradit optim map tradit map yield hitratio map use g result hitratio reduc threefold increas number data hit tradit map theorem lemma prove follow result subscript figur label denot number processor given dimens theorem redistribut block block block cyclicc respect block size z integ z number processor n number data element dimens logic processor map function equat achiev theta b c block opt trad map data hit data hit data element trad opt figur block block cyclic cyclic redistribut twodimension onedimension redistribut redistribut mdimension array need necessarili involv redistribut dimen sion instanc data matrix may initi distribut blockblock redistribut cyclic row dimens distribut extend map techniqu case defin vector equat defin new function map cartesian lpid placehold lemma establish maxhitratio twodimension onedimension redistribut base valu gcdz p lemmata establish map function h achiev max hitratio lemma redistribut block cyclicc let b block size row dimens c cyclic block size n theta n data matrix proof proof quit similar proof lemma here twodimension grid processor remap vector place holder sinc distribut one dimens substitut p p p lemma remaind proof follow point thu b count number hit row dimens number multipli block size column dimens b obtain total number data hit prove map function h yield maxhitratio first establish correspond lpid place holder show map yield given ratio sinc column coordin s lpid relev valu comput h lpid row coordin r map place holder q hr sinc exactli lpid map place holder thu greater p place holder map h lemma establish h achiev maxhitratio lpid row coordin r map place holder lemma establish result kp lpid map place holder lemma let z natur number exactli p place holder map hr s p lpid map place holder q hr remap way place holder q hr q hr q hr q permut yield proof first conjectur lemma follow directli lemma proof second part lemma similar proof lemma instead map place holder lemma p lpid map place denot hpf distribut pattern holder establish previous remaind proof follow make substitut maxhitratio result lemma except b must factor sinc number data element row contribut number data hit lemma let z natur number place holder map hr s hr s map lpid rjp ks place holder q hr lpid map place holder q hr remap p k way place holder q permut yield proof first portion lemma follow directli lemma proof second part lemma similar proof lemmata current situat lpid map place holder remaind proof follow make substitut maxhitratio lemma figur demonstr map function h appli theta matrix matrix origin distribut blockblock theta processor grid redistribut cyclic six processor vector lpid bold ital befor lpid rs identifi processor cartesian system block size row dimens blockblock eight b cyclic block size cyclic two fig show tradit map sinc map set cartesian lpid vector lpid undefin hpf use h map lpid produc hitratio distribut column inconsequenti number data hit achiev sinc row distribut word redistribut block cyclicc cyclic would result number data hit redistribut blockblock cyclic addit lpid row index r could permut data hit ratio achiev eg lpid lpid figur show exampl gcdz processor lpid s lpid s could permut possibl way optim data hit ratio would obtain lemmata prove follow result theorem redistribut block cyclicc b c respect block size row dimens integ z b block size column dimens defin grid processor n theta n number global data array block data element data hit figur block block cyclic redistribut element logic processor map function equat achiev onedimension twodimension redistribut anoth possibl data matrix initi distribut one dimens redistribut two dimens deriv new map function equat redistribut block cyclicc sinc specifi twodimension data distribut declar twodimension grid place holder q function map processor lpid place holder q rs thu i comput order pair establish maxhitratio onedimension twodimension redistribut base valu gcdz p lemmata prove achiev maxhitratio block data element data hit figur block block cyclic redistribut redistribut block cyclicc let b block size row dimens c cyclic block size n theta n data matrix proof proof quit similar proof lemmata singl dimens case lemma redistribut block cyclicc consid current situat first dimens block cyclicc thu proof lemma appli lemma must includ factor b second dimens lemma let z natur number place holder map i map yield maxhitratio d b proof proof similar proof lemma sinc i map lpid i z place thu lpid map first row data block block pattern sinc lpid own b element second dimens factor contribut maxhitratio lemma let z natur number place holder map i place holder p dimens map place holder p dimens map exactli k lpid map place holder k lpid map place holder q rs remap place holder q k way achiev maxhitratio d b proof proof similar proof lemma lemma current situat second p processor dimens place holder dimens map dimens case onedimension map lemma sinc lpid own b element second dimens factor contribut maxhitratio figur illustr lpid map function appli theta matrix matrix initi distribut block across six processor redistribut cyclic block across theta processor grid lpid in bold ital superimpos matrix data hit ratio use map function exampl gcdz fig illustr situat gcdz latter figur demonstr flexibl permut lpid instanc processor lpid lpid could permut data hit ratio would achiev lemmata prove follow result theorem redistribut block cyclicc b c respect block size row dimens integ z b block size column dimens defin grid processor n theta n number global data array element logic processor map function equat achiev analysi perform section discuss possibl impact dataparallel programm compil respect use optim map techniqu present execut time benefit block data data hit figur block cyclic block redistribut optim map techniqu compar tradit map techniqu number data redistribut case effect optim map programm permut lpid optim map function section may incur undesir side effect thu may suitabl redistribut instanc instanc programm may lose neighbor data relationship fig tradit cyclic map lpid neighbor optim map lpid neighbor suppos programm impart particular logic physic processor map given program util inform maintain neighbor data relationship physic processor order preserv relationship programm may favor tradit processordata processor neighbor own data element adjac perspect global data data element data hit figur block cyclic block redistribut map optim map call data redistribut figur illustr static ie vari one program execut anoth logic physic processor map logic processor alway map physic node pi a tradit map ie preserv neighbor processor relationship b neighbor data relationship physic node corrupt use optim map ie make p p becom neighbor previous p p neighbor compil runtim system uniqu determin logic physic processor map howev programm unabl maintain neighbor data relationship physic node inde indirectnetwork multicomput eg ibm spx workstat cluster concept neighbor node furthermor alloc parallel job thu data may vari distinct execut program sinc differ physic processor may alloc job figur illustr number program execut alloc logic processor physic node differ invoc top part fig logic physic p a tradit logic physic p b optim figur tradit optim map physic node map static tradit processordata map three distinct program execut bottom portion fig illustr optim processordata map three distinct program execut perspect physic node machin permut lpid whether tradit optim transpar programm argu situat use optim map alway justifi sinc neighbor data relationship physic node cannot maintain exploit programm logic physic p b second execut logic physic p a first execut logic physic c third execut p optim logic physic p a first execut logic physic c third execut p logic physic p b second execut figur tradit optim map physic node map dynam permut lpid also facilit greater flexibl programm may want influenc data element redistribut processor remain onprocessor tradit map techniqu programm one choic ie lpid map increas numer order optim map techniqu often sever option sinc number lpid may map place holder see fig b compil support programmerspecif lpid permut user inher greater control data processor map flexibl may becom even signific data align introduc situat number data element may map local indic fewer data element map indic therefor abl influenc indic redistribut indic remain onprocessor could enhanc overal perform effect optim map compil loss neighbor data relationship may complic role compil gener spmd node program hpf sourc code let us examin simpl exampl let element array initi distribut block redistribut cyclic see fig assum follow refer pattern appear compilergener spmd code follow call redistribut cyclic use tradit map techniqu compil gener follow commun paradigm obtain offprocessor element exclud boundari processor lpid commun lpid igamma lpid i optim map techniqu neighbor processor relationship maintain thu commun paradigm cannot use exampl lpid commun lpid lpid lpid commun lpid lpid problem easili overcom howev compil util place holder map inform determin optim map function let array place holder lpid gener equat optim map compil would specifi neighbor commun nonboundari processor follow lpid place holder q j commun lpid q j gamma q j essenti compil perform tabl lookup determin neighbor lpid relationship extens boundari processor straightforward exclud present discuss perform integr optim map techniqu data redistribut librari darel written c use mpif perform result obtain ibm spx argonn nation laboratori assess runtim perform optim map techniqu compar data redistribut execut time use optim techniqu redistribut execut time use tradit map figur illustr variou perform comparison two map tech niqu figur show block cyclicc redistribut node rang matrix size thousand million byte float point element block size cyclicc pattern row dimens maintain onehalf block size b block distribut optim map techniqu appli row dimens matrix demonstr significantli lower execut time tradit map optim techniqu achiev redistribut result roughli time need redistribut use tradit techniqu larger valu c respect b greater effect optim map techniqu overal execut time significantli data hit occur rel tradit map smaller valu c rel b benefit optim techniqu lessen sinc rel differ number data hit reduc redistribut instanc regardless block size b c global data size find optim techniqu outperform equal tradit map figur demonstr blockblock cyclicccyclicc redistribut logic theta logic theta processor configur respect matrix size rang million float point number perform plot map techniqu appli dimens matric befor optim map redistribut outperform tradit map case block size c maintain onesixth oneeighth respect block size row column dimens matric execut time advantag optim map techniqu remain consist larger processor configur well comput send receiv processor set includ execut time plot shown execut time attribut calcul repres small fraction overal total ie order hundr microsecond small data size order ten millisecond largest data size plot see detail conclus distinct comput phase algorithm perform effici differ data distribut pattern effici runtim redistribut enhanc overal algorithm perform effect use data redistribut dataparallel program promot minim runtim cost perform data exchang among node machin import aspect reduc redistribut cost minim amount data move among processor memori without violat semant distribut pattern paper present techniqu map logic processor id data element data redistribut prove techniqu maxim ratio data retain local tradit optim matrix size byte float time sec figur block cyclicc theta processor tradit optim matrix size byte float time sec figur blockblock cyclicccyclicc theta processor tradit optim matrix size byte float time sec figur blockblock cyclicccyclicc theta processor total amount data exchang among processor perform redistribut block cyclicc arbitrari number dimens architectureindepend techniqu improv data redistribut execut time perform approxim wide rang data size examin impact techniqu dataparallel programm compil respect believ minim amount interprocessor data exchang effect optim data redistribut dataparallel program work remain gener techniqu arbitrari block size necessarili integ multipl number processor pad data array becom integ multipl number processor one possibl addit extend techniqu redistribut differ size processor set redistribut cyclicc area futur studi acknowledg grate argonn nation laboratori staff use ibm spx also thank anonym refere construct comment r high perform fortran languag specif version draft fortran languag specif languag specif version program distribut memori architectur use kali the dino parallel program languag interprocedur compil fortran mimd machin hypertask support dynam redistribut resiz array ipsc dynam data distribut distribut rout algorithm broadcast person commun hypercub on gener effici data commun distributedmemori machin runtim array redistribut hpf program gener commun array statement design implement evalu automat gener effici array redistribut routin distribut memori multicomput commun optim use paradigm compil distributedmemori multicomput darel portabl data redistribut librari distributedmemori machin effici algorithm index oper messagepass system a survey collect commun wormholerout massiv parallel comput an approach communicationeffici data redistribut the complex reshap array boolean cube processor map techniqu toward effici data redistribut optim redistribut array distribut memori multicomput mpi ibm spsp current statu futur direct tr ctr jihwoei huang chihp chu effici commun schedul method processor map techniqu appli data redistribut journal supercomput v n p septemb wang minyi guo dame wei divideandconqu algorithm irregular redistribut parallel compil journal supercomput v n p august chinghsien hsu shengwen bai yehch chung chuse yang gener basiccycl calcul method effici array redistribut ieee transact parallel distribut system v n p decemb frddric desprez cyril randriamaro jack dongarra antoni petitet yve robert schedul blockcycl array redistribut ieee transact parallel distribut system v n p februari chinghsien hsu shihchang chen chaoyang lan schedul contentionfre irregular redistribut parallel compil journal supercomput v n p june yehch chung chinghsien hsu shengwen bai basiccycl calcul techniqu effici dynam data redistribut ieee transact parallel distribut system v n p april chinghsien hsu spars matrix blockcycl realign distribut memori machin journal supercomput v n p septemb chinghsien hsu yehch chung effici method kr r r kr array redistribut journal supercomput v n p may chinghsien hsu yehch chung donlin yang chyiren dow gener processor map techniqu array redistribut ieee transact parallel distribut system v n p juli chinghsien hsu yehch chung chyiren dow effici method multidimension array redistribut journal supercomput v n p aug minyi guo yi pan improv commun schedul array redistribut journal parallel distribut comput v n p may minyi guo ikuo nakata framework effici data redistribut distribut memori multicomput journal supercomput v n p novemb chinghsien hsu kunm yu compress diagon remap techniqu dynam data redistribut band spars matrix journal supercomput v n p august peizong lee wenyao chen gener commun set array assign statement blockcycl distribut distribut memori parallel comput parallel comput v n p septemb stavro souravla mano roumelioti pipelin techniqu dynam data transfer multiprocessor grid intern journal parallel program v n p octob antoin p petitet jack j dongarra algorithm redistribut method blockcycl decomposit ieee transact parallel distribut system v n p decemb